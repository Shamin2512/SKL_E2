{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fcba82d",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d94d2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Example 2 is inbalanced data set; ~2200 in PD and ~1100 in SNP\\n    Goal is to predict if mutation is SNP or PD\\n    improve_MCC branch\\n    \\n    Total samples: 3368\\n    2254 PD samples\\n    1111 SNP samples\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Example 2 is inbalanced data set; ~2200 in PD and ~1100 in SNP\n",
    "    Goal is to predict if mutation is SNP or PD\n",
    "    improve_MCC branch\n",
    "    \n",
    "    Total samples: 3368\n",
    "    2254 PD samples\n",
    "    1111 SNP samples\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5737f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Imports the required libraries and packages \"\"\"\n",
    "\n",
    "import pandas as pd  # Data manipulation in dataframes\n",
    "import numpy as np  # Array manipulation\n",
    "\n",
    "import random as rd # Random seed generation\n",
    "import time #Time program run time\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch #CV visualise\n",
    "\n",
    "# import PDB2AC\n",
    "\n",
    "from sklearn.metrics import(\n",
    "    matthews_corrcoef,  # MCC for evaluation\n",
    "    balanced_accuracy_score, #hyperparameter evaluation\n",
    "    f1_score,  #hyperparameter evaluation\n",
    "    confusion_matrix,  # confusion matrix for classification evalutation\n",
    "    classification_report #Return the F1, precision, and recall of a prediction\n",
    "    )\n",
    "\n",
    "from sklearn.model_selection import(\n",
    "    train_test_split,  # Splits data frame into the training set and testing set\n",
    "    GridSearchCV,  # Searches all hyperparameters\n",
    "    RandomizedSearchCV, # Searches random range of hyperparameters\n",
    "    GroupKFold, # K-fold CV with as groups\n",
    "        )\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier #SK learn API for classificastion random forests\n",
    "from sklearn.tree import DecisionTreeClassifier #Single tree decisions \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier #allows for confidence scores to be predicted for each\n",
    "\n",
    "# np.set_printoptions(precision = 3,threshold=sys.maxsize, suppress=True) #full array printing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb451c9e",
   "metadata": {},
   "source": [
    "### Split dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbfacd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Test_Split(file):\n",
    "    \"\"\"      \n",
    "    Input:      file             Pre-processed dataset done by PDB2AC script\n",
    "\n",
    "    Returns:    Training_Set     80% training set split\n",
    "                Testing_Set      20% testing set split\n",
    "                labels           Class labels for training set\n",
    "\n",
    "    80% training and 20% testing split. Writes the data to txt files. Splits are shuffled randomly\n",
    "    \"\"\"\n",
    "    AC_dataset = pd.read_csv(file)    \n",
    "    Training_Set, Testing_Set = train_test_split(AC_dataset,train_size = 0.8)\n",
    "    labels = Training_Set['dataset_pd'].astype('int32')\n",
    "    Training_Set.reset_index(drop=True, inplace = True)\n",
    "    Testing_Set.reset_index(drop=True, inplace = True)\n",
    "    \n",
    "#     Training_file = Training_Set.drop(['Binding', 'SProtFT0', 'SProtFT1', 'SProtFT2', 'SProtFT3', 'SProtFT4', 'SProtFT5', 'SProtFT6', 'SProtFT7', 'SProtFT8', 'SProtFT9', 'SProtFT10', 'SProtFT11', 'SProtFT12', 'Interface', 'Relaccess', 'Impact', 'HBonds', 'SPhobic', 'CPhilic', 'BCharge', 'SSGeom', 'Voids', 'MLargest1', 'MLargest2', 'MLargest3', 'MLargest4', 'MLargest5', 'MLargest6', 'MLargest7', 'MLargest8', 'MLargest9', 'MLargest10', 'NLargest1', 'NLargest2', 'NLargest3', 'NLargest4', 'NLargest5', 'NLargest6', 'NLargest7', 'NLargest8', 'NLargest9', 'NLargest10', 'Clash', 'Glycine', 'Proline', 'CisPro'],axis=1)\n",
    "#     Testing_file = Testing_Set.drop(['Binding', 'SProtFT0', 'SProtFT1', 'SProtFT2', 'SProtFT3', 'SProtFT4', 'SProtFT5', 'SProtFT6', 'SProtFT7', 'SProtFT8', 'SProtFT9', 'SProtFT10', 'SProtFT11', 'SProtFT12', 'Interface', 'Relaccess', 'Impact', 'HBonds', 'SPhobic', 'CPhilic', 'BCharge', 'SSGeom', 'Voids', 'MLargest1', 'MLargest2', 'MLargest3', 'MLargest4', 'MLargest5', 'MLargest6', 'MLargest7', 'MLargest8', 'MLargest9', 'MLargest10', 'NLargest1', 'NLargest2', 'NLargest3', 'NLargest4', 'NLargest5', 'NLargest6', 'NLargest7', 'NLargest8', 'NLargest9', 'NLargest10', 'Clash', 'Glycine', 'Proline', 'CisPro'],axis=1)\n",
    "\n",
    "    with open('Training set.txt', 'w') as file: #Writes training data to files\n",
    "        file.write(Training_Set.to_string())\n",
    "    with open('Testing set.txt', 'w') as file: #Writes testing data to files\n",
    "        file.write(Testing_Set.to_string())\n",
    "\n",
    "\n",
    "    return Training_Set, Testing_Set, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8b7f5",
   "metadata": {},
   "source": [
    "### Initial evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf8d9857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(Initial_RFC, Input_test, Classes_test):\n",
    "#     \"\"\" Input:  Input_test      Features test data\n",
    "#                 Classes_test    Class label test data\n",
    "\n",
    "#         Evaluates the training data before balancing. Random forest classifier makes prediction using the test features. True values \n",
    "#         are the class labels testing data\n",
    "#     \"\"\"\n",
    "\n",
    "#     Output_pred = Initial_RFC.predict(Input_test) #Always perdict on the unseen test data, as train has been used by the estimastor\n",
    "#     print(f\"              **Initial Evaluation**\\n\")\n",
    "#     print(f\"Confusion Matrix:\\n {confusion_matrix(Classes_test, Output_pred)}\")\n",
    "#     print(f\"{classification_report(Classes_test, Output_pred)}\\nMCC                {matthews_corrcoef(Classes_test, Output_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b3a2df",
   "metadata": {},
   "source": [
    "## Group K-fold CV (outer loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "975ff775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV(Training_Set):\n",
    "    \"\"\"      \n",
    "    Input:      Training_Set     80% training set split\n",
    "            \n",
    "    Returns:    Input_train     Training features, for each fold\n",
    "                Classes_train   Ttraining classes, for each fold\n",
    "                Input_val       Validating features, for each fold\n",
    "                Classes_val     Validating classes, for each fold\n",
    "\n",
    "    Group K-fold CV that maintains protein groups, attempts to preserve number of samples of each class \n",
    "    for each fold, and ensures protein groups are separated. Creates 5 folds.\n",
    "    \"\"\"\n",
    "    \n",
    "    Input_CV       = Training_Set.drop(['dataset_pd'], axis =1)         #Input features for training\n",
    "    Output_CV      = Training_Set['dataset_pd'].copy().astype('int32')  #Class labels for training\n",
    "    Protein_Groups = Training_Set['AC Code'].to_list()                  #List of proteins for grouping\n",
    "        \n",
    "    CV             = GroupKFold(n_splits = 5)                           #Only shuffles proteins in each group, not groups in fold\n",
    "    \n",
    "    IT_list = []\n",
    "    LT_list = []\n",
    "    IV_list = []\n",
    "    LV_list = []\n",
    "    \n",
    "    for train_idx, val_idx in CV.split(Input_CV, Output_CV, Protein_Groups):\n",
    "        Rd = np.random.randint(time.time())                             #Random number from 1 to time since epoch\n",
    "\n",
    "        Input_train                        = Input_CV.loc[train_idx]\n",
    "        Classes_train                      = Output_CV.loc[train_idx]\n",
    "        Input_train.drop(['AC Code'], axis = 1, inplace = True)         #Group identifer not needed for training\n",
    "\n",
    "        Input_val                          = Input_CV.loc[val_idx]\n",
    "        Classes_val                        = Output_CV.loc[val_idx]\n",
    "        Input_val.drop(['AC Code'], axis   = 1, inplace = True)\n",
    "        \n",
    "        IT_list.append(Input_train.sample(frac=1, random_state=Rd)) \n",
    "        LT_list.append(Classes_train.sample(frac=1, random_state=Rd))\n",
    "        IV_list.append(Input_val.sample(frac=1, random_state=Rd))\n",
    "        LV_list.append(Classes_val.sample(frac=1, random_state=Rd))\n",
    "        \n",
    "    with open('CV validation data.txt', 'w') as f:\n",
    "        for number, fold in zip(range(len(LV_list)), LV_list):\n",
    "            f.write(f\"Fold: {number}\\n\\n{fold.to_string()}\\n\\n\\n\")\n",
    "\n",
    "    return(IT_list, LT_list, IV_list, LV_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a335a585",
   "metadata": {},
   "source": [
    "## Balancing (inner loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b6924e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_minority_class(classData):\n",
    "    \"\"\" Input:    classData  Array of class labels\n",
    "    \n",
    "        Returns:  minClass   The label for the minority class\n",
    "                  minSize    The number of items in the minority class\n",
    "                  maxSize    The number of items in the majority class\n",
    "\n",
    "        Finds information about the inbalance in class sizes\n",
    "    \"\"\"\n",
    "    \n",
    "    Minority_count = 0\n",
    "    Majority_count = 0\n",
    "    for datum in classData:\n",
    "        if datum == 1:\n",
    "            Majority_count += 1\n",
    "        elif datum == 0:\n",
    "            Minority_count += 1\n",
    "\n",
    "    minClass = 0\n",
    "    minSize = Minority_count\n",
    "    maxSize = Majority_count\n",
    "    if Minority_count > Majority_count:\n",
    "        minClass = 1\n",
    "        minSize = Majority_count\n",
    "        maxSize = Minority_count\n",
    "\n",
    "    return minClass, minSize, maxSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d1241bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance(inData, classData, minClass, minSize):\n",
    "    \"\"\" Input:    inData          array of input data\n",
    "                  classData       array of classes assigned\n",
    "                  minorityClass   class label for the minority class\n",
    "                  minoritySize    size of the minority class\n",
    "\n",
    "        Returns: array of indexes that are of interest for a balanced dataset\n",
    "\n",
    "        Perform the actual balancing for a fold between SNPs and PDs\n",
    "    \"\"\"\n",
    "    usedLines = [False] * len(inData) #Array of false for length of data\n",
    "    for i in range(len(inData)):\n",
    "        if classData[i] == minClass:\n",
    "            usedLines[i] = True\n",
    "            \n",
    "    usedCount = 0\n",
    "    while usedCount < minSize:\n",
    "        i = rd.randrange(len(inData))\n",
    "        if usedLines[i] == False:\n",
    "            usedCount += 1\n",
    "            usedLines[i] = True       \n",
    "\n",
    "    return usedLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5c54edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(inData, classData, usedLines):\n",
    "    \"\"\"     Input:     inData      array of input training data\n",
    "                       classData   array of classes assigned to training data\n",
    "                       usedLines   array of line indexes to print\n",
    "\n",
    "            Returns:   input_balance  Array of balanced input training data\n",
    "                       label_balance  Array of balanced labels training data\n",
    "                       \n",
    "        Create array of the input training data and classes used. Converts from array to dataframe for CV code compatibility.\n",
    "        The index [i] is the identifier between the two arrays.\n",
    "    \"\"\"\n",
    "    input_balance = []\n",
    "    label_balance = []\n",
    "    \n",
    "    for i in range(len(inData)):\n",
    "        if usedLines[i]:\n",
    "            input_balance.append(inData[i])\n",
    "            label_balance.append(classData[i])  \n",
    "    \n",
    "#     Dataframe format\n",
    "#     label_balance = pd.DataFrame(label_balance, columns = ['Class'])\n",
    "    \n",
    "    return input_balance, label_balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27419a33",
   "metadata": {},
   "source": [
    "### Balance for n folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6746be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Balance_ratio(maxSize, minSize): \n",
    "    \"\"\" Input:      maxSize     The number of items in the majority class\n",
    "                    minSize     The number of items in the minority class\n",
    "\n",
    "        Returns:    BF          Number of balancing folds\n",
    "\n",
    "        Calculate the number of balancing folds needed using ratio of majority to minority class size. Double to ensure sufficient\n",
    "        majority class instances are sampled, then + 1 to make odd to allow weighted vote.\n",
    "    \"\"\"\n",
    "    Divide = maxSize/minSize\n",
    "    BF = (2 * round(Divide)) + 1 #Double ratio to nearest integer\n",
    "    return BF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12239dc9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Balance_Folds(BF, inData, classData, minClass, minSize):\n",
    "    \"\"\" Input:      BF                Number of balancing folds needed\n",
    "                    usedLines         Array of line indexes to print\n",
    "                    input_balance     Dataframe of balanced training data\n",
    "                    label_balance\n",
    "                    \n",
    "        Returns:    Input_folds       List of balanced arrays of training data\n",
    "                    Output_folds\n",
    "\n",
    "        Perform the balance_data() function n number of balancing fold times. Return lists for feature data and labels\n",
    "        where each item is the output of balance_data()\n",
    "    \"\"\"\n",
    "    Input_folds = []\n",
    "    Output_folds = []\n",
    "    fold_df = []\n",
    "\n",
    "    for i in range(BF):\n",
    "        usedLines = balance(inData, classData, minClass, minSize)\n",
    "        input_balance, label_balance = balance_data(inData, classData, usedLines)\n",
    "        \n",
    "        Input_folds.append(input_balance)\n",
    "        Output_folds.append(label_balance)\n",
    "        \n",
    "        df = pd.DataFrame(input_balance)\n",
    "        fold_df.append(df)\n",
    "        \n",
    "    with open('Balanced training data.txt', 'w') as f:\n",
    "        for number, fold in zip(range(BF), fold_df):\n",
    "            f.write(f\"Fold: {number}\\n\\n{fold}\\n\\n\\n\")\n",
    "            \n",
    "    return Input_folds, Output_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd1aaa",
   "metadata": {},
   "source": [
    "### RFC hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0840d9dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  def Hyperparameter(BF, Input_folds, Output_folds):\n",
    "#     \"\"\" Input:      BF                Number of balancing folds needed\n",
    "#                     Input_folds       List of 5 balanced arrays of training data\n",
    "#                     Output_folds      List of 5 balanced arrays of training data's labels\n",
    "\n",
    "#         Returns:    BF_RFC_HP         List of optimized hyperparameters for each RFC\n",
    "\n",
    "#         Perform RandomSearchCV on each RFC to optimize number of trees, max depth and max samples\n",
    "#     \"\"\"  \n",
    "#     estimator = RandomForestClassifier()\n",
    "#     param_grid = {\n",
    "#                 'n_estimators':np.arange(50,500,50),\n",
    "#                 'max_depth': np.arange(2, 10, 2),\n",
    "#                 'max_samples': np.arange(0.2, 1.2, 0.2)\n",
    "#                   }\n",
    "#     BF_RFC_HP = []\n",
    "\n",
    "#     for i in range(BF):\n",
    "#         HPtuning = RandomizedSearchCV(\n",
    "#             estimator,\n",
    "#             param_grid, \n",
    "#             scoring = 'balanced_accuracy',\n",
    "#             cv = 10,\n",
    "#             n_jobs = 6, #how many cores to run in parallel\n",
    "#             verbose = 2\n",
    "#             ).fit(Input_folds[i], Output_folds[i].ravel())\n",
    "#         BF_RFC_HP.append(HPtuning.best_params_)\n",
    "    \n",
    "#     return(BF_RFC_HP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796af0e8",
   "metadata": {},
   "source": [
    "### Train RFC on the trainings folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1decd7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BF_fitting(Input_folds, Output_folds): \n",
    "    \"\"\" Input:\n",
    "                    Input_train_list      List of 5 balanced arrays for training data\n",
    "                    Classes_train_list    List of 5 balanced arrays of training data labels\n",
    "        Returns:    BF_RFC                List of RFC's trained on data in each balancing fold\n",
    "\n",
    "        Create RFC model that returns probability predictions for each fold, using output of Balance_Folds() as training data\n",
    "    \"\"\"    \n",
    "    BF_RFC = []\n",
    "    for i in range(BF):\n",
    "        BF_RFC.append(RandomForestClassifier(verbose = 1, n_estimators = 1000)) #Generates a RFC for each fold's training data\n",
    "        BF_RFC[i].fit(Input_folds[i], Output_folds[i]) #Fits the RFC to each folds' training data\n",
    "        \n",
    "    return BF_RFC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd278c",
   "metadata": {},
   "source": [
    "#### Validate each RFC on validation set, for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acc41cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BF_validate(BF_RFC, ValData):\n",
    "    \"\"\" Input:  BF_RFC          List of RFC's trained on data in each balancing fold\n",
    "                Input_val_list  Unseen validation data fold from CV fold\n",
    "                \n",
    "        Returns:Prob_matrix     List of arrays. Each item is 2D matrix where the 1st dimension is each subset in balancing fold, \n",
    "                                2nd dimension is predicted probability\n",
    "    \n",
    "        Test the trained RFCs on the test set, then for every instance, outputs the predicted probability for each class\n",
    "    \"\"\"\n",
    "    \n",
    "    Prob_matrix = [] #Empty list\n",
    "    for i in range(len(BF_RFC)):\n",
    "        Prob = BF_RFC[i].predict_proba(ValData.values)\n",
    "        Prob_matrix.append(Prob)   \n",
    "            \n",
    "    with open('Validation fold probabilities.txt', 'w') as f:\n",
    "        for number, line in zip(range(BF), Prob_matrix ):\n",
    "            f.write(f\"Fold: {number}\\n\\n   SNP    PD\\n{line}\\n\\n\\n\")\n",
    "\n",
    "    return Prob_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b8fdd",
   "metadata": {},
   "source": [
    "### Weighted voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71033215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weighted_Vote(Prob_matrix, BF):\n",
    "    \"\"\" Input:      Prob_matrix     List of arrays. 2D matrix where the 1st dimension is each subset in balancing fold, \n",
    "                                    2nd dimension is predicted probability\n",
    "                    BF              Number of balancing folds\n",
    "\n",
    "        Returns:    Final_vote      Weighted vote classification\n",
    "\n",
    "        Calculate the final weighted vote using confidence scores (Sc). Binary classification formula Sc = 2|S0 - 0.5|\n",
    "    \"\"\"\n",
    "    Sc_PD = [] #Empty list\n",
    "    Sc_SNP = [] #Empty list\n",
    "    T = 0.45\n",
    "    for i in range(BF):\n",
    "        Sc_PD.append(2* (Prob_matrix[i][:,1] - 0.5)) #Confidence scores for PD, for each fold\n",
    "        Sc_SNP.append(2*(Prob_matrix[i][:,0] - 0.5)) #Confidence scores for SNP, for each fold\n",
    "\n",
    "    Sum_PD = np.sum(Sc_PD, axis = 0) #Sum of all PD confidence scores. 1D Array\n",
    "    Sum_SNP = np.sum(Sc_SNP, axis = 0) #Sum of all SNP confidence scores. 1D Array     \n",
    "    \n",
    "    Vote_arr = [] #Empty list\n",
    "\n",
    "    for i in range(len(Sum_PD)):\n",
    "        if Sum_PD[i] >= Sum_SNP[i]:\n",
    "            Vote_arr.append([1]) #Append PD classifications to list\n",
    "        elif Sum_SNP[i] > Sum_PD[i]:\n",
    "            Vote_arr.append([0]) #Append SNP classifications to list\n",
    "\n",
    "        Final_vote = np.stack(Vote_arr) #Converts list of arrays to a 2D array\n",
    "        Final_vote = Final_vote.ravel() #Flattens 2D array to 1D array\n",
    "\n",
    "    return(Final_vote, Sum_PD, Sum_SNP) #Returns the final confidence scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91ccea90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1439999999999999, 0.362, 0.266, 0.050000000000000044, 0.43400000000000005, 0.23199999999999998, 0.6299999999999999, 0.366, 0.29000000000000004, 0.07400000000000007, 0.6599999999999999, 0.71, 0.508, 0.8959999999999999, 0.256, 0.44999999999999996, 0.006000000000000005, 0.602, 0.16799999999999993, 0.6060000000000001, 0.42399999999999993, 0.02400000000000002, 0.28, 0.3979999999999999, 0.6579999999999999, 0.15399999999999991, 0.3740000000000001, 0.404, 0.45399999999999996, 0.486, 0.554, 0.18000000000000005, 0.20999999999999996, 0.5880000000000001, 0.352, 0.13, 0.20199999999999996, 0.534, 0.23199999999999998, 0.11199999999999999, 0.1459999999999999, 0.34199999999999997, 0.37, 0.512, 0.0020000000000000018, 0.126, 0.45399999999999996, 0.4079999999999999, 0.74, 0.6379999999999999, 0.1419999999999999, 0.482, 0.8919999999999999, 0.10400000000000009, 0.0040000000000000036, 0.29200000000000004, 0.1319999999999999, 0.17000000000000004, 0.246, 0.32599999999999996, 0.08600000000000008, 0.26, 0.20399999999999996, 0.1459999999999999, 0.20799999999999996, 0.6859999999999999, 0.43999999999999995, 0.5, 0.54, 0.17599999999999993, 0.5800000000000001, 0.132, 0.236, 0.44599999999999995, 0.506, 0.118, 0.3480000000000001, 0.64, 0.5, 0.352, 0.31200000000000006, 0.5800000000000001, 0.3440000000000001, 0.238, 0.1319999999999999, 0.09600000000000009, 0.06400000000000006, 0.16400000000000003, 0.016000000000000014, 0.45399999999999996, 0.06999999999999995, 0.33399999999999996, 0.1200000000000001, 0.752, 0.3620000000000001, 0.6779999999999999, 0.42600000000000005, 0.36, 0.506, 0.6719999999999999, 0.18200000000000005, 0.21399999999999997, 0.18400000000000005, 0.238, 0.20999999999999996, 0.3799999999999999, 0.6499999999999999, 0.04200000000000004, 0.3979999999999999, 0.21399999999999997, 0.49, 0.04600000000000004, 0.23399999999999999, 0.236, 0.6279999999999999, 0.358, 0.18599999999999994, 0.1459999999999999, 0.15599999999999992, 0.43000000000000005, 0.30400000000000005, 0.19199999999999995, 0.6160000000000001, 0.8700000000000001, 0.278, 0.0040000000000000036, 0.44999999999999996, 0.368, 0.3360000000000001, 0.44799999999999995, 0.040000000000000036, 0.5840000000000001, 0.3600000000000001, 0.20599999999999996, 0.16199999999999992, 0.544, 0.050000000000000044, 0.5640000000000001, 0.43599999999999994, 0.22399999999999998, 0.42200000000000004, 0.09600000000000009, 0.06800000000000006, 0.77, 0.20199999999999996, 0.614, 0.22799999999999998, 0.5920000000000001, 0.5700000000000001, 0.19199999999999995, 0.266, 0.56, 0.75, 0.02200000000000002, 0.20599999999999996, 0.348, 0.20199999999999996, 0.15800000000000003, 0.19199999999999995, 0.502, 0.474, 0.42400000000000004, 0.3460000000000001, 0.486, 0.3700000000000001, 0.794, 0.24, 0.704, 0.1259999999999999, 0.8200000000000001, 0.5900000000000001, 0.39, 0.1200000000000001, 0.788, 0.45799999999999996, 0.748, 0.052000000000000046, 0.51, 0.358, 0.6259999999999999, 0.358, 0.536, 0.362, 0.20799999999999996, 0.368, 0.392, 0.41999999999999993, 0.006000000000000005, 0.246, 0.19999999999999996, 0.28600000000000003, 0.132, 0.19599999999999995, 0.404, 0.33799999999999997, 0.09800000000000009, 0.6499999999999999, 0.06599999999999995, 0.536, 0.544, 0.16199999999999992, 0.31399999999999995, 0.32400000000000007, 0.43199999999999994, 0.25, 0.028000000000000025, 0.6499999999999999, 0.09599999999999997, 0.21399999999999997, 0.4079999999999999, 0.502, 0.4179999999999999, 0.052000000000000046, 0.45399999999999996, 0.05800000000000005, 0.3939999999999999, 0.1279999999999999, 0.040000000000000036, 0.04800000000000004, 0.026000000000000023, 0.17000000000000004, 0.132, 0.31799999999999995, 0.43999999999999995, 0.6419999999999999, 0.72, 0.252, 0.28200000000000003, 0.08800000000000008, 0.3640000000000001, 0.73, 0.6180000000000001, 0.3460000000000001, 0.6080000000000001, 0.56, 0.126, 0.558, 0.3540000000000001, 0.714, 0.3360000000000001, 0.838, 0.6200000000000001, 0.49, 0.254, 0.48, 0.16600000000000004, 0.29000000000000004, 0.4119999999999999, 0.18400000000000005, 0.258, 0.3340000000000001, 0.09600000000000009, 0.47, 0.050000000000000044, 0.13, 0.07799999999999996, 0.43400000000000005, 0.08400000000000007, 0.11399999999999999, 0.09400000000000008, 0.5660000000000001, 0.766, 0.3600000000000001, 0.6299999999999999, 0.30000000000000004, 0.3879999999999999, 0.5860000000000001, 0.05400000000000005, 0.622, 0.262, 0.03600000000000003, 0.21399999999999997, 0.17399999999999993, 0.44399999999999995, 0.14, 0.262, 0.22399999999999998, 0.128, 0.44799999999999995, 0.4099999999999999, 0.17200000000000004, 0.3620000000000001, 0.276, 0.29600000000000004, 0.606, 0.050000000000000044, 0.31599999999999995, 0.5820000000000001, 0.010000000000000009, 0.5720000000000001, 0.25, 0.556, 0.31799999999999995, 0.3400000000000001, 0.372, 0.778, 0.11199999999999999, 0.77, 0.30800000000000005, 0.3580000000000001, 0.17600000000000005, 0.54, 0.248, 0.29600000000000004, 0.404, 0.22599999999999998, 0.63, 0.3700000000000001, 0.6679999999999999, 0.5700000000000001, 0.15599999999999992, 0.6719999999999999, 0.3340000000000001, 0.19999999999999996, 0.21599999999999997, 0.47, 0.22999999999999998, 0.46399999999999997, 0.122, 0.806, 0.30000000000000004, 0.20199999999999996, 0.376, 0.374, 0.472, 0.02200000000000002, 0.014000000000000012, 0.248, 0.382, 0.47, 0.276, 0.388, 0.602, 0.47, 0.12200000000000011, 0.15399999999999991, 0.35, 0.27, 0.3879999999999999, 0.3979999999999999, 0.596, 0.49, 0.10999999999999999, 0.236, 0.20199999999999996, 0.1160000000000001, 0.6559999999999999, 0.03400000000000003, 0.346, 0.02400000000000002, 0.05400000000000005, 0.4179999999999999, 0.17400000000000004, 0.35, 0.33999999999999997, 0.32799999999999996, 0.254, 0.3959999999999999, 0.79, 0.24, 0.21199999999999997, 0.6519999999999999, 0.038000000000000034, 0.030000000000000027, 0.476, 0.5880000000000001, 0.16399999999999992, 0.04200000000000004, 0.28600000000000003, 0.484, 0.006000000000000005, 0.45399999999999996, 0.19999999999999996, 0.3819999999999999, 0.22599999999999998, 0.8700000000000001, 0.22399999999999998, 0.534, 0.384, 0.76, 0.368, 0.718, 0.264, 0.504, 0.16600000000000004, 0.522, 0.07599999999999996, 0.0040000000000000036, 0.20999999999999996, 0.694, 0.09799999999999998, 0.050000000000000044, 0.1339999999999999, 0.22999999999999998, 0.4059999999999999, 0.19999999999999996, 0.42999999999999994, 0.134, 0.4159999999999999, 0.498, 0.51, 0.24, 0.21999999999999997, 0.718, 0.6479999999999999, 0.4119999999999999, 0.9119999999999999, 0.21599999999999997, 0.42799999999999994, 0.624, 0.16000000000000003, 0.40800000000000003, 0.42399999999999993, 0.16200000000000003, 0.3540000000000001, 0.1299999999999999, 0.518, 0.478, 0.3859999999999999, 0.08400000000000007, 0.08400000000000007, 0.32600000000000007, 0.17000000000000004, 0.538, 0.20199999999999996, 0.372, 0.42799999999999994, 0.1140000000000001, 0.5820000000000001, 0.18600000000000005, 0.6759999999999999, 0.5980000000000001, 0.11399999999999999, 0.07599999999999996, 0.3340000000000001, 0.5660000000000001, 0.29600000000000004, 0.31200000000000006, 0.78, 0.20599999999999996, 0.6160000000000001, 0.492, 0.46799999999999997, 0.5720000000000001, 0.596, 0.538, 0.32000000000000006, 0.3580000000000001, 0.31599999999999995, 0.038000000000000034, 0.548, 0.6859999999999999, 0.5880000000000001, 0.31000000000000005, 0.28, 0.8240000000000001, 0.134, 0.272, 0.28200000000000003, 0.07000000000000006, 0.21799999999999997, 0.8680000000000001, 0.4, 0.3959999999999999, 0.19199999999999995, 0.25, 0.03400000000000003, 0.37, 0.534, 0.15200000000000002, 0.29400000000000004, 0.6519999999999999, 0.486, 0.27, 0.236, 0.31000000000000005, 0.278, 0.18199999999999994, 0.020000000000000018, 0.534, 0.018000000000000016, 0.4039999999999999, 0.264, 0.29800000000000004, 0.6579999999999999, 0.21999999999999997, 0.506, 0.44999999999999996, 0.23399999999999999, 0.45599999999999996, 0.07400000000000007, 0.08999999999999997, 0.45599999999999996, 0.0040000000000000036, 0.25, 0.31800000000000006, 0.722, 0.20599999999999996, 0.30600000000000005, 0.3879999999999999, 0.44799999999999995, 0.472, 0.354, 0.22599999999999998, 0.038000000000000034, 0.30200000000000005, 0.244, 0.746, 0.3720000000000001, 0.6259999999999999, 0.756, 0.3680000000000001, 0.246, 0.15600000000000003, 0.15599999999999992, 0.026000000000000023, 0.5880000000000001, 0.48, 0.1279999999999999, 0.6839999999999999, 0.392, 0.14800000000000002, 0.6319999999999999, 0.558, 0.29600000000000004, 0.33000000000000007, 0.1299999999999999, 0.30800000000000005, 0.16200000000000003, 0.5700000000000001, 0.236, 0.1279999999999999, 0.618, 0.696, 0.702, 0.524, 0.15999999999999992, 0.494]\n",
      "[0.3759999999999999, 0.28600000000000003, 0.3320000000000001, 0.028000000000000025, 0.482, 0.038000000000000034, 0.562, 0.41800000000000004, 0.1120000000000001, 0.05400000000000005, 0.722, 0.75, 0.5840000000000001, 0.8140000000000001, 0.134, 0.248, 0.4119999999999999, 0.488, 0.07799999999999996, 0.6140000000000001, 0.16399999999999992, 0.05400000000000005, 0.20199999999999996, 0.51, 0.44199999999999995, 0.20399999999999996, 0.3819999999999999, 0.526, 0.15000000000000002, 0.5820000000000001, 0.6180000000000001, 0.138, 0.3360000000000001, 0.536, 0.43999999999999995, 0.10599999999999998, 0.474, 0.42799999999999994, 0.42399999999999993, 0.052000000000000046, 0.04800000000000004, 0.11399999999999999, 0.45399999999999996, 0.45999999999999996, 0.028000000000000025, 0.07399999999999995, 0.3660000000000001, 0.46199999999999997, 0.716, 0.474, 0.08600000000000008, 0.5700000000000001, 0.8899999999999999, 0.0020000000000000018, 0.03200000000000003, 0.09399999999999997, 0.45199999999999996, 0.02400000000000002, 0.09399999999999997, 0.31599999999999995, 0.29800000000000004, 0.262, 0.17200000000000004, 0.04800000000000004, 0.14600000000000002, 0.54, 0.808, 0.16399999999999992, 0.5880000000000001, 0.3640000000000001, 0.45199999999999996, 0.14, 0.23199999999999998, 0.31599999999999995, 0.46599999999999997, 0.08999999999999997, 0.29400000000000004, 0.5700000000000001, 0.502, 0.20999999999999996, 0.29200000000000004, 0.508, 0.3320000000000001, 0.244, 0.07200000000000006, 0.1299999999999999, 0.09200000000000008, 0.274, 0.15000000000000002, 0.6779999999999999, 0.038000000000000034, 0.272, 0.15599999999999992, 0.768, 0.498, 0.5740000000000001, 0.45999999999999996, 0.392, 0.774, 0.702, 0.20399999999999996, 0.31800000000000006, 0.30400000000000005, 0.272, 0.10799999999999998, 0.278, 0.758, 0.16199999999999992, 0.3740000000000001, 0.27, 0.5900000000000001, 0.1259999999999999, 0.28600000000000003, 0.238, 0.738, 0.30000000000000004, 0.254, 0.1519999999999999, 0.17599999999999993, 0.496, 0.252, 0.17800000000000005, 0.528, 0.77, 0.348, 0.08200000000000007, 0.5660000000000001, 0.406, 0.3959999999999999, 0.396, 0.07799999999999996, 0.3939999999999999, 0.262, 0.16799999999999993, 0.10400000000000009, 0.65, 0.008000000000000007, 0.736, 0.272, 0.11399999999999999, 0.40800000000000003, 0.04200000000000004, 0.0020000000000000018, 0.8400000000000001, 0.246, 0.5660000000000001, 0.03200000000000003, 0.45599999999999996, 0.522, 0.15000000000000002, 0.266, 0.6619999999999999, 0.782, 0.1120000000000001, 0.1479999999999999, 0.268, 0.254, 0.11599999999999999, 0.1419999999999999, 0.45999999999999996, 0.486, 0.38, 0.3799999999999999, 0.516, 0.3700000000000001, 0.6379999999999999, 0.32600000000000007, 0.77, 0.248, 0.8140000000000001, 0.634, 0.30400000000000005, 0.014000000000000012, 0.6759999999999999, 0.4, 0.802, 0.04600000000000004, 0.482, 0.31599999999999995, 0.6659999999999999, 0.392, 0.386, 0.32799999999999996, 0.3440000000000001, 0.42600000000000005, 0.20399999999999996, 0.21199999999999997, 0.03200000000000003, 0.12, 0.06000000000000005, 0.3340000000000001, 0.236, 0.18799999999999994, 0.14, 0.386, 0.1279999999999999, 0.6839999999999999, 0.118, 0.32800000000000007, 0.522, 0.3400000000000001, 0.31799999999999995, 0.3979999999999999, 0.3420000000000001, 0.248, 0.016000000000000014, 0.6719999999999999, 0.0040000000000000036, 0.18200000000000005, 0.3420000000000001, 0.508, 0.43599999999999994, 0.1279999999999999, 0.45999999999999996, 0.15800000000000003, 0.3620000000000001, 0.3360000000000001, 0.08800000000000008, 0.19999999999999996, 0.06599999999999995, 0.23199999999999998, 0.10199999999999998, 0.372, 0.482, 0.498, 0.526, 0.1359999999999999, 0.1299999999999999, 0.05600000000000005, 0.45399999999999996, 0.506, 0.5920000000000001, 0.43399999999999994, 0.756, 0.476, 0.09799999999999998, 0.43999999999999995, 0.45799999999999996, 0.784, 0.3819999999999999, 0.756, 0.272, 0.3740000000000001, 0.15400000000000003, 0.3620000000000001, 0.07799999999999996, 0.32800000000000007, 0.3700000000000001, 0.19599999999999995, 0.20199999999999996, 0.3700000000000001, 0.1459999999999999, 0.532, 0.18199999999999994, 0.008000000000000007, 0.17200000000000004, 0.552, 0.03200000000000003, 0.10799999999999998, 0.1399999999999999, 0.546, 0.736, 0.10200000000000009, 0.6319999999999999, 0.32599999999999996, 0.256, 0.42399999999999993, 0.09399999999999997, 0.5820000000000001, 0.3839999999999999, 0.09400000000000008, 0.17199999999999993, 0.238, 0.12200000000000011, 0.020000000000000018, 0.32200000000000006, 0.19799999999999995, 0.11399999999999999, 0.49, 0.30400000000000005, 0.32599999999999996, 0.45399999999999996, 0.264, 0.31800000000000006, 0.472, 0.09999999999999998, 0.28, 0.542, 0.03200000000000003, 0.46599999999999997, 0.28200000000000003, 0.46599999999999997, 0.31999999999999995, 0.242, 0.5680000000000001, 0.77, 0.09999999999999998, 0.786, 0.42800000000000005, 0.30800000000000005, 0.11199999999999999, 0.716, 0.378, 0.22599999999999998, 0.31000000000000005, 0.276, 0.542, 0.3500000000000001, 0.656, 0.526, 0.1299999999999999, 0.722, 0.30000000000000004, 0.06399999999999995, 0.31399999999999995, 0.26, 0.16399999999999992, 0.472, 0.126, 0.8400000000000001, 0.36, 0.30200000000000005, 0.394, 0.45799999999999996, 0.618, 0.016000000000000014, 0.07200000000000006, 0.18200000000000005, 0.18999999999999995, 0.3779999999999999, 0.392, 0.33199999999999996, 0.5640000000000001, 0.41200000000000003, 0.09000000000000008, 0.31000000000000005, 0.11599999999999999, 0.3999999999999999, 0.3520000000000001, 0.32400000000000007, 0.5, 0.45999999999999996, 0.14800000000000002, 0.262, 0.08599999999999997, 0.04800000000000004, 0.542, 0.28200000000000003, 0.33999999999999997, 0.1120000000000001, 0.0040000000000000036, 0.536, 0.04800000000000004, 0.29400000000000004, 0.38, 0.22599999999999998, 0.248, 0.3959999999999999, 0.534, 0.28200000000000003, 0.16800000000000004, 0.694, 0.20799999999999996, 0.0040000000000000036, 0.46799999999999997, 0.6020000000000001, 0.17799999999999994, 0.08800000000000008, 0.30000000000000004, 0.3839999999999999, 0.15999999999999992, 0.5640000000000001, 0.22999999999999998, 0.3440000000000001, 0.32799999999999996, 0.76, 0.050000000000000044, 0.528, 0.42000000000000004, 0.6759999999999999, 0.32799999999999996, 0.6639999999999999, 0.242, 0.526, 0.28400000000000003, 0.4099999999999999, 0.08799999999999997, 0.09000000000000008, 0.05400000000000005, 0.518, 0.02200000000000002, 0.1439999999999999, 0.03400000000000003, 0.23199999999999998, 0.1299999999999999, 0.34199999999999997, 0.42799999999999994, 0.03600000000000003, 0.3620000000000001, 0.28800000000000003, 0.6180000000000001, 0.15399999999999991, 0.344, 0.734, 0.6819999999999999, 0.492, 0.9319999999999999, 0.3400000000000001, 0.6519999999999999, 0.716, 0.21199999999999997, 0.32599999999999996, 0.46199999999999997, 0.18400000000000005, 0.19999999999999996, 0.10200000000000009, 0.48, 0.32799999999999996, 0.17199999999999993, 0.062000000000000055, 0.1379999999999999, 0.3500000000000001, 0.16600000000000004, 0.484, 0.24, 0.256, 0.4119999999999999, 0.15799999999999992, 0.45199999999999996, 0.236, 0.8180000000000001, 0.6719999999999999, 0.09599999999999997, 0.050000000000000044, 0.31600000000000006, 0.49, 0.12, 0.3340000000000001, 0.8620000000000001, 0.25, 0.6499999999999999, 0.4059999999999999, 0.29200000000000004, 0.5860000000000001, 0.478, 0.43200000000000005, 0.236, 0.22399999999999998, 0.32799999999999996, 0.09799999999999998, 0.484, 0.648, 0.492, 0.352, 0.3879999999999999, 0.798, 0.026000000000000023, 0.22999999999999998, 0.14600000000000002, 0.09000000000000008, 0.11399999999999999, 0.762, 0.29600000000000004, 0.22999999999999998, 0.19599999999999995, 0.386, 0.03200000000000003, 0.31399999999999995, 0.5660000000000001, 0.18799999999999994, 0.3520000000000001, 0.552, 0.392, 0.256, 0.548, 0.28200000000000003, 0.22799999999999998, 0.5, 0.02200000000000002, 0.51, 0.04800000000000004, 0.238, 0.33799999999999997, 0.28800000000000003, 0.6579999999999999, 0.016000000000000014, 0.5980000000000001, 0.28, 0.348, 0.44799999999999995, 0.09600000000000009, 0.05800000000000005, 0.534, 0.04400000000000004, 0.18600000000000005, 0.42799999999999994, 0.69, 0.358, 0.09199999999999997, 0.256, 0.45999999999999996, 0.46199999999999997, 0.44199999999999995, 0.18799999999999994, 0.18399999999999994, 0.31000000000000005, 0.25, 0.696, 0.3640000000000001, 0.5980000000000001, 0.724, 0.268, 0.3879999999999999, 0.19399999999999995, 0.20999999999999996, 0.0040000000000000036, 0.79, 0.726, 0.20999999999999996, 0.6619999999999999, 0.44199999999999995, 0.018000000000000016, 0.43199999999999994, 0.6579999999999999, 0.3899999999999999, 0.27, 0.19399999999999995, 0.32199999999999995, 0.134, 0.6060000000000001, 0.31200000000000006, 0.038000000000000034, 0.554, 0.494, 0.46399999999999997, 0.44999999999999996, 0.1339999999999999, 0.532]\n",
      "[0.18999999999999995, 0.33799999999999997, 0.26, 0.07400000000000007, 0.526, 0.14600000000000002, 0.5920000000000001, 0.51, 0.32800000000000007, 0.18200000000000005, 0.53, 0.746, 0.6060000000000001, 0.808, 0.06799999999999995, 0.36, 0.4039999999999999, 0.552, 0.07000000000000006, 0.696, 0.10599999999999998, 0.10000000000000009, 0.22999999999999998, 0.42999999999999994, 0.714, 0.30400000000000005, 0.43999999999999995, 0.52, 0.32799999999999996, 0.45799999999999996, 0.3819999999999999, 0.06399999999999995, 0.23399999999999999, 0.544, 0.376, 0.09999999999999998, 0.512, 0.4139999999999999, 0.42999999999999994, 0.038000000000000034, 0.01200000000000001, 0.262, 0.30600000000000005, 0.6319999999999999, 0.17799999999999994, 0.136, 0.558, 0.3540000000000001, 0.6819999999999999, 0.6399999999999999, 0.05600000000000005, 0.5640000000000001, 0.9239999999999999, 0.12400000000000011, 0.04600000000000004, 0.13, 0.3680000000000001, 0.062000000000000055, 0.05400000000000005, 0.37, 0.32400000000000007, 0.272, 0.274, 0.008000000000000007, 0.08999999999999997, 0.642, 0.562, 0.30800000000000005, 0.514, 0.4179999999999999, 0.522, 0.17800000000000005, 0.31200000000000006, 0.41000000000000003, 0.5800000000000001, 0.15000000000000002, 0.3720000000000001, 0.474, 0.494, 0.42200000000000004, 0.31400000000000006, 0.56, 0.3600000000000001, 0.08800000000000008, 0.16799999999999993, 0.1299999999999999, 0.03200000000000003, 0.10999999999999999, 0.132, 0.5760000000000001, 0.04600000000000004, 0.274, 0.06600000000000006, 0.764, 0.5720000000000001, 0.622, 0.404, 0.30600000000000005, 0.6220000000000001, 0.708, 0.09799999999999998, 0.09000000000000008, 0.242, 0.238, 0.23399999999999999, 0.45199999999999996, 0.774, 0.020000000000000018, 0.28600000000000003, 0.242, 0.6639999999999999, 0.1339999999999999, 0.258, 0.21199999999999997, 0.6599999999999999, 0.272, 0.30200000000000005, 0.03200000000000003, 0.12200000000000011, 0.48, 0.246, 0.20999999999999996, 0.706, 0.8220000000000001, 0.44199999999999995, 0.09000000000000008, 0.5660000000000001, 0.388, 0.3500000000000001, 0.43999999999999995, 0.08400000000000007, 0.5960000000000001, 0.30000000000000004, 0.42799999999999994, 0.17799999999999994, 0.63, 0.008000000000000007, 0.6739999999999999, 0.514, 0.11399999999999999, 0.378, 0.016000000000000014, 0.05600000000000005, 0.748, 0.16000000000000003, 0.524, 0.262, 0.646, 0.43600000000000005, 0.32599999999999996, 0.33799999999999997, 0.622, 0.712, 0.1379999999999999, 0.22199999999999998, 0.258, 0.1359999999999999, 0.15400000000000003, 0.1499999999999999, 0.368, 0.502, 0.37, 0.3400000000000001, 0.45999999999999996, 0.45599999999999996, 0.81, 0.44199999999999995, 0.78, 0.1080000000000001, 0.74, 0.552, 0.36, 0.030000000000000027, 0.698, 0.42400000000000004, 0.8720000000000001, 0.08000000000000007, 0.634, 0.608, 0.6399999999999999, 0.45399999999999996, 0.612, 0.19199999999999995, 0.29600000000000004, 0.33799999999999997, 0.46199999999999997, 0.32600000000000007, 0.014000000000000012, 0.388, 0.16000000000000003, 0.07400000000000007, 0.21399999999999997, 0.010000000000000009, 0.18999999999999995, 0.42000000000000004, 0.07400000000000007, 0.6000000000000001, 0.1160000000000001, 0.5720000000000001, 0.5780000000000001, 0.3700000000000001, 0.402, 0.4119999999999999, 0.514, 0.272, 0.07799999999999996, 0.6040000000000001, 0.11199999999999999, 0.11399999999999999, 0.512, 0.51, 0.3740000000000001, 0.1279999999999999, 0.43200000000000005, 0.07799999999999996, 0.474, 0.3799999999999999, 0.07799999999999996, 0.30000000000000004, 0.10399999999999998, 0.08399999999999996, 0.15600000000000003, 0.38, 0.4119999999999999, 0.3799999999999999, 0.6599999999999999, 0.254, 0.22199999999999998, 0.126, 0.494, 0.72, 0.562, 0.45999999999999996, 0.6519999999999999, 0.546, 0.02400000000000002, 0.44799999999999995, 0.524, 0.6419999999999999, 0.3919999999999999, 0.768, 0.6419999999999999, 0.546, 0.19399999999999995, 0.272, 0.18600000000000005, 0.23399999999999999, 0.48, 0.28, 0.29000000000000004, 0.28, 0.014000000000000012, 0.512, 0.008000000000000007, 0.10200000000000009, 0.050000000000000044, 0.502, 0.04200000000000004, 0.16800000000000004, 0.026000000000000023, 0.53, 0.6719999999999999, 0.03600000000000003, 0.3799999999999999, 0.43799999999999994, 0.43799999999999994, 0.55, 0.09400000000000008, 0.548, 0.3839999999999999, 0.02200000000000002, 0.22799999999999998, 0.268, 0.236, 0.07800000000000007, 0.32000000000000006, 0.23399999999999999, 0.236, 0.41600000000000004, 0.3480000000000001, 0.272, 0.28600000000000003, 0.20399999999999996, 0.3500000000000001, 0.712, 0.052000000000000046, 0.42000000000000004, 0.56, 0.026000000000000023, 0.5920000000000001, 0.268, 0.5940000000000001, 0.278, 0.4079999999999999, 0.5720000000000001, 0.794, 0.08200000000000007, 0.708, 0.36, 0.22799999999999998, 0.14600000000000002, 0.5860000000000001, 0.376, 0.17400000000000004, 0.21999999999999997, 0.24, 0.5640000000000001, 0.248, 0.624, 0.538, 0.006000000000000005, 0.736, 0.264, 0.13, 0.376, 0.31400000000000006, 0.0, 0.45999999999999996, 0.038000000000000034, 0.8, 0.28400000000000003, 0.16200000000000003, 0.39, 0.51, 0.56, 0.03600000000000003, 0.050000000000000044, 0.14600000000000002, 0.14800000000000002, 0.31200000000000006, 0.20799999999999996, 0.22999999999999998, 0.552, 0.356, 0.17199999999999993, 0.4079999999999999, 0.28, 0.30800000000000005, 0.506, 0.3580000000000001, 0.626, 0.4079999999999999, 0.04600000000000004, 0.22199999999999998, 0.128, 0.09000000000000008, 0.6220000000000001, 0.1479999999999999, 0.31799999999999995, 0.09999999999999998, 0.10799999999999998, 0.3999999999999999, 0.12, 0.41000000000000003, 0.358, 0.382, 0.038000000000000034, 0.4019999999999999, 0.6319999999999999, 0.20599999999999996, 0.23399999999999999, 0.6599999999999999, 0.21199999999999997, 0.0020000000000000018, 0.44599999999999995, 0.744, 0.254, 0.050000000000000044, 0.32199999999999995, 0.44599999999999995, 0.06000000000000005, 0.45999999999999996, 0.18799999999999994, 0.21599999999999997, 0.30800000000000005, 0.742, 0.01200000000000001, 0.5880000000000001, 0.406, 0.516, 0.43400000000000005, 0.6639999999999999, 0.262, 0.484, 0.29000000000000004, 0.3320000000000001, 0.18999999999999995, 0.08999999999999997, 0.050000000000000044, 0.6399999999999999, 0.03600000000000003, 0.12400000000000011, 0.1080000000000001, 0.14, 0.278, 0.22999999999999998, 0.3819999999999999, 0.050000000000000044, 0.5820000000000001, 0.3879999999999999, 0.6000000000000001, 0.30200000000000005, 0.29800000000000004, 0.652, 0.69, 0.44199999999999995, 0.9119999999999999, 0.28200000000000003, 0.46799999999999997, 0.594, 0.23199999999999998, 0.494, 0.32000000000000006, 0.21999999999999997, 0.3759999999999999, 0.22999999999999998, 0.548, 0.6, 0.3759999999999999, 0.1479999999999999, 0.16799999999999993, 0.29400000000000004, 0.29200000000000004, 0.69, 0.368, 0.344, 0.478, 0.052000000000000046, 0.526, 0.266, 0.806, 0.7, 0.19799999999999995, 0.016000000000000014, 0.272, 0.556, 0.252, 0.238, 0.744, 0.17999999999999994, 0.716, 0.522, 0.3600000000000001, 0.484, 0.5720000000000001, 0.522, 0.3759999999999999, 0.21199999999999997, 0.36, 0.248, 0.534, 0.616, 0.6259999999999999, 0.402, 0.3580000000000001, 0.8340000000000001, 0.134, 0.06399999999999995, 0.21999999999999997, 0.1140000000000001, 0.10799999999999998, 0.8500000000000001, 0.10599999999999998, 0.47, 0.492, 0.28800000000000003, 0.09399999999999997, 0.28600000000000003, 0.504, 0.17399999999999993, 0.3340000000000001, 0.6819999999999999, 0.32599999999999996, 0.08000000000000007, 0.494, 0.3799999999999999, 0.17399999999999993, 0.4179999999999999, 0.118, 0.4039999999999999, 0.22599999999999998, 0.31200000000000006, 0.14800000000000002, 0.29200000000000004, 0.6499999999999999, 0.15799999999999992, 0.4039999999999999, 0.376, 0.28800000000000003, 0.534, 0.04200000000000004, 0.04600000000000004, 0.44199999999999995, 0.1200000000000001, 0.18000000000000005, 0.546, 0.6399999999999999, 0.254, 0.246, 0.09200000000000008, 0.42799999999999994, 0.45199999999999996, 0.44999999999999996, 0.1519999999999999, 0.08600000000000008, 0.3700000000000001, 0.41999999999999993, 0.806, 0.31600000000000006, 0.702, 0.692, 0.246, 0.29400000000000004, 0.24, 0.1120000000000001, 0.236, 0.774, 0.494, 0.028000000000000025, 0.6299999999999999, 0.504, 0.07799999999999996, 0.502, 0.5880000000000001, 0.3759999999999999, 0.3959999999999999, 0.21999999999999997, 0.256, 0.124, 0.4039999999999999, 0.262, 0.18799999999999994, 0.498, 0.494, 0.55, 0.354, 0.20199999999999996, 0.49]\n",
      "[0.08200000000000007, 0.37, 0.4039999999999999, 0.1379999999999999, 0.352, 0.09599999999999997, 0.6120000000000001, 0.43600000000000005, 0.264, 0.07999999999999996, 0.6299999999999999, 0.7, 0.56, 0.802, 0.22199999999999998, 0.32599999999999996, 0.3380000000000001, 0.42400000000000004, 0.29800000000000004, 0.6799999999999999, 0.1419999999999999, 0.18799999999999994, 0.354, 0.3360000000000001, 0.482, 0.15599999999999992, 0.558, 0.512, 0.502, 0.542, 0.45599999999999996, 0.15600000000000003, 0.33000000000000007, 0.6799999999999999, 0.45199999999999996, 0.22199999999999998, 0.4179999999999999, 0.42199999999999993, 0.3740000000000001, 0.10999999999999999, 0.03400000000000003, 0.13, 0.262, 0.502, 0.07600000000000007, 0.01200000000000001, 0.6679999999999999, 0.4019999999999999, 0.6659999999999999, 0.6359999999999999, 0.16599999999999993, 0.508, 0.9019999999999999, 0.1519999999999999, 0.038000000000000034, 0.14400000000000002, 0.21399999999999997, 0.07799999999999996, 0.18999999999999995, 0.36, 0.20399999999999996, 0.32599999999999996, 0.244, 0.014000000000000012, 0.254, 0.628, 0.6619999999999999, 0.3660000000000001, 0.488, 0.3340000000000001, 0.49, 0.08599999999999997, 0.08600000000000008, 0.43400000000000005, 0.522, 0.17200000000000004, 0.42799999999999994, 0.546, 0.5820000000000001, 0.32999999999999996, 0.17999999999999994, 0.484, 0.17599999999999993, 0.256, 0.1439999999999999, 0.10400000000000009, 0.07799999999999996, 0.22799999999999998, 0.08799999999999997, 0.498, 0.03200000000000003, 0.31399999999999995, 0.242, 0.77, 0.516, 0.628, 0.526, 0.266, 0.5820000000000001, 0.708, 0.07999999999999996, 0.08600000000000008, 0.34199999999999997, 0.20599999999999996, 0.08999999999999997, 0.4159999999999999, 0.69, 0.028000000000000025, 0.20599999999999996, 0.21599999999999997, 0.6679999999999999, 0.16799999999999993, 0.21999999999999997, 0.28, 0.6180000000000001, 0.384, 0.20999999999999996, 0.1319999999999999, 0.258, 0.42600000000000005, 0.3420000000000001, 0.244, 0.702, 0.8580000000000001, 0.30200000000000005, 0.1499999999999999, 0.46399999999999997, 0.514, 0.28800000000000003, 0.44199999999999995, 0.1100000000000001, 0.6739999999999999, 0.09400000000000008, 0.27, 0.1060000000000001, 0.6719999999999999, 0.06000000000000005, 0.6060000000000001, 0.502, 0.42000000000000004, 0.636, 0.10399999999999998, 0.14200000000000002, 0.794, 0.236, 0.526, 0.27, 0.5920000000000001, 0.43200000000000005, 0.20799999999999996, 0.14800000000000002, 0.522, 0.734, 0.15999999999999992, 0.09400000000000008, 0.28, 0.49, 0.21399999999999997, 0.08799999999999997, 0.5, 0.5, 0.31599999999999995, 0.29000000000000004, 0.6020000000000001, 0.5880000000000001, 0.8180000000000001, 0.22799999999999998, 0.71, 0.0040000000000000036, 0.8180000000000001, 0.628, 0.21199999999999997, 0.030000000000000027, 0.6819999999999999, 0.42400000000000004, 0.8160000000000001, 0.028000000000000025, 0.484, 0.42600000000000005, 0.6339999999999999, 0.388, 0.40800000000000003, 0.278, 0.278, 0.356, 0.29400000000000004, 0.472, 0.030000000000000027, 0.496, 0.11199999999999999, 0.1100000000000001, 0.252, 0.1060000000000001, 0.0020000000000000018, 0.35, 0.07200000000000006, 0.6439999999999999, 0.08999999999999997, 0.484, 0.482, 0.32600000000000007, 0.31000000000000005, 0.30600000000000005, 0.45399999999999996, 0.262, 0.02200000000000002, 0.742, 0.030000000000000027, 0.17800000000000005, 0.3540000000000001, 0.43799999999999994, 0.3440000000000001, 0.16399999999999992, 0.358, 0.26, 0.5780000000000001, 0.15999999999999992, 0.09199999999999997, 0.19399999999999995, 0.040000000000000036, 0.262, 0.06000000000000005, 0.31799999999999995, 0.556, 0.46199999999999997, 0.6439999999999999, 0.18799999999999994, 0.09800000000000009, 0.19199999999999995, 0.518, 0.6499999999999999, 0.6499999999999999, 0.49, 0.6279999999999999, 0.46199999999999997, 0.14, 0.386, 0.43799999999999994, 0.6859999999999999, 0.31600000000000006, 0.77, 0.278, 0.44999999999999996, 0.266, 0.5740000000000001, 0.11199999999999999, 0.3939999999999999, 0.54, 0.24, 0.362, 0.4139999999999999, 0.050000000000000044, 0.43999999999999995, 0.04600000000000004, 0.06400000000000006, 0.0040000000000000036, 0.6859999999999999, 0.1319999999999999, 0.06399999999999995, 0.268, 0.44399999999999995, 0.742, 0.272, 0.492, 0.30400000000000005, 0.242, 0.518, 0.08400000000000007, 0.482, 0.22599999999999998, 0.252, 0.1100000000000001, 0.258, 0.42599999999999993, 0.19599999999999995, 0.3520000000000001, 0.20399999999999996, 0.030000000000000027, 0.33999999999999997, 0.45399999999999996, 0.268, 0.43199999999999994, 0.236, 0.29600000000000004, 0.5780000000000001, 0.026000000000000023, 0.27, 0.6240000000000001, 0.04400000000000004, 0.762, 0.32799999999999996, 0.688, 0.20599999999999996, 0.5780000000000001, 0.5900000000000001, 0.74, 0.014000000000000012, 0.752, 0.32999999999999996, 0.3340000000000001, 0.01200000000000001, 0.6279999999999999, 0.42400000000000004, 0.19799999999999995, 0.394, 0.3819999999999999, 0.508, 0.242, 0.602, 0.6479999999999999, 0.07999999999999996, 0.778, 0.246, 0.19599999999999995, 0.28, 0.3520000000000001, 0.02200000000000002, 0.5, 0.0020000000000000018, 0.764, 0.25, 0.366, 0.5700000000000001, 0.43200000000000005, 0.518, 0.052000000000000046, 0.07800000000000007, 0.21599999999999997, 0.038000000000000034, 0.3959999999999999, 0.15000000000000002, 0.43600000000000005, 0.518, 0.386, 0.03400000000000003, 0.18999999999999995, 0.17600000000000005, 0.31800000000000006, 0.3799999999999999, 0.21799999999999997, 0.46199999999999997, 0.44199999999999995, 0.10599999999999998, 0.15999999999999992, 0.18999999999999995, 0.17799999999999994, 0.6699999999999999, 0.1299999999999999, 0.474, 0.050000000000000044, 0.09599999999999997, 0.512, 0.10199999999999998, 0.33599999999999997, 0.26, 0.272, 0.19199999999999995, 0.33000000000000007, 0.51, 0.42999999999999994, 0.236, 0.734, 0.18200000000000005, 0.04600000000000004, 0.32599999999999996, 0.6779999999999999, 0.1259999999999999, 0.10799999999999998, 0.384, 0.48, 0.05800000000000005, 0.43400000000000005, 0.22799999999999998, 0.256, 0.22999999999999998, 0.696, 0.04400000000000004, 0.5740000000000001, 0.41600000000000004, 0.8, 0.33399999999999996, 0.632, 0.19999999999999996, 0.5900000000000001, 0.45799999999999996, 0.3660000000000001, 0.10199999999999998, 0.18999999999999995, 0.02200000000000002, 0.6319999999999999, 0.04200000000000004, 0.07399999999999995, 0.21199999999999997, 0.244, 0.18599999999999994, 0.29400000000000004, 0.43999999999999995, 0.04600000000000004, 0.46799999999999997, 0.3360000000000001, 0.546, 0.28200000000000003, 0.20599999999999996, 0.704, 0.6499999999999999, 0.45599999999999996, 0.8979999999999999, 0.31600000000000006, 0.46399999999999997, 0.638, 0.20999999999999996, 0.494, 0.46199999999999997, 0.27, 0.6080000000000001, 0.08400000000000007, 0.398, 0.402, 0.19999999999999996, 0.04600000000000004, 0.17199999999999993, 0.16399999999999992, 0.22799999999999998, 0.6579999999999999, 0.24, 0.43600000000000005, 0.6259999999999999, 0.07400000000000007, 0.42599999999999993, 0.246, 0.8240000000000001, 0.6739999999999999, 0.18000000000000005, 0.052000000000000046, 0.4139999999999999, 0.5740000000000001, 0.22999999999999998, 0.3759999999999999, 0.798, 0.1319999999999999, 0.754, 0.4159999999999999, 0.28600000000000003, 0.5860000000000001, 0.508, 0.41800000000000004, 0.3779999999999999, 0.28600000000000003, 0.23399999999999999, 0.124, 0.42600000000000005, 0.46199999999999997, 0.6319999999999999, 0.404, 0.30200000000000005, 0.75, 0.014000000000000012, 0.23199999999999998, 0.11199999999999999, 0.3400000000000001, 0.22999999999999998, 0.716, 0.04800000000000004, 0.3779999999999999, 0.16799999999999993, 0.248, 0.10199999999999998, 0.552, 0.42000000000000004, 0.21199999999999997, 0.45399999999999996, 0.766, 0.354, 0.28400000000000003, 0.478, 0.3420000000000001, 0.31400000000000006, 0.31600000000000006, 0.08199999999999996, 0.46199999999999997, 0.07600000000000007, 0.31200000000000006, 0.15200000000000002, 0.43799999999999994, 0.6100000000000001, 0.1160000000000001, 0.506, 0.29800000000000004, 0.242, 0.4, 0.15999999999999992, 0.0040000000000000036, 0.42200000000000004, 0.07200000000000006, 0.09199999999999997, 0.45399999999999996, 0.798, 0.33199999999999996, 0.248, 0.07400000000000007, 0.546, 0.46799999999999997, 0.392, 0.238, 0.16599999999999993, 0.556, 0.262, 0.812, 0.3740000000000001, 0.6519999999999999, 0.76, 0.44399999999999995, 0.20799999999999996, 0.010000000000000009, 0.038000000000000034, 0.18200000000000005, 0.794, 0.6180000000000001, 0.23199999999999998, 0.8160000000000001, 0.396, 0.15200000000000002, 0.46199999999999997, 0.76, 0.3500000000000001, 0.17599999999999993, 0.03200000000000003, 0.36, 0.19199999999999995, 0.526, 0.21799999999999997, 0.21599999999999997, 0.42800000000000005, 0.6739999999999999, 0.69, 0.486, 0.21599999999999997, 0.46399999999999997]\n",
      "[0.28800000000000003, 0.258, 0.31400000000000006, 0.17999999999999994, 0.472, 0.28400000000000003, 0.694, 0.28400000000000003, 0.272, 0.18799999999999994, 0.512, 0.706, 0.558, 0.766, 0.08799999999999997, 0.396, 0.3480000000000001, 0.5780000000000001, 0.16999999999999993, 0.71, 0.09599999999999997, 0.06999999999999995, 0.242, 0.51, 0.728, 0.22199999999999998, 0.43599999999999994, 0.47, 0.388, 0.3939999999999999, 0.6040000000000001, 0.026000000000000023, 0.3660000000000001, 0.3859999999999999, 0.45799999999999996, 0.052000000000000046, 0.494, 0.42599999999999993, 0.3959999999999999, 0.14400000000000002, 0.04400000000000004, 0.378, 0.344, 0.4079999999999999, 0.028000000000000025, 0.038000000000000034, 0.498, 0.3600000000000001, 0.65, 0.554, 0.32600000000000007, 0.45599999999999996, 0.8759999999999999, 0.05600000000000005, 0.21399999999999997, 0.266, 0.3700000000000001, 0.08399999999999996, 0.126, 0.348, 0.3560000000000001, 0.28600000000000003, 0.26, 0.026000000000000023, 0.368, 0.556, 0.6679999999999999, 0.4119999999999999, 0.6160000000000001, 0.45399999999999996, 0.532, 0.14200000000000002, 0.21399999999999997, 0.43999999999999995, 0.6619999999999999, 0.11599999999999999, 0.28600000000000003, 0.5800000000000001, 0.524, 0.21399999999999997, 0.25, 0.472, 0.3500000000000001, 0.3680000000000001, 0.1519999999999999, 0.018000000000000016, 0.16800000000000004, 0.20399999999999996, 0.17000000000000004, 0.484, 0.03600000000000003, 0.354, 0.1259999999999999, 0.702, 0.5800000000000001, 0.602, 0.504, 0.42600000000000005, 0.5800000000000001, 0.7, 0.246, 0.19999999999999996, 0.24, 0.1499999999999999, 0.10999999999999999, 0.28600000000000003, 0.764, 0.268, 0.3420000000000001, 0.1459999999999999, 0.6299999999999999, 0.278, 0.20599999999999996, 0.20999999999999996, 0.542, 0.33799999999999997, 0.28800000000000003, 0.05400000000000005, 0.10200000000000009, 0.392, 0.10200000000000009, 0.15800000000000003, 0.6160000000000001, 0.808, 0.404, 0.08800000000000008, 0.44399999999999995, 0.364, 0.44799999999999995, 0.514, 0.06600000000000006, 0.6399999999999999, 0.274, 0.272, 0.264, 0.6819999999999999, 0.21599999999999997, 0.558, 0.44799999999999995, 0.17200000000000004, 0.386, 0.10000000000000009, 0.08400000000000007, 0.734, 0.16200000000000003, 0.55, 0.22399999999999998, 0.618, 0.47, 0.29600000000000004, 0.132, 0.632, 0.74, 0.1100000000000001, 0.17999999999999994, 0.31399999999999995, 0.264, 0.14400000000000002, 0.006000000000000005, 0.43799999999999994, 0.49, 0.362, 0.45599999999999996, 0.538, 0.478, 0.752, 0.45199999999999996, 0.786, 0.05400000000000005, 0.69, 0.5660000000000001, 0.42600000000000005, 0.008000000000000007, 0.6040000000000001, 0.506, 0.726, 0.122, 0.5920000000000001, 0.43200000000000005, 0.71, 0.268, 0.644, 0.30200000000000005, 0.20599999999999996, 0.348, 0.31599999999999995, 0.3660000000000001, 0.05400000000000005, 0.33999999999999997, 0.12, 0.21599999999999997, 0.258, 0.052000000000000046, 0.33199999999999996, 0.378, 0.06800000000000006, 0.6020000000000001, 0.128, 0.494, 0.486, 0.236, 0.502, 0.42999999999999994, 0.42599999999999993, 0.22399999999999998, 0.11199999999999999, 0.752, 0.07400000000000007, 0.16200000000000003, 0.3879999999999999, 0.38, 0.498, 0.08400000000000007, 0.45199999999999996, 0.238, 0.3899999999999999, 0.3620000000000001, 0.0, 0.1180000000000001, 0.02400000000000002, 0.20999999999999996, 0.13, 0.366, 0.3799999999999999, 0.556, 0.5780000000000001, 0.16599999999999993, 0.30200000000000005, 0.016000000000000014, 0.41999999999999993, 0.506, 0.5960000000000001, 0.49, 0.6459999999999999, 0.536, 0.006000000000000005, 0.484, 0.47, 0.538, 0.44199999999999995, 0.78, 0.6339999999999999, 0.47, 0.246, 0.32800000000000007, 0.132, 0.3660000000000001, 0.3919999999999999, 0.21999999999999997, 0.10999999999999999, 0.4139999999999999, 0.06000000000000005, 0.5740000000000001, 0.19599999999999995, 0.08000000000000007, 0.19199999999999995, 0.494, 0.1379999999999999, 0.062000000000000055, 0.18399999999999994, 0.56, 0.724, 0.1120000000000001, 0.6739999999999999, 0.41200000000000003, 0.30400000000000005, 0.44399999999999995, 0.28200000000000003, 0.538, 0.16799999999999993, 0.02200000000000002, 0.1080000000000001, 0.258, 0.4019999999999999, 0.17400000000000004, 0.272, 0.16399999999999992, 0.14600000000000002, 0.366, 0.534, 0.18400000000000005, 0.42999999999999994, 0.23199999999999998, 0.42199999999999993, 0.602, 0.20599999999999996, 0.28600000000000003, 0.45599999999999996, 0.05400000000000005, 0.508, 0.13, 0.496, 0.352, 0.3560000000000001, 0.43600000000000005, 0.802, 0.07400000000000007, 0.6579999999999999, 0.51, 0.3740000000000001, 0.138, 0.5680000000000001, 0.33999999999999997, 0.30600000000000005, 0.356, 0.246, 0.624, 0.31800000000000006, 0.6619999999999999, 0.5820000000000001, 0.1499999999999999, 0.74, 0.30200000000000005, 0.29200000000000004, 0.382, 0.45999999999999996, 0.1519999999999999, 0.254, 0.236, 0.758, 0.26, 0.392, 0.29000000000000004, 0.352, 0.506, 0.030000000000000027, 0.1479999999999999, 0.28800000000000003, 0.28600000000000003, 0.502, 0.22999999999999998, 0.31999999999999995, 0.55, 0.516, 0.014000000000000012, 0.3899999999999999, 0.354, 0.6120000000000001, 0.3839999999999999, 0.17799999999999994, 0.618, 0.4019999999999999, 0.24, 0.4119999999999999, 0.15400000000000003, 0.15399999999999991, 0.6499999999999999, 0.18999999999999995, 0.32599999999999996, 0.06799999999999995, 0.028000000000000025, 0.42199999999999993, 0.15400000000000003, 0.55, 0.20799999999999996, 0.19999999999999996, 0.236, 0.4139999999999999, 0.3740000000000001, 0.32600000000000007, 0.16000000000000003, 0.6779999999999999, 0.20599999999999996, 0.06600000000000006, 0.41800000000000004, 0.6040000000000001, 0.20799999999999996, 0.016000000000000014, 0.366, 0.522, 0.22199999999999998, 0.5, 0.138, 0.3480000000000001, 0.20599999999999996, 0.79, 0.05800000000000005, 0.498, 0.32599999999999996, 0.6459999999999999, 0.31599999999999995, 0.626, 0.242, 0.43000000000000005, 0.20199999999999996, 0.4079999999999999, 0.09399999999999997, 0.32600000000000007, 0.040000000000000036, 0.546, 0.04400000000000004, 0.1160000000000001, 0.1359999999999999, 0.29600000000000004, 0.3680000000000001, 0.374, 0.3979999999999999, 0.0020000000000000018, 0.45599999999999996, 0.3400000000000001, 0.554, 0.19199999999999995, 0.17800000000000005, 0.624, 0.5660000000000001, 0.3879999999999999, 0.8899999999999999, 0.15999999999999992, 0.472, 0.632, 0.19599999999999995, 0.278, 0.55, 0.19399999999999995, 0.3680000000000001, 0.17399999999999993, 0.52, 0.356, 0.07000000000000006, 0.08000000000000007, 0.238, 0.16199999999999992, 0.278, 0.6020000000000001, 0.394, 0.366, 0.3959999999999999, 0.17999999999999994, 0.3959999999999999, 0.28800000000000003, 0.774, 0.6339999999999999, 0.22199999999999998, 0.05400000000000005, 0.3779999999999999, 0.6559999999999999, 0.36, 0.3720000000000001, 0.756, 0.040000000000000036, 0.5900000000000001, 0.4139999999999999, 0.4139999999999999, 0.54, 0.598, 0.594, 0.252, 0.248, 0.30000000000000004, 0.10799999999999998, 0.528, 0.64, 0.492, 0.366, 0.3819999999999999, 0.71, 0.356, 0.262, 0.14600000000000002, 0.23199999999999998, 0.22599999999999998, 0.69, 0.266, 0.3680000000000001, 0.23199999999999998, 0.32599999999999996, 0.15999999999999992, 0.43799999999999994, 0.5700000000000001, 0.008000000000000007, 0.19799999999999995, 0.6279999999999999, 0.44399999999999995, 0.30400000000000005, 0.44199999999999995, 0.3540000000000001, 0.32800000000000007, 0.268, 0.29000000000000004, 0.492, 0.1499999999999999, 0.30200000000000005, 0.30600000000000005, 0.18200000000000005, 0.726, 0.3640000000000001, 0.488, 0.46199999999999997, 0.278, 0.368, 0.17999999999999994, 0.1459999999999999, 0.42800000000000005, 0.10799999999999998, 0.18400000000000005, 0.556, 0.6619999999999999, 0.31000000000000005, 0.344, 0.248, 0.4139999999999999, 0.384, 0.396, 0.22199999999999998, 0.07400000000000007, 0.3939999999999999, 0.4159999999999999, 0.806, 0.4099999999999999, 0.6779999999999999, 0.6379999999999999, 0.3420000000000001, 0.33000000000000007, 0.030000000000000027, 0.1259999999999999, 0.15200000000000002, 0.758, 0.538, 0.268, 0.722, 0.44799999999999995, 0.19599999999999995, 0.506, 0.732, 0.32800000000000007, 0.21999999999999997, 0.16399999999999992, 0.26, 0.11399999999999999, 0.506, 0.17399999999999993, 0.062000000000000055, 0.5900000000000001, 0.6200000000000001, 0.5660000000000001, 0.46199999999999997, 0.20599999999999996, 0.524]\n"
     ]
    }
   ],
   "source": [
    "T = 0.5\n",
    "for fold in range(BF):\n",
    "    Sc_list = []\n",
    "    for value in range(len(Prob_matrix[fold][:,0])):\n",
    "        S0 = Prob_matrix[fold][:,0][value] #each SNP value in prob matrix fold\n",
    "        if S0 < T:\n",
    "            Sc = (T - S0)/T\n",
    "        elif S0 >= T:\n",
    "            Sc = (S0 - T)/(1 - T)        \n",
    "        Sc_list.append(Sc)   \n",
    "    print(Sc_list)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f5b78",
   "metadata": {},
   "source": [
    "### Final confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ae1157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Final_score(Sum_PD, Sum_SNP, BF):\n",
    "    \"\"\" Input:      Sum_PD      Sum of confidence score for PD predictions\n",
    "                    Sum_SNP     Sum of confidence score for SNP predictions\n",
    "\n",
    "        Returns:    S_out       Final confidence score\n",
    "\n",
    "        Calculate the final confidence score\n",
    "    \"\"\"\n",
    "    \n",
    "    S_Out = np.abs((Sum_PD - Sum_SNP) /(BF*2))\n",
    "    np.savetxt('S_out.txt', S_Out, \"%.3f\")\n",
    "    \n",
    "    return S_Out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92f36545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evalutation(Vallabel, Final_vote, S_Out):\n",
    "    \"\"\" Input:      Classes_val_list   Validation label data\n",
    "                    Final_vote         Weighted vote classification\n",
    "\n",
    "        Evaluation metrics from RFC on validation data with\n",
    "    \"\"\"\n",
    "    Output_pred = Final_vote\n",
    "    print(f\"              ***Final Evaluation***\\n\")\n",
    "    print(f\"Confusion Matrix:\\n {confusion_matrix(Vallabel, Output_pred)}\")\n",
    "    print(f\"{classification_report(Vallabel, Output_pred)}\\nMCC                {matthews_corrcoef(Vallabel, Output_pred)}\")\n",
    "\n",
    "    print(f\"See file 'Classification.txt' for final classifications and confidence scores\")\n",
    "    np.savetxt('Classification.txt',\n",
    "           np.column_stack([Final_vote, S_Out]),\n",
    "           fmt = [\"%.0f\",\"%.3f\"],\n",
    "           delimiter =\"      \",\n",
    "           header = \"Final classifications and confidence scores\\n\\n\"\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa67e232",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a5a9897",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ***Final Evaluation***\n",
      "\n",
      "Confusion Matrix:\n",
      " [[150  26]\n",
      " [156 207]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.85      0.62       176\n",
      "           1       0.89      0.57      0.69       363\n",
      "\n",
      "    accuracy                           0.66       539\n",
      "   macro avg       0.69      0.71      0.66       539\n",
      "weighted avg       0.76      0.66      0.67       539\n",
      "\n",
      "MCC                0.3999621768014918\n",
      "See file 'Classification.txt' for final classifications and confidence scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ***Final Evaluation***\n",
      "\n",
      "Confusion Matrix:\n",
      " [[120  30]\n",
      " [196 193]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.80      0.52       150\n",
      "           1       0.87      0.50      0.63       389\n",
      "\n",
      "    accuracy                           0.58       539\n",
      "   macro avg       0.62      0.65      0.57       539\n",
      "weighted avg       0.73      0.58      0.60       539\n",
      "\n",
      "MCC                0.26948049419089404\n",
      "See file 'Classification.txt' for final classifications and confidence scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ***Final Evaluation***\n",
      "\n",
      "Confusion Matrix:\n",
      " [[147  46]\n",
      " [155 190]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.76      0.59       193\n",
      "           1       0.81      0.55      0.65       345\n",
      "\n",
      "    accuracy                           0.63       538\n",
      "   macro avg       0.65      0.66      0.62       538\n",
      "weighted avg       0.69      0.63      0.63       538\n",
      "\n",
      "MCC                0.3019365302352402\n",
      "See file 'Classification.txt' for final classifications and confidence scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ***Final Evaluation***\n",
      "\n",
      "Confusion Matrix:\n",
      " [[143  28]\n",
      " [140 227]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.84      0.63       171\n",
      "           1       0.89      0.62      0.73       367\n",
      "\n",
      "    accuracy                           0.69       538\n",
      "   macro avg       0.70      0.73      0.68       538\n",
      "weighted avg       0.77      0.69      0.70       538\n",
      "\n",
      "MCC                0.4241063648479094\n",
      "See file 'Classification.txt' for final classifications and confidence scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ***Final Evaluation***\n",
      "\n",
      "Confusion Matrix:\n",
      " [[153  37]\n",
      " [167 181]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.81      0.60       190\n",
      "           1       0.83      0.52      0.64       348\n",
      "\n",
      "    accuracy                           0.62       538\n",
      "   macro avg       0.65      0.66      0.62       538\n",
      "weighted avg       0.71      0.62      0.63       538\n",
      "\n",
      "MCC                0.31677544222777687\n",
      "See file 'Classification.txt' for final classifications and confidence scores\n"
     ]
    }
   ],
   "source": [
    "file = \"AC_dataset.csv\"\n",
    "Training_Set, Testing_Set, labels  = Train_Test_Split(file)\n",
    "IT_list, LT_list, IV_list, LV_list = CV(Training_Set)\n",
    "\n",
    "for i in range(len(IT_list)): #For every CV fold\n",
    "    classData                   = LT_list[i].to_numpy()\n",
    "    inData                      = IT_list[i].to_numpy()\n",
    "    ValData                     = IV_list[i]\n",
    "    Vallabel                    = LV_list[i]\n",
    "\n",
    "    minClass, minSize, maxSize  = find_minority_class(classData)\n",
    "    BF                          = Balance_ratio(maxSize, minSize)\n",
    "    Input_folds, Output_folds   = Balance_Folds(BF, inData, classData, minClass, minSize) #balance() and balance_data() functions are called under this\n",
    "\n",
    "    BF_RFC                      = BF_fitting(Input_folds, Output_folds)\n",
    "    Prob_matrix                 = BF_validate(BF_RFC, ValData)\n",
    "\n",
    "    Final_vote, Sum_PD, Sum_SNP = Weighted_Vote(Prob_matrix, BF)\n",
    "    S_Out                       = Final_score(Sum_PD, Sum_SNP, BF)\n",
    "\n",
    "    evalutation(Vallabel, Final_vote, S_Out)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f572db9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 337.844,
   "position": {
    "height": "359.844px",
    "left": "1536px",
    "right": "20px",
    "top": "112px",
    "width": "354px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
