{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fcba82d",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5737f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 2 is inbalanced data set; ~2200 in PD and ~1100 in SNP\n",
    "#Goal is to predict if mutation is SNP or PD\n",
    "#ImprovedBalancing branch\n",
    "\n",
    "#Imports the required libraries and packages\n",
    "import pandas as pd  #Import for data manipulation in dataframes\n",
    "import numpy as np  #Array manipulation and calculates mean\n",
    "\n",
    "import random as rd\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import(\n",
    "    matthews_corrcoef,  # CC for evaluation\n",
    "    f1_score,  #F1 score for evaluation\n",
    "    confusion_matrix,  #Creates the confusion matrix - stats on how accurate the test set output is\n",
    "    classification_report #Returns the F1 socre, precision, and recall of a prediction using a given model\n",
    "    )\n",
    "from sklearn.model_selection import(\n",
    "    train_test_split,  # Splits data frame into the training set and testing set\n",
    "    GridSearchCV,  # Cross validation to improve hyperparameters\n",
    "    StratifiedKFold\n",
    "        )\n",
    "from sklearn.ensemble import RandomForestClassifier #SK learn API for classificastion random forests\n",
    "from sklearn.tree import DecisionTreeClassifier #Single tree decisions \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier #allows for confidence scores to be predicted for each\n",
    "\n",
    "np.set_printoptions(threshold=np.inf) #full array printing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ffd97",
   "metadata": {},
   "source": [
    "## Random Seed function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6be2ee9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53907"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Random_Seed(): #Generates a random seed\n",
    "    random1 = rd.randrange(1,100) #Random integet between 1 and 100\n",
    "    random2 =  time.time() #Time since UTC epoch\n",
    "    Seed = int(random2//random1//1000)\n",
    "    return Seed\n",
    "Random_Seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddcc865",
   "metadata": {},
   "source": [
    "## Clean dataset in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62413d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 3368\n",
      "2254 PD samples\n",
      "1111 SNP samples\n"
     ]
    }
   ],
   "source": [
    "#Create, clean and convert dataset E2.csv to PD dataframe**\n",
    "df = pd.read_csv('E2.csv')  #Create PD data frame from .csv\n",
    "df.drop(['pdbcode:chain:resnum:mutation'], axis=1, inplace=True)  #Removes unrequired columns. PDBcode may be needed for manual validation \n",
    "df.columns = df.columns.str.replace(' ', '_')  # Removes any blank attributes\n",
    "df.replace(' ', '_', regex=True, inplace=True)  # Replace all blank spaces with underscore (none were present)\n",
    "df.reset_index(drop=True, inplace = True) #Resets index numbering from 0 and drops column\n",
    "Input = df.drop('dataset', axis =1).fillna('0') #DF of input instances for classification training. Unknown attributes assigned 0\n",
    "Output_encoded = pd.get_dummies(df, columns=['dataset']) #One hot encoding dataset column so \"PD\" and \"SNP\" attributes are numerical 0 or 1\n",
    "Output = Output_encoded['dataset_pd'].copy().astype('int32') #Dataframe with 1 column where 1 = PD, 0 = SNP, integer\n",
    "\n",
    "print(\"Total samples:\", len(df))\n",
    "Majority = len(df.loc[df['dataset'] == 'pd'])\n",
    "print(f\"{Majority} PD samples\")\n",
    "Minority = len(df.loc[df['dataset'] == 'snp'])\n",
    "print(f\"{Minority} SNP samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb451c9e",
   "metadata": {},
   "source": [
    "## Split into training and testing, generate RF (whole dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbfacd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    4.0s finished\n"
     ]
    }
   ],
   "source": [
    "Input_train, Input_test, Classes_train, Classes_test = train_test_split(Input, Output, train_size = 0.8, random_state=42, stratify=Output) #80% training and 20% testing split. Strartify ensures fixed poportion of output labels is in both sets. Input attributes and class labels, training attributes and class label etc\n",
    "start=time.time() #Start timer for inital training model building\n",
    "RFC = RandomForestClassifier(random_state = 42, n_estimators = 1000, verbose = 1) #Defines the Random Forest. 42 seeds, 1000 trees\n",
    "RFC.fit(Input_train, Classes_train) #Generates a random forest from the training data\n",
    "with open('Training Data.txt', 'w') as file: #Writes class labels for all instances to text file\n",
    "    file.write(Input_train.to_string())\n",
    "    \n",
    "with open('Class labels.txt', 'w') as file: #Writes class labels for all instances to text file\n",
    "    file.write(Classes_train.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8b7f5",
   "metadata": {},
   "source": [
    "### Training (revisit params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf8d9857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[145  78]\n",
      " [ 27 424]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.65      0.73       223\n",
      "           1       0.84      0.94      0.89       451\n",
      "\n",
      "    accuracy                           0.84       674\n",
      "   macro avg       0.84      0.80      0.81       674\n",
      "weighted avg       0.84      0.84      0.84       674\n",
      "\n",
      "MCC                0.6371468255225344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# StandardScaler().fit(X_train).transform(X_train) #Scales data \n",
    "# pipeline = make_pipeline( #Sets the random forest parameters\n",
    "#     StandardScaler(),\n",
    "#     LogisticRegression(solver='saga', max_iter=2000),\n",
    "#     verbose=2\n",
    "# )\n",
    "RFC.get_params()\n",
    "# Evaluation of training before weighted vote\n",
    "\n",
    "Output_pred = RFC.predict(Input_test) #Always perdict on the unseen test data, as train has been used by the estimastor\n",
    "print(f\"Confusion Matrix:\\n {confusion_matrix(Classes_test, Output_pred)}\")\n",
    "print(f\"{classification_report(Classes_test, Output_pred)}\\nMCC                {matthews_corrcoef(Classes_test, Output_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a335a585",
   "metadata": {},
   "source": [
    "# Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a89536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Minority_length(labels): #Finds the minority class size\n",
    "#     SNP = 0\n",
    "#     PD = 0\n",
    "#     for i in labels:\n",
    "#         if i == 0:\n",
    "#             SNP +=1\n",
    "#         if i == 1:\n",
    "#             PD +=1\n",
    "#     return SNP #returns the minority class length. [891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b6924e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_minority_class(classData):\n",
    "\n",
    "    Minority_count = 0\n",
    "    Majority_count = 0\n",
    "    for datum in classData:\n",
    "        if datum == 1:\n",
    "            Majority_count += 1\n",
    "        elif datum == 0:\n",
    "            Minority_count += 1\n",
    "\n",
    "    minClass = 0\n",
    "    minSize = Minority_count\n",
    "    maxSize = Majority_count\n",
    "    if Minority_count > Majority_count:\n",
    "        minClass = 1\n",
    "        minSize = Majority_count\n",
    "        maxSize = Minority_count\n",
    "\n",
    "    return minClass, minSize, maxSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d1241bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance(inData, classData, minClass, minSize):\n",
    "    \n",
    "    usedLines = [False] * len(inData) #sets used lines as 0\n",
    "    for i in range(len(inData)):\n",
    "        if classData[i] == minClass:\n",
    "            usedLines[i] = True\n",
    "        else:\n",
    "            usedLines[i] = False\n",
    "    usedCount = 0\n",
    "    while usedCount < minSize:\n",
    "        i = rd.randrange(len(inData))\n",
    "        if usedLines[i] == False:\n",
    "            usedCount += 1\n",
    "            usedLines[i] = True       \n",
    "\n",
    "    return usedLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5c54edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data(inData, classData, usedLines):\n",
    "    \"\"\" Input:  inData    array of input training data\n",
    "                classData array of classes assigned to training data\n",
    "                usedLines array of line indexes to print\n",
    "\n",
    "    Prints the selected lines\n",
    "    \"\"\"\n",
    "    Input_fold = []\n",
    "    Label_fold = []\n",
    "    for i in range(len(inData)):\n",
    "        if usedLines[i]:\n",
    "            Input_fold.append(inData[i])\n",
    "            Label_fold.append(classData[i])\n",
    "            \n",
    "    Input_fold = np.stack(Input_fold, axis =0)\n",
    "    Label_fold = np.stack(Label_fold, axis =0)\n",
    "    \n",
    "    return Input_fold, Label_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118067a2",
   "metadata": {},
   "source": [
    "### Find majority:minority ratio and number of balancing folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6746be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Balance_ratio(): #Finds ratio between the 2 classes (i.e the imbalance) and the number of folds required\n",
    "    Divide = maxSize/minSize\n",
    "    if round(Divide) % 2 == 0:\n",
    "        BF = 2 * round(Divide) + 1\n",
    "    else:\n",
    "        BF = round(Divide)\n",
    "    return BF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27419a33",
   "metadata": {},
   "source": [
    "### Define function arguments (find better method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12239dc9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Balance_Folds():\n",
    "    fold_input = []\n",
    "    fold_output = []\n",
    "    for fold in range(BF):\n",
    "        usedLines = balance(inData, classData, minClass, minSize)\n",
    "        Input, Output = print_data(inData, classData, usedLines)\n",
    "        fold_input.append(Input)\n",
    "        fold_output.append(Output)\n",
    "    return fold_input, fold_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afaa2926",
   "metadata": {},
   "outputs": [],
   "source": [
    "inData = pd.DataFrame(Input_train).to_numpy()\n",
    "classData = pd.DataFrame(Classes_train).to_numpy()\n",
    "minClass, minSize, maxSize = find_minority_class(classData)\n",
    "BF = Balance_ratio()\n",
    "\n",
    "fold_input, fold_output = Balance_Folds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea8505e",
   "metadata": {},
   "source": [
    "### Train balanced data on random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1decd7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BF_training(BF): #Creates a model that returns probability predictions for each fold, using Indices_list as input\n",
    "    \n",
    "    BF_RFC = RandomForestClassifier(random_state = 42, n_estimators = 1000, verbose = 1) #Defines the Random Forest. 42 seeds, 1000 trees\n",
    "    Prob_list = []\n",
    "    \n",
    "    for i in range(BF):\n",
    "        BF_RFC.fit(fold_input[i], fold_output[i].ravel()) #Generates a random forest for each fold's training data\n",
    "        Prob = BF_RFC.predict_proba(fold_input[i]) #Predicted class label from input training data\n",
    "        Prob_list.append(Prob)\n",
    "        \n",
    "        with open('Balanced probabilities.txt', 'w') as f:\n",
    "            for number, line in zip(range(BF), Prob_list):\n",
    "                f.write(f\"Fold: {number}\\n\\n{line}\\n\\n\\n\")\n",
    "        \n",
    "    return Prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e1ab758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "Prob_list = BF_training(BF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b8fdd",
   "metadata": {},
   "source": [
    "### Weighted voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71033215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Score(Instance, BF):\n",
    "    BF_prob_PD =[]\n",
    "    BF_prob_SNP =[]\n",
    "    for i in range(BF):\n",
    "        BF_prob_PD.append(Instance[i][:,1] - Instance[i][:,0]) #PD - SNP prob\n",
    "        BF_prob_SNP.append(Instance[i][:,0] - Instance[i][:,1]) #SNP - PD prob     \n",
    "    \n",
    "    PD_Sum = 0\n",
    "    SNP_Sum = 0\n",
    "    for i in range(BF):\n",
    "        PD_Sum += BF_prob_PD[i]\n",
    "        SNP_Sum += BF_prob_SNP[i]\n",
    "        \n",
    "    S_Out = np.abs((PD_Sum - SNP_Sum)/(BF * 2))\n",
    "    \n",
    "    np.savetxt('S_out.txt', S_Out, \"%.3f\")\n",
    "    \n",
    "    return(S_Out) #Returns the final confidence scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a322d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final vote\n",
    "def Final_vote(Value):\n",
    "    \n",
    "    SNP_Count = 0\n",
    "    PD_Count = 0\n",
    "    BalancedValues = []\n",
    "    for i in range(len(Value)):\n",
    "        if Value[i] > 0.5:\n",
    "            PD_Count += 1\n",
    "            BalancedValues.append(round(Value[i]))\n",
    "        elif Value[i] < 0.5:\n",
    "            SNP_Count += 1\n",
    "            BalancedValues.append(round(Value[i]))\n",
    "\n",
    "\n",
    "    final = print(f\"{PD_Count} samples predicted to be PD\\n{SNP_Count} samples predicted to be SNP\")\n",
    "    \n",
    "    return BalancedValues  \n",
    "    # #Evaluation of training after weighted vote\n",
    "    # Classes_pred = RFC.predict(Input_test)\n",
    "    # print(f\"Confusion Matrix:\\n {confusion_matrix(Classes_test, Classes_pred)}\")\n",
    "    # print(f\"MCC:\\n {matthews_corrcoef(Classes_test, Classes_pred)}\")\n",
    "    # print(\"F1:\\n\", f1_score(Classes_test, Classes_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4af5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: retrive the probability from each tree for a single sample  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ce6740d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324 samples predicted to be PD\n",
      "1455 samples predicted to be SNP\n"
     ]
    }
   ],
   "source": [
    "Value = Score(Instance = Prob_list, BF = BF)\n",
    "\n",
    "vote = Final_vote(Value)\n",
    "\n",
    "Array = np.stack(vote, axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69480519",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [674, 1779]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m Output_pred \u001b[38;5;241m=\u001b[39m Array \u001b[38;5;66;03m#Always perdict on the unseen test data, as train has been used by the estimastor\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfusion Matrix:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mClasses_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOutput_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclassification_report(Classes_test, Output_pred)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMCC                \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmatthews_corrcoef(Classes_test, Output_pred)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:307\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfusion_matrix\u001b[39m(\n\u001b[0;32m    223\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    224\u001b[0m ):\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 307\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:387\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    385\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    390\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [674, 1779]"
     ]
    }
   ],
   "source": [
    "Output_pred = Array #Always perdict on the unseen test data, as train has been used by the estimastor\n",
    "print(f\"Confusion Matrix:\\n {confusion_matrix(Classes_test, Output_pred)}\")\n",
    "print(f\"{classification_report(Classes_test, Output_pred)}\\nMCC                {matthews_corrcoef(Classes_test, Output_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965f8777",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426f23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # **Split data into training and test**\n",
    "# with open('SNPorPD.txt', 'w+') as f:\n",
    "#         data=f.read()\n",
    "#         f.write(str(y_test.to_string()))\n",
    "\n",
    "# # pipeline.fit(X, y) #applies list if transformers to give a fitted model\n",
    "\n",
    "# plt.scatter(Classes_test, Output_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f620e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch = GridSearchCV( #validation\n",
    "    estimator = LogisticRegression(solver='saga'),\n",
    "    param_grid = {}, #dictionary of parameters to search through\n",
    "    cv = StratifiedKFold(),\n",
    "    n_jobs = 1, #how many processors to run in parallel\n",
    "    scoring = 'f1',\n",
    "    verbose = 3 \n",
    "    ).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15af356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = clf.predict(X_test)\n",
    "# print(\"Training time:\", stop-start)\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "# print(\"MCC:\\n\", matthews_corrcoef(y_test, y_pred))\n",
    "# print(\"F1:\\n\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de8521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 337.844,
   "position": {
    "height": "359.844px",
    "left": "1536px",
    "right": "20px",
    "top": "112px",
    "width": "354px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
