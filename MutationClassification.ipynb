{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fcba82d",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d94d2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Example 2 is inbalanced data set; ~2200 in PD and ~1100 in SNP\\n    Goal is to predict if mutation is SNP or PD\\n    improve_MCC branch\\n    \\n    Total samples: 3368\\n    2254 PD samples\\n    1111 SNP samples\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Example 2 is inbalanced data set; ~2200 in PD and ~1100 in SNP\n",
    "    Goal is to predict if mutation is SNP or PD\n",
    "    improve_MCC branch\n",
    "    \n",
    "    Total samples: 3368\n",
    "    2254 PD samples\n",
    "    1111 SNP samples\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5737f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Imports the required libraries and packages \"\"\"\n",
    "\n",
    "import pandas as pd  # Data manipulation in dataframes\n",
    "import numpy as np  # Array manipulation\n",
    "\n",
    "import random as rd # Random seed generation\n",
    "import time #Time program run time\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch #CV visualise\n",
    "\n",
    "# import PDB2AC\n",
    "\n",
    "from sklearn.metrics import(\n",
    "    matthews_corrcoef,  # MCC for evaluation\n",
    "    balanced_accuracy_score, #hyperparameter evaluation\n",
    "    f1_score,  #hyperparameter evaluation\n",
    "    confusion_matrix,  # confusion matrix for classification evalutation\n",
    "    classification_report #Return the F1, precision, and recall of a prediction\n",
    "    )\n",
    "\n",
    "from sklearn.model_selection import(\n",
    "    train_test_split,  # Splits data frame into the training set and testing set\n",
    "    GridSearchCV,  # Searches all hyperparameters\n",
    "    RandomizedSearchCV, # Searches random range of hyperparameters\n",
    "    GroupKFold, # K-fold CV with as groups\n",
    "        )\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier #SK learn API for classificastion random forests\n",
    "from sklearn.tree import DecisionTreeClassifier #Single tree decisions \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier #allows for confidence scores to be predicted for each\n",
    "\n",
    "np.set_printoptions(precision = 3,threshold=np.inf, suppress=True) #full array printing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb451c9e",
   "metadata": {},
   "source": [
    "### Split dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbfacd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Test_Split(file):\n",
    "    \"\"\"      \n",
    "    Input:      file             Pre-processed dataset done by PDB2AC script\n",
    "\n",
    "    Returns:    Training_Set     80% training set split\n",
    "                Testing_Set      20% testing set split\n",
    "                labels           Class labels for training set\n",
    "\n",
    "    80% training and 20% testing split. Writes the data to txt files. Splits are shuffled randomly\n",
    "    \"\"\"\n",
    "    AC_dataset = pd.read_csv(file)    \n",
    "    Training_Set, Testing_Set = train_test_split(AC_dataset,train_size = 0.8)\n",
    "    labels = Training_Set['dataset_pd'].astype('int32')\n",
    "    Training_Set.reset_index(drop=True, inplace = True)\n",
    "    Testing_Set.reset_index(drop=True, inplace = True)\n",
    "    \n",
    "#     Training_file = Training_Set.drop(['Binding', 'SProtFT0', 'SProtFT1', 'SProtFT2', 'SProtFT3', 'SProtFT4', 'SProtFT5', 'SProtFT6', 'SProtFT7', 'SProtFT8', 'SProtFT9', 'SProtFT10', 'SProtFT11', 'SProtFT12', 'Interface', 'Relaccess', 'Impact', 'HBonds', 'SPhobic', 'CPhilic', 'BCharge', 'SSGeom', 'Voids', 'MLargest1', 'MLargest2', 'MLargest3', 'MLargest4', 'MLargest5', 'MLargest6', 'MLargest7', 'MLargest8', 'MLargest9', 'MLargest10', 'NLargest1', 'NLargest2', 'NLargest3', 'NLargest4', 'NLargest5', 'NLargest6', 'NLargest7', 'NLargest8', 'NLargest9', 'NLargest10', 'Clash', 'Glycine', 'Proline', 'CisPro'],axis=1)\n",
    "#     Testing_file = Testing_Set.drop(['Binding', 'SProtFT0', 'SProtFT1', 'SProtFT2', 'SProtFT3', 'SProtFT4', 'SProtFT5', 'SProtFT6', 'SProtFT7', 'SProtFT8', 'SProtFT9', 'SProtFT10', 'SProtFT11', 'SProtFT12', 'Interface', 'Relaccess', 'Impact', 'HBonds', 'SPhobic', 'CPhilic', 'BCharge', 'SSGeom', 'Voids', 'MLargest1', 'MLargest2', 'MLargest3', 'MLargest4', 'MLargest5', 'MLargest6', 'MLargest7', 'MLargest8', 'MLargest9', 'MLargest10', 'NLargest1', 'NLargest2', 'NLargest3', 'NLargest4', 'NLargest5', 'NLargest6', 'NLargest7', 'NLargest8', 'NLargest9', 'NLargest10', 'Clash', 'Glycine', 'Proline', 'CisPro'],axis=1)\n",
    "\n",
    "    with open('Training set.txt', 'w') as file: #Writes training data to files\n",
    "        file.write(Training_Set.to_string())\n",
    "    with open('Testing set.txt', 'w') as file: #Writes testing data to files\n",
    "        file.write(Testing_Set.to_string())\n",
    "\n",
    "\n",
    "    return Training_Set, Testing_Set, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8b7f5",
   "metadata": {},
   "source": [
    "### Initial evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf8d9857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(Initial_RFC, Input_test, Classes_test):\n",
    "#     \"\"\" Input:  Input_test      Features test data\n",
    "#                 Classes_test    Class label test data\n",
    "\n",
    "#         Evaluates the training data before balancing. Random forest classifier makes prediction using the test features. True values \n",
    "#         are the class labels testing data\n",
    "#     \"\"\"\n",
    "\n",
    "#     Output_pred = Initial_RFC.predict(Input_test) #Always perdict on the unseen test data, as train has been used by the estimastor\n",
    "#     print(f\"              **Initial Evaluation**\\n\")\n",
    "#     print(f\"Confusion Matrix:\\n {confusion_matrix(Classes_test, Output_pred)}\")\n",
    "#     print(f\"{classification_report(Classes_test, Output_pred)}\\nMCC                {matthews_corrcoef(Classes_test, Output_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b3a2df",
   "metadata": {},
   "source": [
    "## Group K-fold CV (outer loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "975ff775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV(Training_Set):\n",
    "    \"\"\"      \n",
    "    Input:      Training_Set     80% training set split\n",
    "            \n",
    "    Returns:    Input_train     Training features, for each fold\n",
    "                Classes_train   Ttraining classes, for each fold\n",
    "                Input_val       Validating features, for each fold\n",
    "                Classes_val     Validating classes, for each fold\n",
    "\n",
    "    Group K-fold CV that maintains protein groups, attempts to preserve number of samples of each class \n",
    "    for each fold, and ensures protein groups are separated. Creates 5 folds.\n",
    "    \"\"\"\n",
    "    \n",
    "    Input_CV       = Training_Set.drop(['dataset_pd'], axis =1)         #Input features for training\n",
    "    Output_CV      = Training_Set['dataset_pd'].copy().astype('int32')  #Class labels for training\n",
    "    Protein_Groups = Training_Set['AC Code'].to_list()                  #List of proteins for grouping\n",
    "        \n",
    "    CV             = GroupKFold(n_splits = 5)                           #Only shuffles proteins in each group, not groups in fold\n",
    "    \n",
    "    IT_list = []\n",
    "    LT_list = []\n",
    "    IV_list = []\n",
    "    LV_list = []\n",
    "    \n",
    "    for train_idx, val_idx in CV.split(Input_CV, Output_CV, Protein_Groups):\n",
    "        Rd = np.random.randint(time.time())                             #Random number from 1 to time since epoch\n",
    "\n",
    "        Input_train                        = Input_CV.loc[train_idx]\n",
    "        Classes_train                      = Output_CV.loc[train_idx]\n",
    "        Input_train.drop(['AC Code'], axis = 1, inplace = True)         #Group identifer not needed for training\n",
    "\n",
    "        Input_val                          = Input_CV.loc[val_idx]\n",
    "        Classes_val                        = Output_CV.loc[val_idx]\n",
    "        Input_val.drop(['AC Code'], axis   = 1, inplace = True)\n",
    "        \n",
    "        IT_list.append(Input_train.sample(frac=1, random_state=Rd)) \n",
    "        LT_list.append(Classes_train.sample(frac=1, random_state=Rd))\n",
    "        IV_list.append(Input_val.sample(frac=1, random_state=Rd))\n",
    "        LV_list.append(Classes_val.sample(frac=1, random_state=Rd))\n",
    "        \n",
    "    with open('CV validation data.txt', 'w') as f:\n",
    "        for number, fold in zip(range(len(LV_list)), LV_list):\n",
    "            f.write(f\"Fold: {number}\\n\\n{fold.to_string()}\\n\\n\\n\")\n",
    "\n",
    "    return(IT_list, LT_list, IV_list, LV_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a335a585",
   "metadata": {},
   "source": [
    "## Balancing (inner loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b6924e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_minority_class(classData):\n",
    "    \"\"\" Input:    classData  Array of class labels\n",
    "    \n",
    "        Returns:  minClass   The label for the minority class\n",
    "                  minSize    The number of items in the minority class\n",
    "                  maxSize    The number of items in the majority class\n",
    "\n",
    "        Finds information about the inbalance in class sizes\n",
    "    \"\"\"\n",
    "    \n",
    "    Minority_count = 0\n",
    "    Majority_count = 0\n",
    "    for datum in classData:\n",
    "        if datum == 1:\n",
    "            Majority_count += 1\n",
    "        elif datum == 0:\n",
    "            Minority_count += 1\n",
    "\n",
    "    minClass = 0\n",
    "    minSize = Minority_count\n",
    "    maxSize = Majority_count\n",
    "    if Minority_count > Majority_count:\n",
    "        minClass = 1\n",
    "        minSize = Majority_count\n",
    "        maxSize = Minority_count\n",
    "\n",
    "    return minClass, minSize, maxSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d1241bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance(inData, classData, minClass, minSize):\n",
    "    \"\"\" Input:    inData          array of input data\n",
    "                  classData       array of classes assigned\n",
    "                  minorityClass   class label for the minority class\n",
    "                  minoritySize    size of the minority class\n",
    "\n",
    "        Returns: array of indexes that are of interest for a balanced dataset\n",
    "\n",
    "        Perform the actual balancing for a fold between SNPs and PDs\n",
    "    \"\"\"\n",
    "    usedLines = [False] * len(inData) #Array of false for length of data\n",
    "    for i in range(len(inData)):\n",
    "        if classData[i] == minClass:\n",
    "            usedLines[i] = True\n",
    "            \n",
    "    usedCount = 0\n",
    "    while usedCount < minSize:\n",
    "        i = rd.randrange(len(inData))\n",
    "        if usedLines[i] == False:\n",
    "            usedCount += 1\n",
    "            usedLines[i] = True       \n",
    "\n",
    "    return usedLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5c54edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(inData, classData, usedLines):\n",
    "    \"\"\"     Input:     inData      array of input training data\n",
    "                       classData   array of classes assigned to training data\n",
    "                       usedLines   array of line indexes to print\n",
    "\n",
    "            Returns:   input_balance  Array of balanced input training data\n",
    "                       label_balance  Array of balanced labels training data\n",
    "                       \n",
    "        Create array of the input training data and classes used. Converts from array to dataframe for CV code compatibility.\n",
    "        The index [i] is the identifier between the two arrays.\n",
    "    \"\"\"\n",
    "    input_balance = []\n",
    "    label_balance = []\n",
    "    \n",
    "    for i in range(len(inData)):\n",
    "        if usedLines[i]:\n",
    "            input_balance.append(inData[i])\n",
    "            label_balance.append(classData[i])  \n",
    "    \n",
    "#     Dataframe format\n",
    "#     label_balance = pd.DataFrame(label_balance, columns = ['Class'])\n",
    "    \n",
    "    return input_balance, label_balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27419a33",
   "metadata": {},
   "source": [
    "### Balance for n folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6746be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Balance_ratio(maxSize, minSize): \n",
    "    \"\"\" Input:      maxSize     The number of items in the majority class\n",
    "                    minSize     The number of items in the minority class\n",
    "\n",
    "        Returns:    BF          Number of balancing folds\n",
    "\n",
    "        Calculate the number of balancing folds needed using ratio of majority to minority class size. Double to ensure sufficient\n",
    "        majority class instances are sampled, then + 1 to make odd to allow weighted vote.\n",
    "    \"\"\"\n",
    "    Divide = maxSize/minSize\n",
    "    BF = (2 * round(Divide)) + 1 #Double ratio to nearest integer\n",
    "    return BF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12239dc9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Balance_Folds(BF, inData, classData, minClass, minSize):\n",
    "    \"\"\" Input:      BF                Number of balancing folds needed\n",
    "                    usedLines         Array of line indexes to print\n",
    "                    input_balance     Dataframe of balanced training data\n",
    "                    label_balance\n",
    "                    \n",
    "        Returns:    Input_folds       List of balanced arrays of training data\n",
    "                    Output_folds\n",
    "\n",
    "        Perform the balance_data() function n number of balancing fold times. Return lists for feature data and labels\n",
    "        where each item is the output of balance_data()\n",
    "    \"\"\"\n",
    "    Input_folds = []\n",
    "    Output_folds = []\n",
    "    fold_df = []\n",
    "\n",
    "    for i in range(BF):\n",
    "        usedLines = balance(inData, classData, minClass, minSize)\n",
    "        input_balance, label_balance = balance_data(inData, classData, usedLines)\n",
    "        \n",
    "        Input_folds.append(input_balance)\n",
    "        Output_folds.append(label_balance)\n",
    "        \n",
    "        df = pd.DataFrame(input_balance)\n",
    "        fold_df.append(df)\n",
    "        \n",
    "    with open('Balanced training data.txt', 'w') as f:\n",
    "        for number, fold in zip(range(BF), fold_df):\n",
    "            f.write(f\"Fold: {number}\\n\\n{fold}\\n\\n\\n\")\n",
    "            \n",
    "    return Input_folds, Output_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd1aaa",
   "metadata": {},
   "source": [
    "### RFC hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0840d9dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  def Hyperparameter(BF, Input_folds, Output_folds):\n",
    "#     \"\"\" Input:      BF                Number of balancing folds needed\n",
    "#                     Input_folds       List of 5 balanced arrays of training data\n",
    "#                     Output_folds      List of 5 balanced arrays of training data's labels\n",
    "\n",
    "#         Returns:    BF_RFC_HP         List of optimized hyperparameters for each RFC\n",
    "\n",
    "#         Perform RandomSearchCV on each RFC to optimize number of trees, max depth and max samples\n",
    "#     \"\"\"  \n",
    "#     estimator = RandomForestClassifier()\n",
    "#     param_grid = {\n",
    "#                 'n_estimators':np.arange(50,500,50),\n",
    "#                 'max_depth': np.arange(2, 10, 2),\n",
    "#                 'max_samples': np.arange(0.2, 1.2, 0.2)\n",
    "#                   }\n",
    "#     BF_RFC_HP = []\n",
    "\n",
    "#     for i in range(BF):\n",
    "#         HPtuning = RandomizedSearchCV(\n",
    "#             estimator,\n",
    "#             param_grid, \n",
    "#             scoring = 'balanced_accuracy',\n",
    "#             cv = 10,\n",
    "#             n_jobs = 6, #how many cores to run in parallel\n",
    "#             verbose = 2\n",
    "#             ).fit(Input_folds[i], Output_folds[i].ravel())\n",
    "#         BF_RFC_HP.append(HPtuning.best_params_)\n",
    "    \n",
    "#     return(BF_RFC_HP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796af0e8",
   "metadata": {},
   "source": [
    "### Train RFC on the trainings folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1decd7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BF_fitting(Input_folds, Output_folds): \n",
    "    \"\"\" Input:\n",
    "                    Input_train_list      List of 5 balanced arrays for training data\n",
    "                    Classes_train_list    List of 5 balanced arrays of training data labels\n",
    "        Returns:    BF_RFC                List of RFC's trained on data in each balancing fold\n",
    "\n",
    "        Create RFC model that returns probability predictions for each fold, using output of Balance_Folds() as training data\n",
    "    \"\"\"    \n",
    "    BF_RFC = []\n",
    "    for i in range(BF):\n",
    "        BF_RFC.append(RandomForestClassifier(verbose = 1, n_estimators = 1000)) #Generates a RFC for each fold's training data\n",
    "        BF_RFC[i].fit(Input_folds[i], Output_folds[i]) #Fits the RFC to each folds' training data\n",
    "        \n",
    "    return BF_RFC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd278c",
   "metadata": {},
   "source": [
    "#### Validate each RFC on validation set, for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acc41cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BF_validate(BF_RFC, ValData):\n",
    "    \"\"\" Input:  BF_RFC          List of RFC's trained on data in each balancing fold\n",
    "                Input_val_list  Unseen validation data fold from CV fold\n",
    "                \n",
    "        Returns:Prob_matrix     List of arrays. Each item is 2D matrix where the 1st dimension is each subset in balancing fold, \n",
    "                                2nd dimension is predicted probability\n",
    "    \n",
    "        Test the trained RFCs on the test set, then for every instance, outputs the predicted probability for each class\n",
    "    \"\"\"\n",
    "    \n",
    "    Prob_matrix = [] #Empty list\n",
    "    for i in range(len(BF_RFC)):\n",
    "        Prob = BF_RFC[i].predict_proba(ValData.values)\n",
    "        Prob_matrix.append(Prob)   \n",
    "            \n",
    "    with open('Validation fold probabilities.txt', 'w') as f:\n",
    "        for number, line in zip(range(BF), Prob_matrix ):\n",
    "            f.write(f\"Fold: {number}\\n\\n   SNP    PD\\n{line}\\n\\n\\n\")\n",
    "\n",
    "    return Prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b4eafa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.597"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Prob_matrix[0][:,0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b8fdd",
   "metadata": {},
   "source": [
    "### Weighted voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71033215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weighted_Vote(Prob_matrix, BF):\n",
    "    \"\"\" Input:      Prob_matrix     List of arrays. 2D matrix where the 1st dimension is each subset in balancing fold, \n",
    "                                    2nd dimension is predicted probability\n",
    "                    BF              Number of balancing folds\n",
    "\n",
    "        Returns:    Final_vote      Weighted vote classification\n",
    "\n",
    "        Calculate the final weighted vote using confidence scores (Sc). Binary classification formula Sc = 2|S0 - 0.5|\n",
    "    \"\"\"\n",
    "    Sc_PD = []\n",
    "    Sc_SNP = []\n",
    "    for i in range(BF):\n",
    "        Sc_PD.append(2* (Prob_matrix[i][:,1] - 0.5)) #Confidence scores for PD, for each fold\n",
    "        Sc_SNP.append(2*(Prob_matrix[i][:,0] - 0.5)) #Confidence scores for SNP, for each fold\n",
    "\n",
    "    Sum_PD = np.sum(Sc_PD, axis = 0) #Sum of all PD confidence scores. 1D Array\n",
    "    Sum_SNP = np.sum(Sc_SNP, axis = 0) #Sum of all SNP confidence scores. 1D Array     \n",
    "    print(Sum_PD)\n",
    "    Vote_arr = [] #Empty list\n",
    "\n",
    "    for i in range(len(Sum_PD)):\n",
    "        if Sum_PD[i] >= Sum_SNP[i]:\n",
    "            Vote_arr.append([1]) #Append PD classifications to list\n",
    "        elif Sum_SNP[i] > Sum_PD[i]:\n",
    "            Vote_arr.append([0]) #Append SNP classifications to list\n",
    "\n",
    "        Final_vote = np.stack(Vote_arr) #Converts list of arrays to a 2D array\n",
    "        Final_vote = Final_vote.ravel() #Flattens 2D array to 1D array\n",
    "\n",
    "    return(Final_vote, Sum_PD, Sum_SNP) #Returns the final confidence scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b202f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2054545454545453,\n",
       " 0.061818181818181765,\n",
       " 0.5509090909090909,\n",
       " 0.1018181818181818,\n",
       " 0.20181818181818187,\n",
       " 0.7618181818181817,\n",
       " 0.21111111111111117,\n",
       " 0.23272727272727262,\n",
       " 0.0745454545454545,\n",
       " 0.7363636363636362,\n",
       " 0.4127272727272728,\n",
       " 0.6418181818181818,\n",
       " 0.2618181818181817,\n",
       " 0.09555555555555564,\n",
       " 0.36363636363636365,\n",
       " 0.14181818181818184,\n",
       " 0.5781818181818181,\n",
       " 0.05454545454545449,\n",
       " 0.7418181818181817,\n",
       " 0.7272727272727272,\n",
       " 0.031111111111111138,\n",
       " 0.11777777777777776,\n",
       " 0.0036363636363636394,\n",
       " 0.16000000000000003,\n",
       " 0.7399999999999999,\n",
       " 0.013333333333333345,\n",
       " 0.3709090909090909,\n",
       " 0.6666666666666667,\n",
       " 0.1654545454545455,\n",
       " 0.7927272727272726,\n",
       " 0.13333333333333333,\n",
       " 0.7,\n",
       " 0.0036363636363636394,\n",
       " 0.47111111111111115,\n",
       " 0.3155555555555556,\n",
       " 0.0,\n",
       " 0.06727272727272722,\n",
       " 0.07090909090909087,\n",
       " 0.5399999999999999,\n",
       " 0.509090909090909,\n",
       " 0.4636363636363635,\n",
       " 0.3244444444444445,\n",
       " 0.06666666666666672,\n",
       " 0.0745454545454545,\n",
       " 0.33999999999999997,\n",
       " 0.21090909090909077,\n",
       " 0.25818181818181807,\n",
       " 0.32222222222222224,\n",
       " 0.2599999999999999,\n",
       " 0.709090909090909,\n",
       " 0.1866666666666667,\n",
       " 0.0036363636363636394,\n",
       " 0.2622222222222222,\n",
       " 0.37454545454545457,\n",
       " 0.4927272727272726,\n",
       " 0.8266666666666667,\n",
       " 0.5618181818181818,\n",
       " 0.2911111111111111,\n",
       " 0.1945454545454546,\n",
       " 0.09999999999999998,\n",
       " 0.015555555555555569,\n",
       " 0.16909090909090913,\n",
       " 0.1222222222222222,\n",
       " 0.46909090909090895,\n",
       " 0.7836363636363636,\n",
       " 0.2745454545454545,\n",
       " 0.22909090909090898,\n",
       " 0.4545454545454544,\n",
       " 0.05333333333333338,\n",
       " 0.3563636363636363,\n",
       " 0.45636363636363625,\n",
       " 0.36,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.3155555555555556,\n",
       " 0.018181818181818195,\n",
       " 0.16909090909090913,\n",
       " 0.5490909090909091,\n",
       " 0.5018181818181817,\n",
       " 0.31999999999999995,\n",
       " 0.10363636363636362,\n",
       " 0.39090909090909093,\n",
       " 0.2781818181818181,\n",
       " 0.3709090909090909,\n",
       " 0.5472727272727272,\n",
       " 0.010909090909090919,\n",
       " 0.4254545454545455,\n",
       " 0.5490909090909091,\n",
       " 0.8054545454545454,\n",
       " 0.02666666666666669,\n",
       " 0.6472727272727273,\n",
       " 0.45636363636363625,\n",
       " 0.12363636363636364,\n",
       " 0.3163636363636363,\n",
       " 0.1222222222222222,\n",
       " 0.5472727272727272,\n",
       " 0.5854545454545454,\n",
       " 0.38363636363636366,\n",
       " 0.18363636363636368,\n",
       " 0.1488888888888889,\n",
       " 0.4272727272727273,\n",
       " 0.018181818181818195,\n",
       " 0.04444444444444448,\n",
       " 0.5945454545454545,\n",
       " 0.16888888888888892,\n",
       " 0.6054545454545455,\n",
       " 0.5133333333333333,\n",
       " 0.3066666666666667,\n",
       " 0.3177777777777778,\n",
       " 0.2963636363636363,\n",
       " 0.15555555555555556,\n",
       " 0.18363636363636368,\n",
       " 0.36,\n",
       " 0.22666666666666674,\n",
       " 0.8109090909090909,\n",
       " 0.5444444444444445,\n",
       " 0.007272727272727279,\n",
       " 0.37555555555555553,\n",
       " 0.18181818181818185,\n",
       " 0.020000000000000018,\n",
       " 0.47999999999999987,\n",
       " 0.3155555555555556,\n",
       " 0.4,\n",
       " 0.12545454545454546,\n",
       " 0.0,\n",
       " 0.061818181818181765,\n",
       " 0.6763636363636362,\n",
       " 0.012727272727272738,\n",
       " 0.45999999999999985,\n",
       " 0.3444444444444445,\n",
       " 0.7672727272727272,\n",
       " 0.458181818181818,\n",
       " 0.08666666666666674,\n",
       " 0.3509090909090909,\n",
       " 0.5563636363636363,\n",
       " 0.37636363636363634,\n",
       " 0.34222222222222226,\n",
       " 0.5854545454545454,\n",
       " 0.3733333333333334,\n",
       " 0.010909090909090919,\n",
       " 0.46545454545454534,\n",
       " 0.6363636363636364,\n",
       " 0.30888888888888894,\n",
       " 0.11999999999999998,\n",
       " 0.3709090909090909,\n",
       " 0.5266666666666667,\n",
       " 0.4555555555555556,\n",
       " 0.48545454545454536,\n",
       " 0.3145454545454545,\n",
       " 0.5690909090909091,\n",
       " 0.3563636363636363,\n",
       " 0.1672727272727273,\n",
       " 0.2872727272727272,\n",
       " 0.10444444444444441,\n",
       " 0.16181818181818186,\n",
       " 0.024444444444444467,\n",
       " 0.13777777777777778,\n",
       " 0.13555555555555554,\n",
       " 0.5018181818181817,\n",
       " 0.05333333333333338,\n",
       " 0.4927272727272726,\n",
       " 0.040000000000000036,\n",
       " 0.10727272727272726,\n",
       " 0.5381818181818181,\n",
       " 0.3418181818181818,\n",
       " 0.1511111111111111,\n",
       " 0.4355555555555556,\n",
       " 0.19818181818181824,\n",
       " 0.3036363636363636,\n",
       " 0.016363636363636375,\n",
       " 0.025454545454545476,\n",
       " 0.6266666666666667,\n",
       " 0.01111111111111112,\n",
       " 0.020000000000000018,\n",
       " 0.0927272727272727,\n",
       " 0.6490909090909092,\n",
       " 0.5377777777777778,\n",
       " 0.19636363636363643,\n",
       " 0.2963636363636363,\n",
       " 0.015555555555555569,\n",
       " 0.46,\n",
       " 0.5036363636363635,\n",
       " 0.7709090909090909,\n",
       " 0.6036363636363636,\n",
       " 0.36727272727272725,\n",
       " 0.4818181818181817,\n",
       " 0.10545454545454544,\n",
       " 0.27555555555555555,\n",
       " 0.19777777777777783,\n",
       " 0.36727272727272725,\n",
       " 0.48545454545454536,\n",
       " 0.47636363636363627,\n",
       " 0.2418181818181817,\n",
       " 0.4999999999999999,\n",
       " 0.5472727272727272,\n",
       " 0.26666666666666666,\n",
       " 0.4545454545454544,\n",
       " 0.08181818181818179,\n",
       " 0.08000000000000007,\n",
       " 0.36,\n",
       " 0.1311111111111111,\n",
       " 0.3777777777777777,\n",
       " 0.4072727272727273,\n",
       " 0.41555555555555557,\n",
       " 0.11454545454545453,\n",
       " 0.7599999999999999,\n",
       " 0.6509090909090909,\n",
       " 0.8527272727272727,\n",
       " 0.30181818181818176,\n",
       " 0.34222222222222226,\n",
       " 0.32909090909090905,\n",
       " 0.2444444444444444,\n",
       " 0.5981818181818181,\n",
       " 0.24222222222222217,\n",
       " 0.015555555555555569,\n",
       " 0.6381818181818182,\n",
       " 0.39555555555555555,\n",
       " 0.5363636363636363,\n",
       " 0.07090909090909087,\n",
       " 0.37272727272727274,\n",
       " 0.4181818181818182,\n",
       " 0.3127272727272727,\n",
       " 0.1288888888888889,\n",
       " 0.5218181818181817,\n",
       " 0.2709090909090908,\n",
       " 0.06363636363636359,\n",
       " 0.4181818181818182,\n",
       " 0.005454545454545459,\n",
       " 0.438181818181818,\n",
       " 0.3072727272727272,\n",
       " 0.06666666666666672,\n",
       " 0.458181818181818,\n",
       " 0.2218181818181817,\n",
       " 0.45090909090909076,\n",
       " 0.49454545454545445,\n",
       " 0.5472727272727272,\n",
       " 0.7127272727272725,\n",
       " 0.012727272727272738,\n",
       " 0.3155555555555556,\n",
       " 0.11272727272727272,\n",
       " 0.46909090909090895,\n",
       " 0.15272727272727274,\n",
       " 0.031111111111111138,\n",
       " 0.25272727272727263,\n",
       " 0.6745454545454543,\n",
       " 0.5290909090909091,\n",
       " 0.33999999999999997,\n",
       " 0.03999999999999993,\n",
       " 0.22909090909090898,\n",
       " 0.10727272727272726,\n",
       " 0.5054545454545454,\n",
       " 0.16909090909090913,\n",
       " 0.45999999999999985,\n",
       " 0.07111111111111117,\n",
       " 0.4527272727272726,\n",
       " 0.6909090909090908,\n",
       " 0.002222222222222224,\n",
       " 0.5381818181818181,\n",
       " 0.3527272727272727,\n",
       " 0.041818181818181754,\n",
       " 0.709090909090909,\n",
       " 0.2963636363636363,\n",
       " 0.3309090909090909,\n",
       " 0.040000000000000036,\n",
       " 0.05999999999999995,\n",
       " 0.48363636363636353,\n",
       " 0.42,\n",
       " 0.1945454545454546,\n",
       " 0.05272727272727267,\n",
       " 0.2799999999999999,\n",
       " 0.1563636363636364,\n",
       " 0.07777777777777785,\n",
       " 0.19818181818181824,\n",
       " 0.5109090909090909,\n",
       " 0.2822222222222222,\n",
       " 0.2054545454545453,\n",
       " 0.07818181818181814,\n",
       " 0.36666666666666675,\n",
       " 0.3272727272727272,\n",
       " 0.7127272727272725,\n",
       " 0.39999999999999997,\n",
       " 0.2963636363636363,\n",
       " 0.3911111111111111,\n",
       " 0.07272727272727268,\n",
       " 0.3933333333333333,\n",
       " 0.3218181818181818,\n",
       " 0.23818181818181808,\n",
       " 0.1672727272727273,\n",
       " 0.0644444444444445,\n",
       " 0.35818181818181816,\n",
       " 0.30181818181818176,\n",
       " 0.39999999999999997,\n",
       " 0.023636363636363657,\n",
       " 0.14545454545454548,\n",
       " 0.5254545454545454,\n",
       " 0.0555555555555556,\n",
       " 0.08727272727272724,\n",
       " 0.11555555555555554,\n",
       " 0.7763636363636363,\n",
       " 0.2618181818181817,\n",
       " 0.13999999999999999,\n",
       " 0.06909090909090905,\n",
       " 0.5272727272727272,\n",
       " 0.5163636363636362,\n",
       " 0.3181818181818181,\n",
       " 0.6527272727272727,\n",
       " 0.4236363636363637,\n",
       " 0.16909090909090913,\n",
       " 0.05272727272727267,\n",
       " 0.11272727272727272,\n",
       " 0.25454545454545446,\n",
       " 0.64,\n",
       " 0.668888888888889,\n",
       " 0.6022222222222222,\n",
       " 0.11555555555555554,\n",
       " 0.5309090909090909,\n",
       " 0.5254545454545454,\n",
       " 0.04444444444444448,\n",
       " 0.12545454545454546,\n",
       " 0.8466666666666667,\n",
       " 0.4111111111111111,\n",
       " 0.11272727272727272,\n",
       " 0.18363636363636368,\n",
       " 0.6527272727272727,\n",
       " 0.03272727272727275,\n",
       " 0.8745454545454545,\n",
       " 0.09636363636363635,\n",
       " 0.14,\n",
       " 0.5636363636363636,\n",
       " 0.2977777777777778,\n",
       " 0.008888888888888896,\n",
       " 0.6545454545454545,\n",
       " 0.2563636363636363,\n",
       " 0.2872727272727272,\n",
       " 0.26727272727272716,\n",
       " 0.2981818181818181,\n",
       " 0.13636363636363638,\n",
       " 0.17818181818181822,\n",
       " 0.10363636363636362,\n",
       " 0.36,\n",
       " 0.05272727272727267,\n",
       " 0.84,\n",
       " 0.5599999999999999,\n",
       " 0.3163636363636363,\n",
       " 0.09090909090909088,\n",
       " 0.5163636363636362,\n",
       " 0.5036363636363635,\n",
       " 0.4618181818181817,\n",
       " 0.3163636363636363,\n",
       " 0.23111111111111118,\n",
       " 0.2963636363636363,\n",
       " 0.4,\n",
       " 0.40545454545454546,\n",
       " 0.82,\n",
       " 0.4072727272727273,\n",
       " 0.058181818181818126,\n",
       " 0.16181818181818186,\n",
       " 0.05090909090909085,\n",
       " 0.42909090909090913,\n",
       " 0.42444444444444446,\n",
       " 0.23999999999999988,\n",
       " 0.34666666666666673,\n",
       " 0.8309090909090908,\n",
       " 0.45999999999999985,\n",
       " 0.38181818181818183,\n",
       " 0.44909090909090893,\n",
       " 0.1872727272727273,\n",
       " 0.6090909090909091,\n",
       " 0.6418181818181818,\n",
       " 0.6418181818181818,\n",
       " 0.2618181818181817,\n",
       " 0.1488888888888889,\n",
       " 0.5377777777777778,\n",
       " 0.28181818181818175,\n",
       " 0.49454545454545445,\n",
       " 0.2599999999999999,\n",
       " 0.29090909090909084,\n",
       " 0.14545454545454548,\n",
       " 0.6309090909090909,\n",
       " 0.3145454545454545,\n",
       " 0.012727272727272738,\n",
       " 0.4963636363636363,\n",
       " 0.5763636363636363,\n",
       " 0.1577777777777778,\n",
       " 0.5345454545454544,\n",
       " 0.35111111111111115,\n",
       " 0.12545454545454546,\n",
       " 0.041818181818181754,\n",
       " 0.5854545454545454,\n",
       " 0.19111111111111115,\n",
       " 0.1945454545454546,\n",
       " 0.43636363636363623,\n",
       " 0.1490909090909091,\n",
       " 0.5345454545454544,\n",
       " 0.5709090909090908,\n",
       " 0.03454545454545447,\n",
       " 0.03272727272727275,\n",
       " 0.04727272727272721,\n",
       " 0.18000000000000002,\n",
       " 0.3933333333333333,\n",
       " 0.6577777777777779,\n",
       " 0.37454545454545457,\n",
       " 0.8018181818181818,\n",
       " 0.19818181818181824,\n",
       " 0.6727272727272726,\n",
       " 0.17555555555555558,\n",
       " 0.018181818181818195,\n",
       " 0.07272727272727268,\n",
       " 0.015555555555555569,\n",
       " 0.4636363636363635,\n",
       " 0.26363636363636356,\n",
       " 0.1290909090909091,\n",
       " 0.6509090909090909,\n",
       " 0.3890909090909091,\n",
       " 0.2854545454545454,\n",
       " 0.58,\n",
       " 0.3444444444444445,\n",
       " 0.39090909090909093,\n",
       " 0.20222222222222228,\n",
       " 0.42000000000000004,\n",
       " 0.20000000000000004,\n",
       " 0.5,\n",
       " 0.38,\n",
       " 0.2133333333333334,\n",
       " 0.3981818181818182,\n",
       " 0.07777777777777785,\n",
       " 0.12545454545454546,\n",
       " 0.26363636363636356,\n",
       " 0.3022222222222222,\n",
       " 0.23272727272727262,\n",
       " 0.5309090909090909,\n",
       " 0.09090909090909088,\n",
       " 0.11777777777777776,\n",
       " 0.718181818181818,\n",
       " 0.1018181818181818,\n",
       " 0.3309090909090909,\n",
       " 0.5654545454545454,\n",
       " 0.49333333333333335,\n",
       " 0.2309090909090908,\n",
       " 0.6,\n",
       " 0.3418181818181818,\n",
       " 0.11454545454545453,\n",
       " 0.3555555555555556,\n",
       " 0.4111111111111111,\n",
       " 0.8345454545454545,\n",
       " 0.3866666666666666,\n",
       " 0.5381818181818181,\n",
       " 0.34909090909090906,\n",
       " 0.15454545454545457,\n",
       " 0.4272727272727273,\n",
       " 0.5927272727272727,\n",
       " 0.35111111111111115,\n",
       " 0.2036363636363637,\n",
       " 0.09999999999999998,\n",
       " 0.5345454545454544,\n",
       " 0.1511111111111111,\n",
       " 0.2836363636363636,\n",
       " 0.09999999999999998,\n",
       " 0.7472727272727272,\n",
       " 0.438181818181818,\n",
       " 0.44181818181818167,\n",
       " 0.5272727272727272,\n",
       " 0.17555555555555558,\n",
       " 0.44909090909090893,\n",
       " 0.6563636363636364,\n",
       " 0.22909090909090898,\n",
       " 0.6236363636363637,\n",
       " 0.7444444444444445,\n",
       " 0.5311111111111111,\n",
       " 0.12666666666666665,\n",
       " 0.4345454545454544,\n",
       " 0.11111111111111108,\n",
       " 0.6145454545454545,\n",
       " 0.2133333333333334,\n",
       " 0.6181818181818182,\n",
       " 0.01111111111111112,\n",
       " 0.5672727272727273,\n",
       " 0.7345454545454544,\n",
       " 0.40888888888888886,\n",
       " 0.4527272727272726,\n",
       " 0.17636363636363642,\n",
       " 0.2872727272727272,\n",
       " 0.1290909090909091,\n",
       " 0.008888888888888896,\n",
       " 0.4311111111111111,\n",
       " 0.16181818181818186,\n",
       " 0.24363636363636354,\n",
       " 0.13818181818181818,\n",
       " 0.7199999999999999,\n",
       " 0.03777777777777781,\n",
       " 0.5636363636363636,\n",
       " 0.5345454545454544,\n",
       " 0.4909090909090908,\n",
       " 0.7272727272727272,\n",
       " 0.29090909090909084,\n",
       " 0.11272727272727272,\n",
       " 0.23272727272727262,\n",
       " 0.24545454545454534,\n",
       " 0.45090909090909076,\n",
       " 0.4311111111111111,\n",
       " 0.43272727272727257,\n",
       " 0.007272727272727279,\n",
       " 0.7018181818181817,\n",
       " 0.23111111111111118,\n",
       " 0.8927272727272726,\n",
       " 0.3555555555555556,\n",
       " 0.07555555555555563,\n",
       " 0.6509090909090909,\n",
       " 0.21818181818181806,\n",
       " 0.8563636363636363,\n",
       " 0.17333333333333337,\n",
       " 0.1290909090909091,\n",
       " 0.40545454545454546,\n",
       " 0.33454545454545453,\n",
       " 0.4927272727272726,\n",
       " 0.58,\n",
       " 0.49818181818181806,\n",
       " 0.33454545454545453,\n",
       " 0.4444444444444445,\n",
       " 0.2127272727272726,\n",
       " 0.8418181818181818,\n",
       " 0.22727272727272715,\n",
       " 0.5511111111111111,\n",
       " 0.5509090909090909,\n",
       " 0.14181818181818184,\n",
       " 0.5,\n",
       " 0.031111111111111138,\n",
       " 0.20666666666666672,\n",
       " 0.5381818181818181,\n",
       " 0.5072727272727272,\n",
       " 0.8836363636363637,\n",
       " 0.3145454545454545,\n",
       " 0.33999999999999997,\n",
       " 0.35454545454545455,\n",
       " 0.4236363636363637,\n",
       " 0.08181818181818179,\n",
       " 0.4618181818181817,\n",
       " 0.09999999999999996]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    T = 0.45\n",
    "\n",
    "    for fold in range(BF): # Calculates SNP Sc all instances in each fold\n",
    "        Sc_SNP = []\n",
    "        for value in range(len(Prob_matrix[fold][:,0])):\n",
    "            S0 = Prob_matrix[fold][:,0][value] #each SNP value in prob matrix fold\n",
    "            if S0 < T:\n",
    "                Sc = (T - S0)/T\n",
    "            elif S0 >= T:\n",
    "                Sc = (S0 - T)/(1 - T)        \n",
    "            Sc_SNP.append(Sc)   \n",
    "\n",
    "    for fold in range(BF):# Calculates PD Sc all instances in each fold\n",
    "        Sc_PD = []\n",
    "        for value in range(len(Prob_matrix[fold][:,1])):\n",
    "            S0 = Prob_matrix[fold][:,1][value] #each PD value in prob matrix fold\n",
    "            if S0 < T:\n",
    "                Sc = (T - S0)/T\n",
    "            elif S0 >= T:\n",
    "                Sc = (S0 - T)/(1 - T)        \n",
    "            Sc_PD.append(Sc) \n",
    "            \n",
    "Sc_SNP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f5b78",
   "metadata": {},
   "source": [
    "### Final confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ae1157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Final_score(Sum_PD, Sum_SNP, BF):\n",
    "    \"\"\" Input:      Sum_PD      Sum of confidence score for PD predictions\n",
    "                    Sum_SNP     Sum of confidence score for SNP predictions\n",
    "\n",
    "        Returns:    S_out       Final confidence score\n",
    "\n",
    "        Calculate the final confidence score\n",
    "    \"\"\"\n",
    "    \n",
    "    S_Out = np.abs((Sum_PD - Sum_SNP) /(BF*2))\n",
    "    np.savetxt('S_out.txt', S_Out, \"%.3f\")\n",
    "    \n",
    "    return S_Out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92f36545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evalutation(Vallabel, Final_vote, S_Out):\n",
    "    \"\"\" Input:      Classes_val_list   Validation label data\n",
    "                    Final_vote         Weighted vote classification\n",
    "\n",
    "        Evaluation metrics from RFC on validation data with\n",
    "    \"\"\"\n",
    "    Output_pred = Final_vote\n",
    "    print(f\"              ***Final Evaluation***\\n\")\n",
    "    print(f\"Confusion Matrix:\\n {confusion_matrix(Vallabel, Output_pred)}\")\n",
    "    print(f\"{classification_report(Vallabel, Output_pred)}\\nMCC                {matthews_corrcoef(Vallabel, Output_pred)}\")\n",
    "\n",
    "    print(f\"See file 'Classification.txt' for final classifications and confidence scores\")\n",
    "    np.savetxt('Classification.txt',\n",
    "           np.column_stack([Final_vote, S_Out]),\n",
    "           fmt = [\"%.0f\",\"%.3f\"],\n",
    "           delimiter =\"      \",\n",
    "           header = \"Final classifications and confidence scores\\n\\n\"\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa67e232",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a5a9897",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.202  2.344 -0.97  -2.072  0.066 -1.796  2.048 -1.53  -1.698 -2.704\n",
      " -0.906  2.21  -0.93   1.712 -0.76   1.226 -0.92   1.926  0.702 -2.092\n",
      " -3.054 -1.662  0.576  2.302 -0.444 -0.392 -1.218 -1.71   2.94   1.248\n",
      " -1.766  0.424 -1.188  0.018  1.956 -1.456 -2.482 -1.564 -2.752  0.408\n",
      " -1.362 -2.614 -1.672 -1.552  3.624  3.446 -2.294  0.918 -1.112 -2.632\n",
      " -2.806 -1.484  0.426 -3.014 -0.832  0.456  1.424 -4.078 -0.866 -0.724\n",
      " -1.446  1.848 -1.136 -0.832  0.508  0.156 -2.396 -3.582 -1.034 -0.374\n",
      " -0.058 -0.464  0.562  0.754  1.254  0.21  -1.604 -0.312  1.346  1.05\n",
      " -2.496 -1.934 -0.624  0.77  -0.736  1.808 -0.026  2.256  0.494  2.608\n",
      "  2.84   2.084 -1.86  -0.006 -2.898  0.19   0.228 -2.38  -3.778  2.758\n",
      " -3.008 -1.016 -2.024  0.148 -1.992  0.514 -2.296  0.91  -0.644  1.078\n",
      " -0.836 -3.724  1.446 -3.774 -0.312  2.692  1.124 -2.188 -0.322 -0.706\n",
      " -2.58  -4.264  0.712 -1.268 -0.668 -3.558 -0.666  2.432 -3.012 -3.068\n",
      "  1.734 -0.376  0.198  2.902  1.036  2.06  -2.84  -1.706 -1.98   1.34\n",
      " -1.35  -0.854 -3.016 -2.562  2.792  1.418  0.802 -0.246 -2.212  0.64\n",
      " -1.248  0.018  2.72   0.004 -0.926 -2.246 -2.466  2.338 -2.05   2.474\n",
      " -1.054 -1.054  3.95  -0.612 -1.524  0.512 -1.04   0.784 -0.092  2.592\n",
      " -3.22  -2.538  0.402  0.04  -0.46   0.57  -1.088 -1.212  2.44  -1.518\n",
      " -0.936  0.448  1.64   0.48  -0.03  -1.894 -2.872  1.562  3.172 -1.122\n",
      " -0.414 -1.952 -0.064 -1.844  2.654 -1.402  2.238 -1.638 -0.764  0.77\n",
      "  0.12   1.286 -0.534 -0.488 -3.76   1.346  2.998 -2.682  0.754 -3.446\n",
      " -2.088 -2.272  1.41  -0.254  0.454  2.724 -1.116 -0.862 -2.62  -0.7\n",
      "  1.768 -1.52  -0.27   2.438  0.796 -1.704 -0.05   0.538  0.608  1.872\n",
      " -2.534 -0.056  3.026 -2.376  0.896 -2.482 -1.152 -0.422  0.656 -1.532\n",
      " -1.872 -1.67  -0.376  1.936  1.906  1.092 -3.368  1.66  -0.05  -2.98\n",
      "  0.404 -3.472 -0.62   0.368 -3.554 -1.21  -2.88  -2.272  2.986 -2.114\n",
      " -3.162 -2.876 -0.782 -1.534  2.     0.37   3.186 -1.356 -0.004 -1.278\n",
      "  1.678 -1.974  0.524 -1.542 -0.712  2.324 -1.818 -1.238 -0.868 -1.838\n",
      " -0.098  2.338 -0.736  0.388  0.804  2.376 -1.926  1.274  0.742  2.122\n",
      " -2.74   1.722 -0.71  -1.558  1.032 -0.35  -0.076  0.98   1.864 -0.502\n",
      " -0.54  -0.592  2.138 -2.044 -2.256 -0.562 -0.47  -4.074 -1.03   2.028\n",
      " -0.488 -1.472  0.218 -2.02  -1.52   1.558  2.182  3.366  2.256 -1.564\n",
      " -2.018  3.1   -2.518 -3.264 -1.896 -3.578 -1.878  2.864  1.336  2.094\n",
      "  3.568  0.378  1.77   0.98  -2.646 -1.69  -2.472  2.394 -2.676  1.634\n",
      " -1.458 -1.726  0.918  1.542  2.17   1.196 -2.884 -0.562 -2.352 -1.34\n",
      " -3.894  1.07   1.316  0.594 -1.002 -1.248 -2.336 -2.984 -1.352  0.702\n",
      "  2.804 -1.562 -3.048  2.184  1.43  -1.084  1.052 -1.274  1.608  1.956\n",
      "  1.984  1.734 -0.25  -2.152 -0.596 -0.492  2.288 -1.086  0.502 -2.736\n",
      " -0.716 -0.462  0.18   1.054  0.132 -2.566  1.014  0.302 -0.566  3.382\n",
      "  2.518  0.324  1.906  1.082  0.826 -0.866 -1.706 -0.966 -0.84  -0.476\n",
      " -0.958  0.794 -2.256 -1.266 -2.41  -3.296 -3.002 -3.26  -0.352 -1.35\n",
      "  2.118  0.326  0.342 -1.108 -3.286 -2.076 -0.07  -1.67   1.84  -2.698\n",
      " -1.664  0.798  1.36  -0.572 -1.466 -0.092  0.686 -2.99  -2.944  0.788\n",
      " -1.428  1.054 -2.522 -0.322 -3.74  -0.884  2.878 -2.05   1.216 -1.248\n",
      "  1.404 -0.384 -1.874  1.672  0.126  2.882 -2.734 -0.392  2.67   0.38\n",
      "  0.178 -1.012 -1.73  -0.542  1.546  0.392 -3.054  2.6   -3.844 -0.38\n",
      "  1.106  1.506  2.336  0.58  -1.09  -2.1   -2.088 -0.764  1.736 -4.35\n",
      " -1.688  1.832 -2.532 -2.226 -1.062  3.278 -2.824  0.49   0.972 -1.658\n",
      "  0.166 -1.422 -1.712  2.794 -0.872 -3.27   2.852 -0.474 -0.598  3.512\n",
      "  2.57   1.23  -2.046 -0.888  2.162 -2.44   1.52  -1.284 -1.128 -0.102\n",
      " -1.988 -0.796 -0.26  -0.45  -1.308 -3.298 -2.526  2.462 -3.83  -3.332\n",
      "  1.726 -0.26  -0.166 -1.84   0.684 -1.816 -3.924  0.362  2.47  -0.686\n",
      " -0.346 -1.32   0.158 -2.138  3.48   1.88  -1.998 -1.644  1.226  0.148\n",
      "  1.774 -0.414 -1.794  0.15  -3.394 -1.25   0.324 -2.806 -4.062]\n",
      "              ***Final Evaluation***\n",
      "\n",
      "Confusion Matrix:\n",
      " [[156  26]\n",
      " [162 195]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.86      0.62       182\n",
      "           1       0.88      0.55      0.67       357\n",
      "\n",
      "    accuracy                           0.65       539\n",
      "   macro avg       0.69      0.70      0.65       539\n",
      "weighted avg       0.75      0.65      0.66       539\n",
      "\n",
      "MCC                0.3878415923257102\n",
      "See file 'Classification.txt' for final classifications and confidence scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.402 -1.348  0.13  -1.106  1.556 -3.252  0.27  -1.74  -0.5   -3.404\n",
      " -2.228  1.246 -0.404  0.244 -0.728 -3.616 -0.806 -2.616 -0.152 -0.188\n",
      "  0.524  1.318  0.668 -1.024  0.766 -2.982  0.116 -2.946  2.228  1.794\n",
      " -2.924  3.234 -0.036 -1.362  2.786 -2.452 -0.2    0.618 -2.632 -1.956\n",
      " -1.552 -2.446  1.114  1.372 -3.148  1.934  1.476  1.608 -1.904 -2.018\n",
      " -0.218 -3.156 -3.122  1.952  1.556  1.708 -2.158 -3.656  0.86   1.646\n",
      " -3.152 -1.756  0.92   0.588 -2.828 -0.978 -0.1    2.864  2.308 -1.014\n",
      "  0.554  0.754 -0.606 -1.926  2.092  2.546 -1.85  -0.718 -0.068  2.41\n",
      " -0.7   -3.24  -0.35  -1.306 -2.936 -2.642 -1.234 -1.322 -0.56  -2.678\n",
      " -1.24  -1.424  2.086 -0.636 -1.66  -0.538  1.34   2.66  -3.236  0.138\n",
      " -2.258 -2.016  2.258  0.55  -2.746 -1.436 -2.01  -1.124 -0.846 -2.404\n",
      " -2.986  2.98  -3.866  2.274 -2.58   0.194  1.436  2.942  2.31  -0.362\n",
      "  2.142 -1.948 -0.312 -0.316  1.008  0.218 -1.502  2.132 -1.922  1.208\n",
      "  0.088 -0.75  -3.528  2.634  1.272  1.364 -1.602 -1.708 -0.816 -0.588\n",
      " -1.04   0.916  3.162  1.012  2.37   0.456 -3.56  -4.072 -0.864 -1.968\n",
      "  2.678 -2.562  1.422 -1.966 -3.32  -2.534  2.622 -0.204  2.408  0.28\n",
      " -1.346 -1.006  0.322 -0.428 -1.406  0.584 -0.408  3.054 -0.204 -1.28\n",
      " -1.762  2.05  -3.62  -1.076  0.242 -0.124 -1.978  2.444 -3.284  2.782\n",
      "  0.756  0.038 -1.814 -1.758 -1.592  1.194  3.114  0.334  0.696 -0.738\n",
      " -1.552  1.08  -1.43   2.586 -1.174 -0.91  -2.912 -3.364 -3.968 -1.762\n",
      "  2.322 -0.396  0.328 -2.126 -1.75   1.312 -0.702 -1.996 -0.912  0.414\n",
      " -0.878  1.266 -1.45  -3.558  2.048 -3.764 -1.972  1.7    3.176 -3.292\n",
      " -3.784 -1.368 -1.568  1.366  0.588  0.55   1.866 -0.4   -1.772 -1.67\n",
      " -2.408  0.222 -1.44  -0.322 -1.096 -2.3   -2.984 -0.63  -0.33  -0.656\n",
      "  1.08   1.562 -1.174 -3.708 -0.632  1.128 -2.416  3.376  0.944  0.886\n",
      "  2.192 -0.51  -0.206  1.984 -1.488  2.294  1.24  -2.01  -2.008 -3.522\n",
      " -2.176  3.168  0.278 -2.464 -1.438  0.202  1.674  0.88  -1.722 -0.838\n",
      " -0.92  -0.65  -2.028 -1.606 -2.082  2.374 -0.056 -0.792 -1.226 -1.292\n",
      "  0.562  1.228 -0.434 -0.66  -3.188 -3.772 -0.27   0.21  -1.06  -1.514\n",
      "  2.236  0.078 -0.412 -3.276 -1.118 -2.622  0.34  -3.02   1.084 -2.878\n",
      "  1.272 -2.808 -0.548 -1.82  -1.478  3.038  0.27  -3.006  1.758  2.046\n",
      "  0.486 -0.984  1.714 -0.732 -0.576 -2.96   0.91  -0.442 -1.124 -2.06\n",
      "  1.428 -2.396  1.916  1.05  -2.256 -0.798  1.104 -0.316  0.578 -1.658\n",
      "  2.222 -2.816 -2.128  1.328  1.834  0.382 -1.166  1.346 -1.22   2.572\n",
      " -0.386 -1.716 -3.908 -0.602 -2.528 -2.088 -1.404 -1.964 -2.424 -2.548\n",
      " -0.822  1.734  1.01   2.944  0.186  0.9   -0.95  -2.116  0.004 -2.122\n",
      " -1.562 -1.698 -3.15  -1.864 -0.614 -0.318 -2.746 -0.292 -2.414 -0.298\n",
      " -2.092  3.238 -0.896 -2.262 -2.368  0.756 -2.412  0.692 -2.382 -0.508\n",
      "  2.514 -3.862  2.576 -2.73   1.576  2.266 -3.162  1.396 -0.436  1.118\n",
      " -1.076  0.342  1.068 -3.108 -1.686  1.078  0.066 -3.186 -1.374  0.806\n",
      " -1.366 -1.204  0.304  0.348 -1.798 -1.594  1.424 -1.124 -2.922 -2.996\n",
      " -0.27   1.276  0.494 -2.538 -0.884  0.792  0.338 -0.956 -0.162  0.852\n",
      " -3.394  1.14  -3.806  3.134 -1.6    3.154  0.202  0.628  1.194 -2.066\n",
      "  2.396 -1.724 -0.128 -0.36  -0.722 -2.542 -2.47  -2.884 -3.18   3.634\n",
      "  1.252 -2.546  0.72  -2.468  2.284  2.41  -1.532  1.106  1.336 -1.386\n",
      " -0.316 -3.208  0.264  0.64  -2.838 -1.12   1.09  -1.244  2.81   0.582\n",
      "  0.238 -0.476 -3.838 -1.214 -3.432 -3.06  -0.86   1.886 -3.21  -2.342\n",
      "  1.004 -0.154  3.046  1.93  -1.97  -0.88   1.292  0.538 -3.086 -0.108\n",
      "  1.644 -0.042  1.552  0.072 -1.012  1.078 -1.222  1.128 -2.228  0.552\n",
      " -0.576 -2.634 -1.416 -1.868  1.858 -1.706 -0.044  2.706  1.396 -1.564\n",
      "  2.72   0.468  0.894  2.954  1.814 -1.162 -1.808 -2.548  1.498 -0.576\n",
      "  1.2   -1.102 -3.198 -2.6   -2.516 -1.194  1.514  1.57  -3.49  -0.514\n",
      " -0.012 -2.008  0.13  -2.87  -3.932 -1.504  1.06   3.272 -1.412 -1.28\n",
      " -3.672 -1.252  3.066 -1.042  1.586 -1.194 -3.084 -2.4   -2.402]\n",
      "              ***Final Evaluation***\n",
      "\n",
      "Confusion Matrix:\n",
      " [[163  23]\n",
      " [164 189]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.88      0.64       186\n",
      "           1       0.89      0.54      0.67       353\n",
      "\n",
      "    accuracy                           0.65       539\n",
      "   macro avg       0.69      0.71      0.65       539\n",
      "weighted avg       0.76      0.65      0.66       539\n",
      "\n",
      "MCC                0.4007197449983022\n",
      "See file 'Classification.txt' for final classifications and confidence scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.516  1.102 -1.148 -2.27  -0.056  0.768 -1.464  0.282 -2.462 -2.246\n",
      " -2.508 -1.986  0.766  3.344  0.832  1.358 -1.422 -0.242 -2.238  0.67\n",
      " -0.814 -0.88   0.246 -0.094  1.606 -1.724 -1.394  2.78   2.414 -0.99\n",
      "  0.518 -1.906 -1.184 -2.056  0.91  -1.642 -1.116 -3.068 -1.644 -0.554\n",
      " -1.594  1.424 -1.428  1.08   1.516  1.37   0.166 -0.488 -1.558 -1.066\n",
      " -1.266 -2.504  0.916 -2.104  2.896 -2.488 -3.874  0.914  1.444  1.34\n",
      "  2.744  0.33  -1.082 -0.74   0.034 -0.19   1.532  2.66  -0.892  1.086\n",
      "  0.138  0.054  1.156 -3.812  0.784  0.702  0.452 -0.424 -0.918 -1.31\n",
      " -3.978  2.118 -0.724 -0.944  1.826 -2.29   0.156  0.884 -1.764 -1.482\n",
      " -0.436 -1.794 -0.562 -0.996  2.946 -2.824 -1.256 -2.96  -1.42  -1.962\n",
      "  0.236 -0.074  0.408  1.138  2.608 -1.276 -0.346 -1.614 -1.28   0.402\n",
      " -2.646  2.75   1.38   0.318 -1.542 -2.438 -3.438 -0.238  1.83  -0.938\n",
      " -3.11  -1.14   2.068  0.782 -0.796  0.524 -1.468  0.348 -1.88  -1.502\n",
      " -0.73  -1.058 -1.352 -1.422 -0.896  2.308 -2.106  0.812 -1.768 -0.644\n",
      "  3.138 -2.004 -1.188  1.878 -0.264 -2.794 -1.346  3.24   1.404  1.318\n",
      " -2.146 -0.092 -0.06  -0.45  -2.204 -2.894 -2.094 -0.636 -2.772 -0.684\n",
      "  2.562  2.178 -1.428  3.386 -1.54   1.548  0.256 -3.48   1.452  2.016\n",
      "  0.016 -1.92  -1.548  1.076 -2.228 -1.346  1.816 -1.374 -0.86  -3.2\n",
      " -2.776  0.582 -2.33  -0.724 -0.278 -1.482 -1.526  1.008 -3.252 -1.152\n",
      " -2.384  0.156 -0.57   1.228 -0.554 -2.124 -3.662  1.024 -1.052  2.784\n",
      " -0.752 -1.964  0.226  0.654  0.67   0.686 -2.624  0.916  2.18  -0.43\n",
      " -1.74   2.648 -2.074 -0.962 -3.59  -3.17   0.956  0.246 -2.35  -2.742\n",
      " -2.188  2.806  0.008 -1.346 -0.726 -2.222 -3.446 -1.77   0.294 -1.232\n",
      "  0.634 -2.104 -0.228  1.218 -1.766 -0.032 -0.82  -1.194 -1.41   1.892\n",
      " -1.262 -1.992  0.172 -1.284 -0.206 -0.738  2.528 -1.544 -1.72  -1.048\n",
      "  1.808  0.662  2.222  1.398 -1.978  0.884 -0.87  -0.556  1.444  0.532\n",
      "  0.91  -2.198 -2.978  1.288 -1.738 -0.606  0.016 -1.706 -2.58   1.642\n",
      " -2.62   1.062  0.66  -2.206  1.962  0.538 -1.698 -2.438  2.444  0.746\n",
      " -2.18   0.482 -2.522 -1.492 -2.514  2.066 -0.078 -0.392 -0.034 -0.476\n",
      "  1.174  3.572  2.77  -2.62  -0.158  1.744 -3.406  4.142  1.514  0.114\n",
      " -0.316 -1.582  0.516 -3.396  0.062  0.73  -0.68  -3.082 -2.522 -1.19\n",
      " -2.326 -2.032 -0.748  0.986  2.61  -0.198 -2.86  -0.678 -2.28  -1.364\n",
      "  1.198 -0.798 -1.476 -1.266 -1.412 -1.846  0.494 -1.868 -3.33   1.4\n",
      " -1.964  1.638 -1.996  1.286  1.168 -1.802 -1.806 -1.19  -1.044 -1.48\n",
      "  0.622  1.394 -0.932 -2.548 -3.118 -1.11   0.172  0.284 -1.162 -1.626\n",
      " -2.06  -2.246 -2.804  1.462  2.126  1.726 -1.864  1.762 -1.622 -3.428\n",
      " -0.196  1.256  2.382  0.28   3.126 -2.316  1.846 -1.448  0.728  2.7\n",
      " -1.424 -1.954 -2.08  -2.032 -0.298  1.174 -2.534 -1.616 -0.514 -1.29\n",
      "  0.612 -2.644  2.138 -3.2    1.596 -2.216 -0.832  3.738 -0.33  -0.552\n",
      "  0.412  0.176  2.846 -2.7   -2.036 -1.728 -3.91  -1.244 -0.722  0.516\n",
      " -2.418 -3.066 -1.004  1.698 -2.642 -2.274 -2.794 -0.964  2.456 -2.548\n",
      "  0.87   0.326 -1.636 -0.178  0.582 -2.096 -1.46  -1.376  1.442 -0.436\n",
      " -0.188  0.224  0.646 -3.9   -3.464 -0.488 -3.078 -1.402 -1.72   0.198\n",
      "  0.688 -2.976  0.102 -0.096 -0.438 -2.628  0.784 -2.258 -2.302 -1.012\n",
      " -0.198 -2.108  1.46   2.006  2.832 -1.438 -2.526 -3.902 -1.6   -0.622\n",
      "  1.378  0.754  2.068 -2.952 -2.408  1.194  0.208  0.472  0.252 -1.762\n",
      " -2.402  2.48   1.252 -0.848  1.84  -1.4   -3.028  0.198  2.416 -0.68\n",
      "  1.834 -2.034 -0.226  1.708 -1.2    0.248  1.14  -1.292 -0.682 -1.16\n",
      " -1.068  1.65  -1.132 -1.406 -1.432  2.168 -2.872  3.196 -0.134 -0.156\n",
      "  2.382  0.802  2.23  -2.968  2.126  0.14  -0.748 -0.726 -2.028  0.112\n",
      "  1.558 -0.722 -2.268 -1.01  -1.696 -2.508 -2.372 -2.192 -0.658  1.22\n",
      " -2.144 -2.198  0.942  0.494  2.362  0.88  -1.43   1.446 -2.404 -0.394\n",
      " -2.964  1.566  0.856 -1.9    2.83   1.074  0.952 -1.274 -0.218 -0.076\n",
      "  0.306 -3.256 -0.674 -0.098  0.75  -0.066 -0.544 -1.584]\n",
      "              ***Final Evaluation***\n",
      "\n",
      "Confusion Matrix:\n",
      " [[152  35]\n",
      " [177 174]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.81      0.59       187\n",
      "           1       0.83      0.50      0.62       351\n",
      "\n",
      "    accuracy                           0.61       538\n",
      "   macro avg       0.65      0.65      0.61       538\n",
      "weighted avg       0.70      0.61      0.61       538\n",
      "\n",
      "MCC                0.3014698668822797\n",
      "See file 'Classification.txt' for final classifications and confidence scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.13  -3.902  1.702  0.612  1.79  -0.196 -1.656 -0.424 -2.518 -3.772\n",
      "  2.35   1.862 -3.136 -2.31   3.246 -3.15  -1.19   2.352 -2.37   0.798\n",
      " -2.028  2.132 -2.468 -0.012 -1.428  0.774  0.662 -1.336 -1.944 -2.756\n",
      "  1.018  2.91   0.332 -1.032  1.81  -0.576 -0.218  2.156 -0.23   1.94\n",
      " -0.85   1.4    0.352  1.09  -4.228 -2.024  0.18   3.464  3.044 -1.48\n",
      "  1.316 -2.178  0.912  1.158  1.58  -4.064 -2.232  0.474 -2.366 -2.366\n",
      " -0.342 -3.032  0.448 -2.73  -1.3   -0.4    2.82   3.192 -2.906  0.664\n",
      "  1.388  3.69   0.978 -2.508 -1.742  0.776 -2.248 -1.34  -0.764 -1.042\n",
      " -2.754 -2.75   2.504 -4.232  1.52  -2.262  0.07  -0.524 -2.992  1.652\n",
      "  1.028  2.486 -3.276  1.026 -2.152  0.124  0.858 -2.756  0.404  1.606\n",
      " -0.094 -2.178  0.928 -1.354 -1.224 -2.018  1.64  -2.36   0.432 -3.83\n",
      "  0.644 -1.868  0.832 -0.318 -0.302 -1.812 -3.426  1.364 -2.246 -2.038\n",
      "  1.224  1.71   3.254 -2.104 -0.62  -2.294  2.682 -1.064  1.174 -0.05\n",
      " -2.142 -2.878 -0.126 -2.138  0.544  1.912 -0.44  -1.394 -0.86  -1.466\n",
      "  0.438  1.008 -2.686 -1.596  1.4   -3.88   1.972  1.804 -0.768 -2.882\n",
      " -0.088 -2.15  -1.172  1.034 -0.184 -2.772 -1.958 -0.846  3.048 -2.93\n",
      "  2.69  -1.942 -0.124 -3.23   3.182  0.51  -3.3   -1.564 -0.478 -1.584\n",
      " -1.698 -3.074 -1.708 -2.358 -2.302  3.332  1.138 -2.272 -1.452 -0.042\n",
      " -0.07   1.648  1.778  2.368  0.936 -0.75  -0.314  1.056  0.74  -0.732\n",
      "  1.064 -4.076 -1.644 -2.022 -0.522 -0.18   1.946  1.886  2.812 -2.604\n",
      "  2.01  -0.622  0.51  -2.88   0.006  0.864 -1.86  -3.92  -1.46  -1.034\n",
      "  1.33  -2.604  0.998 -1.764 -0.842  0.428 -1.096 -0.712  2.074 -2.526\n",
      " -0.496 -2.842 -1.318  1.74  -1.754  1.18   2.198  0.01  -1.038 -1.598\n",
      "  2.636 -1.262 -3.176  2.21  -1.81  -3.306 -1.154  0.962  1.122 -3.104\n",
      "  1.65   3.194  1.668  0.29  -1.898 -0.15   3.624  0.06  -0.086  1.726\n",
      "  2.366 -1.662  1.218 -4.12  -1.972 -0.6   -1.964 -0.08  -1.584  1.126\n",
      "  0.994  2.732  0.972 -1.798  2.82  -2.968  1.852 -3.406  1.124 -0.694\n",
      " -1.762  1.586  2.964 -0.114  1.526 -1.856 -2.356  2.646 -0.444  2.936\n",
      " -2.782 -2.426  0.626  0.102  0.398 -4.08  -3.966  1.876  2.11  -1.44\n",
      "  1.488 -4.236 -0.336 -0.964 -3.728 -3.828 -1.234  0.002  0.762  2.974\n",
      " -1.074  2.342 -1.714 -2.182 -0.962 -1.258  2.212 -4.136 -0.156 -1.78\n",
      "  2.03  -3.702 -3.26   2.114 -3.454  1.47  -1.02  -2.174 -3.872 -2.548\n",
      " -0.294  2.058 -4.128  3.358 -1.902  0.958 -0.826 -3.37  -2.876 -1.726\n",
      "  1.154 -0.81  -1.39   2.016 -3.15  -0.74  -2.482 -1.174 -3.688 -2.488\n",
      " -1.468 -2.5   -0.29  -1.734  1.224 -1.55   0.87   1.158 -1.936 -0.048\n",
      " -4.524 -1.542 -2.58  -3.248  1.496  3.14  -1.446 -2.046 -3.062  0.992\n",
      " -1.71  -2.04  -4.23   2.24  -2.242 -1.162  3.132  1.122 -1.294 -2.966\n",
      " -2.904 -1.78  -0.286 -0.136 -1.6   -0.4   -0.326 -3.1   -2.5    1.824\n",
      "  2.696  0.61   1.032  2.84  -3.24  -2.714  3.07   1.178 -2.534  0.862\n",
      "  2.652  1.84  -1.94  -1.006  0.212 -1.27   2.31   2.422 -1.516  2.194\n",
      " -1.288 -0.274 -0.3    1.988 -3.15  -0.458  0.302 -1.888  0.886 -1.596\n",
      "  4.17  -1.764  2.404 -0.268  1.54   2.458  2.134  2.222 -1.342  2.27\n",
      "  2.658  1.85  -1.98  -1.524 -0.664 -1.316 -0.554 -2.956  2.77  -2.118\n",
      "  1.616 -2.168 -3.366 -0.162 -0.198  1.436 -2.77  -0.098 -0.478 -3.748\n",
      " -0.27   0.782  1.94  -1.074  0.038 -1.662  3.728  0.984  1.55  -0.814\n",
      " -1.21   2.642  1.636 -2.154 -2.134 -2.174 -0.26  -1.078 -2.402 -1.334\n",
      " -0.632 -1.798  1.3   -3.244  1.378 -2.892  0.592  3.498 -0.402 -3.696\n",
      " -0.128 -3.266 -0.698 -0.398  2.916 -0.9   -2.716  0.444  2.808  1.074\n",
      "  0.03  -2.218 -2.824 -1.322 -1.756  2.102  2.498 -2.226  0.592  1.114\n",
      " -0.726  0.076  1.41  -1.602 -0.42  -1.554  0.74   0.37   1.728 -3.258\n",
      " -2.208 -1.392  1.63  -1.316 -1.562 -0.146 -3.02   1.06  -3.388 -0.516\n",
      " -1.864 -1.758  1.494  0.156  0.03  -2.106 -2.618 -2.136  0.268  0.382\n",
      " -1.242  2.034  3.412  2.762  2.586 -0.86  -3.004 -1.37   0.314 -2.108\n",
      "  0.668 -0.168  2.404 -1.468 -1.12   1.486  0.932  2.61 ]\n",
      "              ***Final Evaluation***\n",
      "\n",
      "Confusion Matrix:\n",
      " [[138  32]\n",
      " [180 188]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.81      0.57       170\n",
      "           1       0.85      0.51      0.64       368\n",
      "\n",
      "    accuracy                           0.61       538\n",
      "   macro avg       0.64      0.66      0.60       538\n",
      "weighted avg       0.72      0.61      0.62       538\n",
      "\n",
      "MCC                0.3050942109058486\n",
      "See file 'Classification.txt' for final classifications and confidence scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.926  0.426 -2.712 -0.192 -1.012 -3.338  1.426 -0.71  -0.128 -3.528\n",
      " -1.786 -3.168 -1.418  1.034 -1.9   -0.552 -2.812 -0.044 -3.26  -2.956\n",
      "  0.014  1.442 -0.084 -0.578 -3.24   1.628 -1.338  3.574 -0.866 -3.584\n",
      "  1.118  3.556  0.768  2.632  1.606  0.288  0.894  0.212 -2.644 -2.352\n",
      " -2.604  1.81   0.374 -0.042 -2.188 -0.282 -0.806  1.48  -0.412 -3.76\n",
      "  1.372  0.62   1.972 -1.498 -1.37   3.888 -3.504  2.322 -0.644 -0.234\n",
      "  0.51  -0.438  1.62  -2.522 -4.026 -1.088 -0.38  -1.508  1.038 -1.618\n",
      " -2.286 -1.192 -0.246 -0.444  1.922  0.31   0.07  -3.012 -2.49  -1.674\n",
      " -0.158 -1.566 -0.922 -2.012 -3.01   0.016 -1.6   -1.704 -4.06   1.066\n",
      " -2.668 -1.798  0.202 -1.382  1.104 -2.722 -3.102 -2.216  0.494  1.026\n",
      " -1.592  0.202  0.546 -2.724  1.396 -2.638  2.612  1.652  2.652 -1.098\n",
      "  1.906 -0.724 -1.65   1.222 -3.952  2.864  0.248  1.942 -0.578  0.536\n",
      " -1.9    1.78  -0.77   0.374  0.578 -0.18  -3.176  0.206 -1.806  1.92\n",
      " -3.446 -1.66   1.15  -1.328 -2.712 -2.116  2.188 -2.88   1.884  1.44\n",
      " -1.702 -3.154  2.558  0.734 -1.36   2.344  2.516 -2.17  -1.064 -3.002\n",
      " -2.15  -0.262 -1.544  0.558 -0.302  0.644  1.4    1.882 -2.422  0.572\n",
      " -2.448  0.486 -0.002 -2.094 -0.928  0.978  2.388 -0.632 -1.522  0.564\n",
      "  0.048  3.354  1.42   1.332  0.6   -3.114  2.838 -0.962 -1.474  0.954\n",
      "  2.822 -1.722 -3.638 -2.378 -2.016 -2.094 -0.254  1.624  1.842 -1.712\n",
      " -2.088 -2.084 -0.79  -2.116 -2.862  1.722 -1.726 -0.266  0.368 -0.708\n",
      "  1.148  1.85  -1.586  2.128  0.236 -3.336 -2.81  -3.874 -1.49   2.61\n",
      " -0.958  2.162 -2.834  1.776  0.29  -2.772  1.778 -2.72  -0.32  -0.906\n",
      " -1.984 -1.65   1.546 -2.022 -0.404  0.444 -1.39   0.552 -2.132 -1.764\n",
      "  0.246 -2.214 -0.274 -2.826 -1.458 -2.95  -3.23  -0.044  1.882  0.98\n",
      " -1.714 -0.498  0.642  0.002 -3.086 -2.254 -0.782  1.114 -0.294 -0.57\n",
      " -2.778 -0.206 -1.96   0.594 -2.092 -3.592  0.618 -1.918 -1.838  0.206\n",
      " -3.442 -0.298 -0.556  0.612  0.904 -2.07   2.392  0.062 -0.03  -0.826\n",
      " -0.366  0.594 -0.612 -2.368  2.286 -0.508  0.312  2.266 -1.706 -3.656\n",
      "  2.446 -0.236  2.876 -0.262  2.66  -1.08  -0.712 -0.004  0.666 -0.536\n",
      " -0.652  2.028  0.424  0.086 -1.616  0.156  0.17   0.924 -4.028 -1.148\n",
      "  0.808  0.436 -3.034 -3.128 -1.19  -3.234 -1.516 -0.234  0.418 -0.034\n",
      " -0.548 -2.728  3.496  3.392  0.95  -2.594 -2.442  1.416 -0.184  4.214\n",
      "  2.818 -0.242 -0.516 -3.308  0.62  -4.052  0.02  -0.794 -2.654  1.446\n",
      "  0.904 -3.17  -1.124 -1.606 -0.474 -1.076 -0.496 -0.162 -0.454 -1.26\n",
      "  0.634 -4.062 -2.002 -0.23  -0.204 -1.952 -2.776 -0.672 -1.68   1.062\n",
      " -1.272 -1.698 -0.596 -4.06  -2.164 -0.386 -0.244 -0.164 -1.78   2.422\n",
      " -0.064  2.076 -4.038 -2.078 -2.168 -1.774 -0.28  -2.852 -3.056 -3.43\n",
      " -1.012  0.954  2.43  -1.768 -2.928 -1.034 -1.878  0.64  -2.444 -1.63\n",
      "  0.564 -2.678 -3.044  1.084 -2.354  2.302 -0.606  0.824 -2.638  1.83\n",
      " -0.79  -1.822  0.138 -2.732 -2.604  1.134  0.75   0.266  1.366  2.272\n",
      "  3.656 -2.018 -3.968 -0.458 -3.418  1.718  0.202 -0.43   0.296 -2.016\n",
      " -0.462  0.288 -3.002 -1.406  0.032  2.892  1.7   -2.012  1.57  -1.818\n",
      "  1.424  2.752 -1.024  1.354 -1.652  0.692 -0.368 -0.772  2.13   0.332\n",
      " -2.432 -0.272  0.224 -3.262 -0.45  -1.14  -2.692  3.012 -0.66  -2.302\n",
      " -1.414 -0.436  2.15   2.894 -3.964  1.74  -1.724 -1.402 -0.358 -1.536\n",
      " -2.512  2.088 -0.83  -0.276 -2.692  1.058 -1.25   0.1   -3.178 -2.188\n",
      " -0.638 -2.222  1.562 -1.728 -2.718 -0.474 -2.376  4.122  3.038  1.224\n",
      " -1.97   0.904 -3.384  1.75  -3.242  0.392 -1.646 -3.228  2.034 -2.5\n",
      " -0.13  -1.374 -0.294  0.626  2.86  -0.792 -0.882 -0.002 -2.87   0.776\n",
      " -2.724 -2.104 -2.806 -2.908 -1.664 -0.344 -0.748 -0.678 -2.076  2.912\n",
      " -1.628  0.33  -3.154  1.764 -4.088  1.732  0.482 -2.956 -1.106 -4.172\n",
      "  1.284 -0.28  -1.678 -0.22  -2.134 -2.698 -2.824 -1.232  2.224 -0.588\n",
      " -3.928 -0.726  2.82  -2.724  0.65   2.756  0.736  1.51  -2.5   -2.004\n",
      " -4.152 -0.284 -1.384 -2.046 -2.17  -0.186 -1.82   1.204]\n",
      "              ***Final Evaluation***\n",
      "\n",
      "Confusion Matrix:\n",
      " [[160  28]\n",
      " [180 170]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.85      0.61       188\n",
      "           1       0.86      0.49      0.62       350\n",
      "\n",
      "    accuracy                           0.61       538\n",
      "   macro avg       0.66      0.67      0.61       538\n",
      "weighted avg       0.72      0.61      0.62       538\n",
      "\n",
      "MCC                0.3329543977916777\n",
      "See file 'Classification.txt' for final classifications and confidence scores\n"
     ]
    }
   ],
   "source": [
    "file = \"AC_dataset.csv\"\n",
    "Training_Set, Testing_Set, labels  = Train_Test_Split(file)\n",
    "IT_list, LT_list, IV_list, LV_list = CV(Training_Set)\n",
    "\n",
    "for i in range(len(IT_list)): #For every CV fold\n",
    "    classData                   = LT_list[i].to_numpy()\n",
    "    inData                      = IT_list[i].to_numpy()\n",
    "    ValData                     = IV_list[i]\n",
    "    Vallabel                    = LV_list[i]\n",
    "\n",
    "    minClass, minSize, maxSize  = find_minority_class(classData)\n",
    "    BF                          = Balance_ratio(maxSize, minSize)\n",
    "    Input_folds, Output_folds   = Balance_Folds(BF, inData, classData, minClass, minSize) #balance() and balance_data() functions are called under this\n",
    "\n",
    "    BF_RFC                      = BF_fitting(Input_folds, Output_folds)\n",
    "    Prob_matrix                 = BF_validate(BF_RFC, ValData)\n",
    "\n",
    "    Final_vote, Sum_PD, Sum_SNP = Weighted_Vote(Prob_matrix, BF)\n",
    "    S_Out                       = Final_score(Sum_PD, Sum_SNP, BF)\n",
    "\n",
    "    evalutation(Vallabel, Final_vote, S_Out)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f572db9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 337.844,
   "position": {
    "height": "359.844px",
    "left": "1536px",
    "right": "20px",
    "top": "112px",
    "width": "354px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
