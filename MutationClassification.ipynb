{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fcba82d",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d94d2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Example 2 is inbalanced data set; ~2200 in PD and ~1100 in SNP\\n    Goal is to predict if mutation is SNP or PD\\n    CV branch\\n    \\n    Total samples: 3368\\n    2254 PD samples\\n    1111 SNP samples\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Example 2 is inbalanced data set; ~2200 in PD and ~1100 in SNP\n",
    "    Goal is to predict if mutation is SNP or PD\n",
    "    CV branch\n",
    "    \n",
    "    Total samples: 3368\n",
    "    2254 PD samples\n",
    "    1111 SNP samples\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5737f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Imports the required libraries and packages\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd  #Import for data manipulation in dataframes\n",
    "import numpy as np  #Array manipulation and calculates mean\n",
    "\n",
    "import random as rd\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import(\n",
    "    matthews_corrcoef,  # MCC for evaluation\n",
    "    balanced_accuracy_score, #hyperparameter evaluation\n",
    "    f1_score,  #hyperparameter evaluation\n",
    "    confusion_matrix,  # confusion matrix for classification evalutation\n",
    "    classification_report #Return the F1, precision, and recall of a prediction\n",
    "    )\n",
    "from sklearn.model_selection import(\n",
    "    train_test_split,  # Splits data frame into the training set and testing set\n",
    "    GridSearchCV,  # Cross validation to improve hyperparameters\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold, # K-fold CV\n",
    "    GroupKFold\n",
    "        )\n",
    "from sklearn.ensemble import RandomForestClassifier #SK learn API for classificastion random forests\n",
    "from sklearn.tree import DecisionTreeClassifier #Single tree decisions \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier #allows for confidence scores to be predicted for each\n",
    "np.set_printoptions(threshold=np.inf, precision=3) #full array printing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cd3ed6",
   "metadata": {},
   "source": [
    "### Generate random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa3af259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Seed(): #Generates a random seed\n",
    "    random1 = rd.randrange(1,100) #Random integet between 1 and 100\n",
    "    random2 =  time.time() #Time since UTC epoch\n",
    "    Seed = int(random2//random1//1000)\n",
    "    return Seed### Generate random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5506fbc1",
   "metadata": {},
   "source": [
    "### Manual K-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "503e943b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2v4m', '2xms', '2if1', '2xwt', '1b1e', '2hw5', '3di2', '1ivh', '1hyn', '3zwf', '2bwg', '3cot', '1og2', '3f1r', '3apa', '2bb5', '2vze', '1z5w', '1rtk', '3l6y', '1iri', '1as4', '1ax8', '2h04', '1d5v', '2h8k', '2cs1', '1tdh', '2ocp', '1a02', '3s6n', '3lbs', '1d1t', '3bn3', '3iai', '1jsg', '1p49', '3g33', '1mq0', '1h6f', '1v04', '1umk', '2w2m', '1z83', '1xsl', '3ldx', '3ans', '2pzd', '3n1o', '1t2k', '1t38', '1z6c', '3c6g', '3kzd', '3qe9', '2de0', '3con', '3p0f', '1jcn', '3iar', '2yhf', '3ee6', '2ns2', '3dls', '3e3r', '1jv0', '1fqy', '3nxo', '3gfw', '1w6j', '3lbx', '3dlq', '3gqc', '3o9l', '2l4m', '3qvu', '3fuf', '2ozo', '1rgw', '3hff', '2c9y', '1zsy', '1dmw', '2l5s', '1j04', '3ozl', '3d25', '3p1j', '1wrm', '1r54', '3c2i', '2yrg', '2l9m', '2nz2', '1f4o', '1t2f', '1brq', '3gki', '1k62', '2c3n', '2clp', '1gh7', '1y9r', '1ihi', '2coe', '3thc', '1ijq', '3nya', '1jbi', '3na3', '2l7z', '2bo9', '3hme', '1lf8', '3dal', '1v5w', '2zoc', '3fer', '2vsw', '2emj', '2i6s', '2kmu', '2doa', '1ug3', '1hdr', '1i3d', '1fsu', '1x9n', '1p4r', '2lkj', '1efh', '2gw6', '2nvu', '3smq', '1ufx', '3gzc', '1dic', '1imv', '3ghn', '1ald', '3rl7', '3au4', '2gf9', '1eak', '2l7b', '1aut', '2ax4', '1w72', '3e04', '2af9', '3r2p', '2nn6', '3lxp', '3cmy', '2xzz', '1x0x', '2a7r', '2obd', '2h3n', '3p56', '3ojy', '1o1u', '2lcf', '2ger', '1ohd', '3p08', '3n2g', '2esl', '3pv6', '2l0b', '3ff3', '1j1d', '2b3y', '1uzq', '3ajb', '1g55', '2vm6', '2yuj', '1fuj', '1deu', '2qnk', '3ry5', '3qum', '2zv2', '2d85', '1kfu', '3p8c', '1d7p', '3d34', '2vns', '2kr6', '1eg3', '1apy', '3i4u', '2hi4', '2jdg', '2fie', '1bqq', '2obv', '1ya4', '1bp1', '2pfr', '3k75', '2cw6', '3lkj', '2fp0', '1kwm', '2aw2', '3u2o', '1agw', '2d9z', '3mt5', '1fdq', '3i69', '1pl8', '2ygg', '1dtw', '2fim', '2byj', '3pm0', '3t1p', '2ds4', '3b2d', '1woj', '2i7a', '2g6z', '2hlz', '3s41', '2fdb', '1j7v', '3gdh', '2oq5', '2img', '3coi', '3f8u', '1i10', '1ht0', '3omh', '3lrq', '1d8d', '1oll', '3bpn', '2a1x', '3rbg', '3sf8', '1z3u', '2ew9', '2yhd', '3ifa', '1z29', '3si0', '3zs2', '3igu', '2xrc', '3bzh', '3sh4', '1e51', '2ko0', '1g5w', '1iju', '2bkq', '2a8b', '3p70', '2z0w', '2b5i', '2jjs', '1zrh', '2iw2', '1x86', '3kys', '3inb', '2ka3', '1h8u', '3dwb', '1u7t', '2di7', '2y96', '3rig', '1ds6', '3se6', '1rx0', '2q4r', '2eax', '3sje', '2yqp', '1ebm', '1l1f', '2kll', '2ozl', '1fq3', '2bka', '2h06', '3fy7', '1ua2', '3ecr', '1ext', '2r17', '2hye', '2yrb', '2edo', '2k27', '1yxm', '1r5i', '2j6l', '3txo', '1z10', '2i3c', '3p4u', '3eo3', '2wh0', '2rr3', '1oip', '1mdu', '1b08', '1f8u', '2nup', '1ey2', '2b8w', '1sl6', '1r0d', '3m1n', '2kef', '3s9l', '1au8', '1vry', '2fvl', '2ys1', '2nru', '3qoa', '2pqt', '1j1l', '1war', '2xpg', '2gk9', '3t6a', '3mpx', '1a22', '3oj2', '3am8', '3ff6', '3hxa', '1uwy', '1h9z', '4gtu', '2ag5', '2e2w', '3cjj', '2om5', '2zw3', '2arq', '2zv6', '2lct', '2x6u', '3exe', '1itb', '2xsn', '1sir', '3g1n', '3s98', '3shw', '1m75', '2vdx', '1li1', '1ifr', '1q8m', '2b49', '1wb0', '2kbr', '3hfv', '2ebk', '3kq4', '1e31', '1r46', '3p3h', '3bua', '2l0w', '3jzc', '3tl0', '3d7t', '2gse', '1a7a', '2ary', '2oc3', '1ant', '2dmo', '3alq', '2x4d', '1orf', '1so0', '3qik', '3o4o', '1hne', '3q2u', '2nzt', '3ffm', '2w8r', '1lv2', '2f9q', '3orh', '3nsi', '2g76', '3ugn', '1ek6', '2c2z', '2jxc', '3mr2', '3cos', '2l3l', '1ln2', '3n9y', '2l4c', '3k2i', '3b5h', '1onq', '2kki', '2e5j', '3b7k', '2nnh', '3i2n', '2h63', '1t09', '3d85', '3c22', '2i50', '2hgs', '2w2j', '2xn5', '2jqk', '2iik', '3gro', '3mva', '1vcu', '1t0p', '2oh2', '2k1a', '3ifw', '3rmu', '3zqs', '1do8', '1i1n', '1roz', '2y9u', '3sn5', '3g9v', '2c2n', '1zbq', '1ish', '2gy7', '3pls', '2z7s', '2gyp', '3lpp', '2ixm', '2vct', '1f13', '2bwj', '2gwh', '2qym', '1zjk', '2qlf', '2zkm', '1zua', '3tac', '2p0r', '1im9', '2brf', '2lk2', '1fj2', '3p0l', '2xik', '1shr', '2r3v', '3iw4', '1v82', '2c0l', '3qof', '1xww', '2h5g', '1wyw', '3ps4', 'PDB file (/acrm/data/pdb/pdb1cnt.ent) does not contain residue (1182)', '1owh', '1a5e', '1hsz', '3oom', '2y7l', '1hti', '1z00', '2dbj', '1itq', '3lue', '1jr2', '2pm8', '1p0s', '3ogt', '1u5d', '3gc8', '2vux', '1eig', '2ivu', '1gvl', '1fb1', '1p22', '2e3o', '1exz', '2xdv', '2hsp', '3qd6', '2v5e', '2xwd', '2jbm', '1ekv', '2yu6', '3tiw', '2v1x', '3pwh', '1yhn', '3bhd', '1l9x', '3a7e', '2enq', '2j91', '3e9k', '1pq3', '2e0a', '2xnx', '2gzk', '3q9p', '2znn', '2p03', '1ybw', '2ysq', '2f57', '2d1k', '3fby', '3mdj', '1wsr', '1quu', '1cd9', 'PDB file (/acrm/data/pdb/pdb2nr8.ent) does not exist', '3qkj', '2ha8', '3fvy', '2dn6', '2ckj', '2zij', '1bhg', '2fxm', '3i2b', '3ay4', '2zoq', '1xwd', '2ep8', '1m27', '1tzs', '1csb', '2xt3', '2wno', '3bpt', '3nkx', '1u8f', '3syx', '3qk3', '1xm9', '2cbz', '3kqo', '2vay', '3stk', '1fw1', '3ikm', '1ik9', '1qki', '2xjc', '2nz6', '2vig', '2kem', '2ctf', '2zej', '3hig', '2lej', '2qql', '2f2s', '3qyb', '2kum', '1zhr', '3tf3', '2pfi', '3isq', '2dah', '3a6p', '1awb', '1hlg', '2ppl', '1d6n', '1jmo', '3bu3', '3msx', '2q3y', '2jlp', '3p3p', '1lw3', '2gjx', '1jy1', '1xu8', '3ohx', '3qrf', '2vx2', '3ry6', '3e0g', '3k8o', '2fd6', '1jj7', '2aij', '2kzu', '4a04', '1zxq', '1hpt', '4a11', '2a2c', '3rbf', '3fv1', '3of6', '2c0c', '1wlp', '3o8y', '2x7k', '2knv', '3pff', '1yb5', '3bh6', '3ayd', '3abh', '2wwe', '2e2x', '3iuy', '2qq5', '3f1s', '2q8g', '3lfm', '3h95', '1z6z', '3cfw', '1jrh', '3bg3', '2z0e', '3bhi', '2a14', '2gwo', '2zqq', '1tn3', '1y2a', '2j0y', '1hlc', '2rfj', '1jv2', '1nun', '3lwk', '1cb6', '3ozc', '1bj4', '2xiq', '1b53', '3obz', '3g0e', '1i0e', '2wz9', '3d67', '2ox8', '2he3', '3p8f', '3my0', '1m6h', '2j5w', '2l53', '2x0b', '2hqh', '2d8s', '2jvx', '2buj', '1h03', '1n1f', '2yvq', '2jii', '2y7j', '2jq3', '2kri', '3szr', '2o0o', '1xa6', '2wos', '1xmj', '2obi', '1ytq', '2glq', '1csg', '2vaf', '2pla', '2aex', '3q93', '2pid', '3qft', '3evs', '2e9x', '1kv3', '3glr', '1j78', '1ya5', '3n5n', '2x5y', '2r2p', '3mdy', '2iag', '3fcx', '2d9q', '1cvw', '3nks', '1d4a', '2a24', '3k35', '1o8t', '3kcz', '2wl1', '2h31', '3gvv', '2hi9', '3kqi', '2dnf', '1bh5', '3ob2', '2vkq', '1a4v', '2xsw', '3ch4', '1x5d', '2oju', '2aik', '3qs7', '3tju', '3cuq', '3fg6', '2bp1', '1zsv', '2pd6', '1hwg', '3pdf', '2a55', '1hzm', '2iq1', '2wtk', '1m4u', '1axn', '2i7d', '3qlc', '1olz', '3u8u', '2hky', '2bei', '3e4e', '1iur', '1j72', '2xdg', '3a1f', '1z7x', '3lfl', '1yde', '2goo', '3fe4', '2uxw', '3mbg', '1bnk', '2xkw', '1pdg', '3d59', '3fe1', '2vr2', '2ksn', '3kse', '2kw1', '1jbq', '4a35', '2g2b', '1g91', '3k6s', '1rj8', '2e9l', '2i0e', '3hy8', '1xdl', '1qg3', '3ow9', '2das', '3b63', '1stf', '1saw', '1us1', '3kq0', '3t1i', '2k40', '1fdh', '3n2z', '2v7d', '1s2a', '3te7', '2idx', '3s7j', '3eqm', '2bt2', '2x4f', '1ggl', '2o10', '6pax', '1iar', '3s35', '2eel', '3r6b', '2xsx', '2gk2', '2o2k', '3c3r', '2h8r', '1bqu', '2wj7', '1jei', '3oai', '3d3l', '1fvo', '2v53', '2xrp', '2k21', '1esr', '2e59', '2kv3', '3fo5', '1nzi', '2cpw', '2y3i', '2ffv', '2bdh', '2pe4', '2fy3', '3ews', '3lb6', '3bxw', '3hd6', '1fc0', '1iyf', '1cjy', '3p1n', '13gs', '2ziu', '3puj', '2yqg', '2ra4', '1edm', '3u8z', '2a9j', '1j86', '3odq', '2p39', '2i3y', '3apu', '2oay', '2jsd', '3k5k', '3rc4', '3e77', '1yow', '2a1t', '3bvo', '1w45', '3llk', '3lxx', '1fxy', '3b6y', '3sza', '1e50', '1d8j', '3ooi', '2v62', '2vxp', '2yun', '1z8d', '3qwl', '3g2f', '3b1t', '3qxm', '2fj9', '1d5r', '3rm4', '1s18', '3hy5', '6rlx', '2kb9', '2vcv', '2o8e', '2ove', '1hrk', '2pqf', '2wul', '2anw', '3jui', '1zlg', '2j0f', '3zs0', '1nuf', '3zzw', '2yl4', '1spj', '2pmf', '2jif', '1gwb', '3rv5', '3a1j', '2gdz', '2vgb', '3bju', '2c9w', '4a3a', '3otd', '3dlj', '1qmt', '2rli', '2wok', '1x68', '1ci6', '2di0', '3r62', '3bqd', '1gr3', '1yz8', '1wrd', '3eab', '3ddu', '2klj', '1wuu', '1r42', '2ld4', '2ke1', '2l9n', '3cok', '3s0n', '2k1m'}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('E2.csv')\n",
    "df.sort_values('pdbcode:chain:resnum:mutation', inplace= True, na_position = 'first')\n",
    "# df.to_csv('order.csv'\n",
    "extract = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    split = df['pdbcode:chain:resnum:mutation'][i].partition(':')\n",
    "    PDB = split[0]\n",
    "    extract.append(PDB)\n",
    "print(set(extract))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddcc865",
   "metadata": {},
   "source": [
    "### Clean dataset in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62413d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clean_data(file):\n",
    "    \"\"\" Input:      file        The dataset to read\n",
    "\n",
    "        Returns:    Input       Dataframe with of input features for training\n",
    "                    Output      Dataframe of class labels for each instance in Input\n",
    "\n",
    "        Create, clean and convert dataset E2.csv to PD dataframe. Drops uneeded columns, removes blank spaces, \n",
    "        and applies \"One Hot Encoding\" to convert classes (PD/SNP) to 1/0\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv('E2.csv')\n",
    "\n",
    "    #Remove unrequired column, replace blank spaces, reset index to run from 0\n",
    "    df.drop(['pdbcode:chain:resnum:mutation'], axis=1, inplace=True)\n",
    "    df.dropna() #drop rows with missing values\n",
    "    df.replace(' ', '_', regex=True, inplace=True)\n",
    "    df.reset_index(drop=True, inplace = True)\n",
    "\n",
    "    Input = df.drop('dataset', axis =1)\n",
    "    Output_encoded = pd.get_dummies(df, columns=['dataset']) #Encode the PD and SNP columns\n",
    "    Output = Output_encoded['dataset_pd'].copy().astype('int32') #PD = 1, SNP = 0\n",
    "\n",
    "    return Input, Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb451c9e",
   "metadata": {},
   "source": [
    "### Split into training and testing, generate RF (whole dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbfacd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Input, Output, Seed):\n",
    "    \"\"\" Input:      Input           Dataframe with of input features for training\n",
    "                    Output          Dataframe of class labels for each instance in Input\n",
    "                    Seed            Random Seed\n",
    "\n",
    "        Returns:    Input_train     Features training data\n",
    "                    Input_test      Features test data\n",
    "                    Classes_train   Class label training data\n",
    "                    Classes_test    Class label test data\n",
    "\n",
    "        80% training and 20% testing split. Strartify ensures fixed poportion of labels are in both sets. \n",
    "        Random forest defined as RFC with x trees, random seed. Outputs the training data to files.\n",
    "        \"\"\"\n",
    "\n",
    "    Input_train, Input_test, Classes_train, Classes_test = train_test_split(Input, Output, train_size = 0.8, random_state= Seed, stratify=Output) \n",
    "    Initial_RFC = RandomForestClassifier(random_state = Seed, verbose = 1)\n",
    "    Initial_RFC.fit(Input_train, Classes_train)\n",
    "\n",
    "    with open('Training Data.txt', 'w') as file: #Writes training features to text file\n",
    "        file.write(Input_train.to_string())\n",
    "    with open('Class labels.txt', 'w') as file: #Writes training class labels to text file\n",
    "        file.write(Classes_train.to_string())\n",
    "    with open('Test Data.txt', 'w') as file: #Writes testing features to text file\n",
    "        file.write(Input_test.to_string())\n",
    "    with open('Test labels.txt', 'w') as file: #Writes testing class labels to text file\n",
    "        file.write(Classes_test.to_string())\n",
    "\n",
    "    return Initial_RFC, Input_test, Classes_test, Input_train, Classes_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8b7f5",
   "metadata": {},
   "source": [
    "### Initial evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf8d9857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(Initial_RFC, Input_test, Classes_test):\n",
    "    \"\"\" Input:  Input_test      Features test data\n",
    "                Classes_test    Class label test data\n",
    "\n",
    "        Evaluates the training data before balancing. Random forest classifier makes prediction using the test features. True values \n",
    "        are the class labels testing data\n",
    "    \"\"\"\n",
    "\n",
    "    Output_pred = Initial_RFC.predict(Input_test) #Always perdict on the unseen test data, as train has been used by the estimastor\n",
    "    print(f\"              **Initial Evaluation**\\n\")\n",
    "    print(f\"Confusion Matrix:\\n {confusion_matrix(Classes_test, Output_pred)}\")\n",
    "    print(f\"{classification_report(Classes_test, Output_pred)}\\nMCC                {matthews_corrcoef(Classes_test, Output_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a335a585",
   "metadata": {},
   "source": [
    "### Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b6924e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_minority_class(classData):\n",
    "    \"\"\" Input:    classData  Array of class labels\n",
    "        Returns:  minClass   The label for the minority class\n",
    "                  minSize    The number of items in the minority class\n",
    "                  maxSize    The number of items in the majority class\n",
    "\n",
    "        Finds information about the inbalance in class sizes\n",
    "    \"\"\"\n",
    "    \n",
    "    Minority_count = 0\n",
    "    Majority_count = 0\n",
    "    for datum in classData:\n",
    "        if datum == 1:\n",
    "            Majority_count += 1\n",
    "        elif datum == 0:\n",
    "            Minority_count += 1\n",
    "\n",
    "    minClass = 0\n",
    "    minSize = Minority_count\n",
    "    maxSize = Majority_count\n",
    "    if Minority_count > Majority_count:\n",
    "        minClass = 1\n",
    "        minSize = Majority_count\n",
    "        maxSize = Minority_count\n",
    "\n",
    "    return minClass, minSize, maxSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6746be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Balance_ratio(maxSize, minSize): \n",
    "    \"\"\" Input:      maxSize     The number of items in the majority class\n",
    "                    minSize     The number of items in the minority class\n",
    "\n",
    "        Returns:    BF          Number of balancing folds\n",
    "\n",
    "        Calculate the number of balancing folds needed using ratio of majority to minority class size. Double to ensure sufficient\n",
    "        majority class instances are sampled, then + 1 to make odd to allow weighted vote.\n",
    "    \"\"\"\n",
    "    Divide = maxSize/minSize\n",
    "    BF = (2 * round(Divide)) + 1 #Double ratio to nearest integer\n",
    "    return BF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d1241bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance(inData, classData, minClass, minSize):\n",
    "    \"\"\" Input:    inData          array of input data\n",
    "                  classData       array of classes assigned\n",
    "                  minorityClass   class label for the minority class\n",
    "                  minoritySize    size of the minority class\n",
    "\n",
    "        Returns: array of indexes that are of interest for a balanced dataset\n",
    "\n",
    "        Perform the actual balancing between SNPs and PDs\n",
    "    \"\"\"\n",
    "    usedLines = [False] * len(inData) #Array of false for length of data\n",
    "    for i in range(len(inData)):\n",
    "        if classData[i] == minClass:\n",
    "            usedLines[i] = True\n",
    "            \n",
    "    usedCount = 0\n",
    "    while usedCount < minSize:\n",
    "        i = rd.randrange(len(inData))\n",
    "        if usedLines[i] == False:\n",
    "            usedCount += 1\n",
    "            usedLines[i] = True       \n",
    "\n",
    "    return usedLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5c54edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(inData, classData, usedLines):\n",
    "    \"\"\"     Input:     inData      array of input training data\n",
    "                       classData   array of classes assigned to training data\n",
    "                       usedLines   array of line indexes to print\n",
    "\n",
    "            Returns:   Input_balance  Array of balanced input training data\n",
    "                       Label_balance  Array of balanced classes assigned to training data\n",
    "        Create arrays for the input training data and classes, as needed for predicting the probability.\n",
    "        The index [i] is the identifier between the two arrays\n",
    "    \"\"\"\n",
    "    Input_balance = []\n",
    "    Label_balance = []\n",
    "    for i in range(len(inData)):\n",
    "        if usedLines[i]:\n",
    "            Input_balance.append(inData[i])\n",
    "            Label_balance.append(classData[i])\n",
    "            \n",
    "    Input_balance = np.stack(Input_balance, axis =0)\n",
    "    Label_balance = np.stack(Label_balance, axis =0)\n",
    "    \n",
    "    return Input_balance, Label_balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27419a33",
   "metadata": {},
   "source": [
    "### Balance for n folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12239dc9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Balance_Folds(BF, usedLines, Input_balance, Label_balance):\n",
    "    \"\"\" Input:      BF                Number of balancing folds needed\n",
    "                    usedLines         Array of line indexes to print\n",
    "                    Input_balance     Input_balance  Array of balanced input training data\n",
    "                    Label_balance     Array of balanced classes assigned to training data\n",
    "\n",
    "        Returns:    Input_folds       List of 5 balanced arrays of training data\n",
    "                    Output_folds      List of 5 balanced arrays of training data's labels\n",
    "\n",
    "        Perform the balance_data() function n number of balancing fold times. Return lists for training data and labels\n",
    "        where each item is the output of balance_data()\n",
    "    \"\"\"\n",
    "    Input_folds = []\n",
    "    Output_folds = []\n",
    "    for fold in range(BF):\n",
    "        Input_folds.append(Input_balance)\n",
    "        Output_folds.append(Label_balance)\n",
    "\n",
    "\n",
    "    with open('Balanced training data.txt', 'w') as f:\n",
    "        for number, fold in zip(range(BF), Input_folds):\n",
    "            f.write(f\"Fold: {number}\\n\\n{fold}\\n\\n\\n\")\n",
    "        \n",
    "    return Input_folds, Output_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd1aaa",
   "metadata": {},
   "source": [
    "### RFC hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0840d9dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " def Hyperparameter(BF, Input_folds, Output_folds):\n",
    "    \"\"\" Input:      BF                Number of balancing folds needed\n",
    "                    Input_folds       List of 5 balanced arrays of training data\n",
    "                    Output_folds      List of 5 balanced arrays of training data's labels\n",
    "\n",
    "        Returns:    BF_RFC_HP         List of optimized hyperparameters for each RFC\n",
    "\n",
    "        Perform RandomSearchCV on each RFC to optimize number of trees, max depth and max samples\n",
    "    \"\"\"  \n",
    "    estimator = RandomForestClassifier()\n",
    "    param_grid = {\n",
    "                'n_estimators':np.arange(50,500,50),\n",
    "                'max_depth': np.arange(2, 10, 2),\n",
    "                'max_samples': np.arange(0.2, 1.2, 0.2)\n",
    "                  }\n",
    "    BF_RFC_HP = []\n",
    "\n",
    "    for i in range(BF):\n",
    "        HPtuning = RandomizedSearchCV(\n",
    "            estimator,\n",
    "            param_grid, \n",
    "            scoring = 'balanced_accuracy',\n",
    "            cv = 10,\n",
    "            n_jobs = 6, #how many cores to run in parallel\n",
    "            verbose = 2\n",
    "            ).fit(Input_folds[i], Output_folds[i].ravel())\n",
    "        BF_RFC_HP.append(HPtuning.best_params_)\n",
    "    \n",
    "    return(BF_RFC_HP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796af0e8",
   "metadata": {},
   "source": [
    "### Train RFC on balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1decd7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BF_training(BF, Input_folds, Output_folds, BF_RFC_HP, Seed): \n",
    "    \"\"\" Input:      BF              Number of balancing folds\n",
    "                    Input_folds     List of 5 balanced arrays for training data\n",
    "                    Output_folds    List of 5 balanced arrays of training data's labels\n",
    "\n",
    "        Returns:    BF_RFC          List of RFC's trained on data in each balancing fold\n",
    "\n",
    "        Create a model that returns probability predictions for each fold, using Balance_Fold() as input\n",
    "    \"\"\"    \n",
    "    BF_RFC = []\n",
    "    \n",
    "    for i in range(BF):\n",
    "        BF_RFC.append(RandomForestClassifier(\n",
    "                                             random_state = Seed,\n",
    "                                             verbose = 1\n",
    "                                            )) #Generates a RF for each fold \n",
    "        BF_RFC[i].fit(Input_folds[i], Output_folds[i].ravel()) #Fits the RFC to balanced training data    \n",
    "        \n",
    "    return BF_RFC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd278c",
   "metadata": {},
   "source": [
    "#### Test RFC on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acc41cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFC_test(BF_RFC, Input_test):\n",
    "    \"\"\" Input:  BF_RFC          List of RFC's trained on data in each balancing fold\n",
    "                Input_test      20% unseen testing data split before the balancing folds\n",
    "                \n",
    "        Returns:Prob_matrix     List of arrays. Each item is 2D matrix where the 1st dimension is each subset in balancing fold, \n",
    "                                2nd dimension is predicted probability\n",
    "    \n",
    "        Test the trained RFCs on the test set, then for every instance, outputs the predicted probability for each class\n",
    "    \"\"\"\n",
    "    Prob_matrix = [] #Empty list\n",
    "    Prob_matrixlist = []\n",
    "    for i in range(len(BF_RFC)):\n",
    "        Prob_list = BF_RFC[i].predict_proba(Input_test.values)\n",
    "        Prob_matrix.append(Prob_list)   \n",
    "        \n",
    "    with open('Test probabilities.txt', 'w') as f:\n",
    "        for number, line in zip(range(BF), Prob_matrix ):\n",
    "            f.write(f\"Fold: {number}\\n\\n   SNP    PD\\n{line}\\n\\n\\n\")\n",
    "\n",
    "    return Prob_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b8fdd",
   "metadata": {},
   "source": [
    "### Weighted voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71033215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weighted_Vote(Prob_matrix, BF):\n",
    "    \"\"\" Input:      Prob_matrix     List of arrays. 2D matrix where the 1st dimension is each subset in balancing fold, \n",
    "                                    2nd dimension is predicted probability\n",
    "                    BF              Number of balancing folds\n",
    "\n",
    "        Returns:    Final_vote      Weighted vote classification\n",
    "\n",
    "        Calculate the final weighted vote using confidence scores (Sc). Binary classification formula Sc = 2|S0 - 0.5|\n",
    "    \"\"\"\n",
    "    Sc_PD = [] #Empty list\n",
    "    Sc_SNP = [] #Empty list\n",
    "    for i in range(BF):\n",
    "        Sc_PD.append(2* (Prob_matrix[i][:,1] - 0.5)) #Confidence scores for PD, for each fold\n",
    "        Sc_SNP.append(2*(Prob_matrix[i][:,0] - 0.5)) #Confidence scores for SNP, for each fold\n",
    "    \n",
    "\n",
    "    Sum_PD = np.sum(Sc_PD, axis = 0) #Sum of all PD confidence scores. 1D Array\n",
    "    Sum_SNP = np.sum(Sc_SNP, axis = 0) #Sum of all SNP confidence scores. 1D Array     \n",
    "\n",
    "    Vote_arr = [] #Empty list\n",
    "    \n",
    "    for i in range(len(Classes_test)):\n",
    "        if Sum_PD[i] >= Sum_SNP[i]:\n",
    "            Vote_arr.append([1]) #Append PD classifications to list\n",
    "        elif Sum_SNP[i] > Sum_PD[i]:\n",
    "            Vote_arr.append([0]) #Append SNP classifications to list\n",
    "            \n",
    "    Final_vote = np.stack(Vote_arr) #Converts list of arrays to a 2D array, shape (674,1)\n",
    "    Final_vote = Final_vote.ravel() #Flattens 2D array to 1D array\n",
    "        \n",
    "    return(Final_vote, Sum_PD, Sum_SNP) #Returns the final confidence scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f5b78",
   "metadata": {},
   "source": [
    "### Final confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ae1157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Final_score(Sum_PD, Sum_SNP, BF):\n",
    "    \"\"\" Input:      Sum_PD      Sum of confidence score for PD predictions\n",
    "                    Sum_SNP     Sum of confidence score for SNP predictions\n",
    "\n",
    "        Returns:    S_out        Final confidence score\n",
    "\n",
    "        Calculate the final confidence score\n",
    "    \"\"\"\n",
    "    \n",
    "    S_Out = np.abs((Sum_PD - Sum_SNP) /(BF*2))\n",
    "    np.savetxt('S_out.txt', S_Out, \"%.3f\")\n",
    "    \n",
    "    return S_Out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92f36545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evalutation(Classes_test, Final_vote, S_Out):\n",
    "    \"\"\" Input:      Classes_test       Class label test data\n",
    "                    Final_vote         Weighted vote classification\n",
    "\n",
    "        Evaluation metrics from RFC on test data with\n",
    "    \"\"\"\n",
    "    Output_pred = Final_vote\n",
    "    print(f\"              ***Final Evaluation***\\n\")\n",
    "    print(f\"Confusion Matrix:\\n {confusion_matrix(Classes_test, Output_pred)}\")\n",
    "    print(f\"{classification_report(Classes_test, Output_pred)}\\nMCC                {matthews_corrcoef(Classes_test, Output_pred)}\")\n",
    "    \n",
    "    print(f\"See file 'Classification.txt' for final classifications and confidence scores\")\n",
    "    np.savetxt('Classification.txt',\n",
    "           np.column_stack([Final_vote, S_Out]),\n",
    "           fmt = [\"%.0f\",\"%.3f\"],\n",
    "           delimiter =\"      \",\n",
    "           header = \"Final classifications and confidence scores\\n\\n\"\n",
    "          )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa67e232",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b6b8f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              **Initial Evaluation**\n",
      "\n",
      "Confusion Matrix:\n",
      " [[156  67]\n",
      " [ 22 429]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.70      0.78       223\n",
      "           1       0.86      0.95      0.91       451\n",
      "\n",
      "    accuracy                           0.87       674\n",
      "   macro avg       0.87      0.83      0.84       674\n",
      "weighted avg       0.87      0.87      0.86       674\n",
      "\n",
      "MCC                0.6945733389993844\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ***Final Evaluation***\n",
      "\n",
      "Confusion Matrix:\n",
      " [[190  33]\n",
      " [ 63 388]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.85      0.80       223\n",
      "           1       0.92      0.86      0.89       451\n",
      "\n",
      "    accuracy                           0.86       674\n",
      "   macro avg       0.84      0.86      0.84       674\n",
      "weighted avg       0.87      0.86      0.86       674\n",
      "\n",
      "MCC                0.6921809272829552\n",
      "See file 'Classification.txt' for final classifications and confidence scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "Seed                         = Random_Seed()\n",
    "\n",
    "file                         = 'E2.csv'\n",
    "Input, Output                = Clean_data(file)\n",
    "RFC, Input_test, Classes_test, Input_train, Classes_train = train(Input, Output, Seed)\n",
    "test(RFC,Input_test, Classes_test)\n",
    "\n",
    "inData                       = pd.DataFrame(Input_train).to_numpy()\n",
    "classData                    = pd.DataFrame(Classes_train).to_numpy()\n",
    "\n",
    "minClass, minSize, maxSize   = find_minority_class(classData)\n",
    "BF                           = Balance_ratio(maxSize, minSize)\n",
    "usedLines                    = balance(inData, classData, minClass, minSize)\n",
    "\n",
    "Input_balance, Label_balance = balance_data(inData, classData, usedLines)\n",
    "Input_folds, Output_folds    = Balance_Folds(BF, usedLines, Input_balance, Label_balance)\n",
    "\n",
    "BF_RFC_HP                    = Hyperparameter(BF, Input_folds, Output_folds)\n",
    "BF_RFC                       = BF_training(BF, Input_folds, Output_folds, BF_RFC_HP, Seed)\n",
    "Prob_matrix                  = BFC_test(BF_RFC, Input_test)\n",
    "\n",
    "Final_vote, Sum_PD, Sum_SNP  = Weighted_Vote(Prob_matrix, BF)\n",
    "S_Out                        = Final_score(Sum_PD, Sum_SNP, BF)\n",
    "\n",
    "evalutation(Classes_test, Final_vote,S_Out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab48735",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 337.844,
   "position": {
    "height": "40px",
    "left": "1536px",
    "right": "20px",
    "top": "112px",
    "width": "354px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
