{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fcba82d",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5737f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2 is inbalanced data set; ~2200 in PD and ~1100 in SNP\n",
    "# Goal is to predict if protein is a SNP or PD\n",
    "#ImprovedBalancing branch\n",
    "\n",
    "#Imports the required libraries and packages\n",
    "import pandas as pd  #Import for data manipulation in dataframes\n",
    "import numpy as np  #Array manipulation and calculates mean\n",
    "import matplotlib.pyplot as plt  #Graphing and plotting\n",
    "# show figures in jupyter instead of prompt window\n",
    "%matplotlib inline \n",
    "\n",
    "import random as rd\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.metrics import(\n",
    "    matthews_corrcoef,  # CC for evaluation\n",
    "    f1_score,  #F1 score for evaluation\n",
    "    balanced_accuracy_score, roc_auc_score, make_scorer,  #Scoring metrics\n",
    "    confusion_matrix,  #Creates the confusion matrix - stats on how accurate the test set output is\n",
    "    classification_report #Returns the F1 socre, precision, and recall of a prediction using a given model\n",
    "    )\n",
    "from sklearn.model_selection import(\n",
    "    train_test_split,  # Splits data frame into the training set and testing set\n",
    "    GridSearchCV,  # Cross validation to improve hyperparameters\n",
    "    StratifiedKFold\n",
    "        )\n",
    "from sklearn.ensemble import RandomForestClassifier #SK learn API for classificastion random forests\n",
    "from sklearn.tree import DecisionTreeClassifier #Single tree decisions \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils import shuffle #shuffles rows\n",
    "from sklearn.neighbors import KNeighborsClassifier #allows for confidence scores to be predicted for each\n",
    "\n",
    "np.set_printoptions(threshold=np.inf) #full array printing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ffd97",
   "metadata": {},
   "source": [
    "## Random Seed function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6be2ee9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37961"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Random_Seed(): #Generates a random seed\n",
    "    random1 = rd.randrange(1,100) #Random integet between 1 and 100\n",
    "    random2 =  time.time() #Time since UTC epoch\n",
    "    Seed = int(random2//random1//1000)\n",
    "    return Seed\n",
    "Random_Seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddcc865",
   "metadata": {},
   "source": [
    "## Read the whole dataset - revis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62413d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 3368\n",
      "2254 PD samples\n",
      "1111 SNP samples\n"
     ]
    }
   ],
   "source": [
    "#Create, clean and convert dataset E2.csv to PD dataframe**\n",
    "df = pd.read_csv('E2.csv')  #Create PD data frame from .csv\n",
    "df.drop(['pdbcode:chain:resnum:mutation'], axis=1, inplace=True)  #Removes unrequired columns. PDBcode may be needed for manual validation \n",
    "df.columns = df.columns.str.replace(' ', '_')  # Removes any blank attributes\n",
    "df.replace(' ', '_', regex=True, inplace=True)  # Replace all blank spaces with underscore (none were present)\n",
    "df.reset_index(drop=True, inplace = True) #Resets index numbering from 0 and drops column\n",
    "Input = df.drop('dataset', axis =1).fillna('0') #DF of input instances for classification training. Unknown attributes assigned 0\n",
    "Output_encoded = pd.get_dummies(df, columns=['dataset']) #One hot encoding dataset column so \"PD\" and \"SNP\" attributes are numerical 0 or 1\n",
    "Output = Output_encoded['dataset_pd'].copy().astype('int32') #Dataframe with 1 column where 1 = PD, 0 = SNP, integer\n",
    "\n",
    "print(\"Total samples:\", len(df))\n",
    "print(f\"{len(df.loc[df['dataset'] == 'pd'])} PD samples\")\n",
    "print(f\"{len(df.loc[df['dataset'] == 'snp'])} SNP samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb451c9e",
   "metadata": {},
   "source": [
    "## Split into training and testing, generate RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bbfacd1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    4.4s finished\n"
     ]
    }
   ],
   "source": [
    "Input_train, Input_test, Classes_train, Classes_test = train_test_split(Input, Output, train_size = 0.8, random_state=42, stratify=Output) #80% training and 20% testing split. Strartify ensures fixed poportion of output labels is in both sets. Input attributes and class labels, training attributes and class label etc\n",
    "start=time.time() #Start timer for inital training model building\n",
    "RFC = RandomForestClassifier(random_state = 42, n_estimators = 1000, verbose = 1) #Defines the Random Forest. 42 seeds, 1000 trees\n",
    "RFC.fit(Input_train, Classes_train) #Generates a random forest from the training data\n",
    "\n",
    "with open('Training Data.txt', 'w') as file: #Writes class labels for all instances to text file\n",
    "    file.write(Input_train.to_string())\n",
    "    \n",
    "with open('Class labels.txt', 'w') as file: #Writes class labels for all instances to text file\n",
    "    file.write(Classes_train.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2cdf65",
   "metadata": {},
   "source": [
    "### Plotting a tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6f86678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (10,10), dpi=130)\n",
    "# tree.plot_tree(RFC.estimators_[45],\n",
    "#                feature_names = None, \n",
    "#                class_names= None,\n",
    "#                filled = True)\n",
    "# plt.show()\n",
    "# plt.savefig('clf_individualtree.png', bbox_inches = 'tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8b7f5",
   "metadata": {},
   "source": [
    "## Training (revisit params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cf8d9857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[145  78]\n",
      " [ 27 424]]\n",
      "MCC:\n",
      " 0.6371468255225344\n",
      "F1:\n",
      " 0.8898216159496328\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.65      0.73       223\n",
      "           1       0.84      0.94      0.89       451\n",
      "\n",
      "    accuracy                           0.84       674\n",
      "   macro avg       0.84      0.80      0.81       674\n",
      "weighted avg       0.84      0.84      0.84       674\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# StandardScaler().fit(X_train).transform(X_train) #Scales data \n",
    "# pipeline = make_pipeline( #Sets the random forest parameters\n",
    "#     StandardScaler(),\n",
    "#     LogisticRegression(solver='saga', max_iter=2000),\n",
    "#     verbose=2\n",
    "#  )\n",
    "RFC.get_params()\n",
    "# Evaluation of training before weighted vote\n",
    "\n",
    "Output_pred = RFC.predict(Input_test) #Always perdict on the unseen test data, as train has been used by the estimastor\n",
    "print(f\"Confusion Matrix:\\n {confusion_matrix(Classes_test, Output_pred)}\")\n",
    "print(f\"MCC:\\n {matthews_corrcoef(Classes_test, Output_pred)}\")\n",
    "print(\"F1:\\n\", f1_score(Classes_test, Output_pred))\n",
    "print(classification_report(Classes_test, Output_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ae3dc",
   "metadata": {},
   "source": [
    "## Weighted Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f4ba36",
   "metadata": {},
   "source": [
    "### Counting the PD and SNP in whole training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1f79e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PD_Count = 0\n",
    "# # SNP_Count = 0\n",
    "# # for i in y_train:\n",
    "# #     if i == 1:\n",
    "# #         PD_Count += 1\n",
    "# #     elif i == 0:\n",
    "# #         SNP_Count += 1\n",
    "# #     else:\n",
    "# #         pass\n",
    "# # print(\"PD:\",PD_Count, \"\\nSNP:\", SNP_Count)\n",
    "\n",
    "# PD_Count = 0\n",
    "# SNP_Count = 0\n",
    "# for i in Majority['Class']:\n",
    "#     if i == 1:\n",
    "#         PD_Count += 1\n",
    "#     elif i == 0:\n",
    "#         SNP_Count += 1\n",
    "#     else:\n",
    "#         pass\n",
    "# for i in Minority['Class']:\n",
    "#     if i == 1:\n",
    "#         PD_Count += 1\n",
    "#     elif i == 0:\n",
    "#         SNP_Count += 1\n",
    "#     else:\n",
    "#         pass\n",
    "# print(\"PD:\",PD_Count, \"\\nSNP:\", SNP_Count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc0bd2d",
   "metadata": {},
   "source": [
    "## Balancing via array index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0435341d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     dataset_pd\n",
      "0             1\n",
      "1             1\n",
      "2             0\n",
      "3             0\n",
      "4             1\n",
      "..          ...\n",
      "669           1\n",
      "670           1\n",
      "671           1\n",
      "672           0\n",
      "673           0\n",
      "\n",
      "[674 rows x 1 columns]\n",
      "[[2929    1]\n",
      " [1898    1]\n",
      " [1084    0]\n",
      " [1246    0]\n",
      " [ 748    1]\n",
      " [3361    1]\n",
      " [ 407    1]\n",
      " [1809    0]\n",
      " [2265    0]\n",
      " [ 840    1]\n",
      " [3053    1]\n",
      " [ 165    1]\n",
      " [  58    1]\n",
      " [1169    0]\n",
      " [1059    1]\n",
      " [2767    0]\n",
      " [2485    0]\n",
      " [ 174    0]\n",
      " [ 782    1]\n",
      " [2280    0]\n",
      " [3315    1]\n",
      " [1650    0]\n",
      " [1038    1]\n",
      " [1955    1]\n",
      " [3335    1]\n",
      " [1718    1]\n",
      " [2153    1]\n",
      " [2973    1]\n",
      " [  65    1]\n",
      " [2832    1]\n",
      " [1725    1]\n",
      " [1554    1]\n",
      " [  85    1]\n",
      " [2977    1]\n",
      " [3206    1]\n",
      " [1437    1]\n",
      " [2953    0]\n",
      " [ 207    0]\n",
      " [ 832    1]\n",
      " [ 214    1]\n",
      " [2156    0]\n",
      " [2053    1]\n",
      " [ 136    0]\n",
      " [2670    1]\n",
      " [1051    1]\n",
      " [ 226    1]\n",
      " [1073    1]\n",
      " [2237    0]\n",
      " [ 554    1]\n",
      " [1624    1]\n",
      " [2363    1]\n",
      " [2558    1]\n",
      " [ 984    1]\n",
      " [ 331    1]\n",
      " [ 418    1]\n",
      " [ 600    1]\n",
      " [2924    1]\n",
      " [2536    0]\n",
      " [1561    1]\n",
      " [ 277    1]\n",
      " [1811    0]\n",
      " [1433    0]\n",
      " [2169    1]\n",
      " [2745    1]\n",
      " [2314    0]\n",
      " [ 602    1]\n",
      " [1401    1]\n",
      " [1464    0]\n",
      " [1837    1]\n",
      " [ 849    1]\n",
      " [ 355    1]\n",
      " [3287    0]\n",
      " [  15    1]\n",
      " [ 711    0]\n",
      " [ 627    0]\n",
      " [ 870    1]\n",
      " [ 245    1]\n",
      " [2864    1]\n",
      " [1749    1]\n",
      " [1150    1]\n",
      " [1976    0]\n",
      " [2287    1]\n",
      " [3048    1]\n",
      " [2288    1]\n",
      " [2489    1]\n",
      " [1105    0]\n",
      " [3366    1]\n",
      " [1713    1]\n",
      " [1751    0]\n",
      " [2645    1]\n",
      " [2853    1]\n",
      " [1582    1]\n",
      " [3167    1]\n",
      " [2620    1]\n",
      " [3194    1]\n",
      " [ 580    1]\n",
      " [1929    1]\n",
      " [  72    0]\n",
      " [2863    0]\n",
      " [1939    1]\n",
      " [2592    0]\n",
      " [1678    1]\n",
      " [2981    1]\n",
      " [ 252    1]\n",
      " [2644    0]\n",
      " [3054    1]\n",
      " [1368    1]\n",
      " [1763    1]\n",
      " [ 880    1]\n",
      " [2860    0]\n",
      " [ 250    0]\n",
      " [2772    0]\n",
      " [ 665    0]\n",
      " [ 420    0]\n",
      " [2529    1]\n",
      " [1410    1]\n",
      " [3019    1]\n",
      " [ 452    1]\n",
      " [1042    0]\n",
      " [ 910    1]\n",
      " [1128    0]\n",
      " [3171    0]\n",
      " [2649    0]\n",
      " [2305    0]\n",
      " [2919    0]\n",
      " [2657    1]\n",
      " [1248    0]\n",
      " [1120    1]\n",
      " [2896    0]\n",
      " [ 379    1]\n",
      " [ 185    1]\n",
      " [  76    1]\n",
      " [ 278    0]\n",
      " [1828    1]\n",
      " [1750    1]\n",
      " [2269    1]\n",
      " [3016    0]\n",
      " [ 684    1]\n",
      " [3302    1]\n",
      " [1373    1]\n",
      " [1330    0]\n",
      " [1380    1]\n",
      " [ 753    1]\n",
      " [2972    1]\n",
      " [2136    0]\n",
      " [ 576    1]\n",
      " [1768    0]\n",
      " [1092    1]\n",
      " [1219    0]\n",
      " [  37    1]\n",
      " [2016    0]\n",
      " [2822    1]\n",
      " [ 667    0]\n",
      " [1260    0]\n",
      " [ 758    0]\n",
      " [ 985    1]\n",
      " [2809    0]\n",
      " [1285    1]\n",
      " [1507    0]\n",
      " [2875    1]\n",
      " [2814    1]\n",
      " [2572    0]\n",
      " [2241    1]\n",
      " [  17    1]\n",
      " [3280    1]\n",
      " [1020    1]\n",
      " [2362    1]\n",
      " [2193    1]\n",
      " [1463    1]\n",
      " [ 701    1]\n",
      " [2696    1]\n",
      " [ 997    1]\n",
      " [3268    1]\n",
      " [ 478    1]\n",
      " [3238    1]\n",
      " [2966    1]\n",
      " [  77    1]\n",
      " [ 632    1]\n",
      " [1427    1]\n",
      " [2227    1]\n",
      " [1961    0]\n",
      " [1861    1]\n",
      " [2655    1]\n",
      " [2728    1]\n",
      " [3296    1]\n",
      " [1815    1]\n",
      " [1229    1]\n",
      " [3246    1]\n",
      " [ 846    1]\n",
      " [  56    1]\n",
      " [1641    0]\n",
      " [1389    0]\n",
      " [2650    0]\n",
      " [2312    1]\n",
      " [ 862    0]\n",
      " [3205    0]\n",
      " [ 809    1]\n",
      " [2360    1]\n",
      " [2424    1]\n",
      " [2081    0]\n",
      " [2114    1]\n",
      " [ 613    1]\n",
      " [2487    0]\n",
      " [2626    1]\n",
      " [2638    1]\n",
      " [ 368    1]\n",
      " [1333    1]\n",
      " [1946    1]\n",
      " [ 197    1]\n",
      " [ 947    1]\n",
      " [2508    1]\n",
      " [ 242    1]\n",
      " [2890    1]\n",
      " [2719    1]\n",
      " [1289    0]\n",
      " [3079    1]\n",
      " [2563    0]\n",
      " [2847    1]\n",
      " [ 572    1]\n",
      " [2311    0]\n",
      " [ 346    1]\n",
      " [2704    1]\n",
      " [2014    1]\n",
      " [ 945    1]\n",
      " [  24    1]\n",
      " [ 890    1]\n",
      " [1230    1]\n",
      " [ 477    1]\n",
      " [1762    1]\n",
      " [2800    1]\n",
      " [3347    1]\n",
      " [1452    0]\n",
      " [1503    0]\n",
      " [2180    1]\n",
      " [1249    0]\n",
      " [2471    1]\n",
      " [ 562    1]\n",
      " [3278    1]\n",
      " [ 538    1]\n",
      " [1113    1]\n",
      " [ 978    0]\n",
      " [ 359    0]\n",
      " [ 484    1]\n",
      " [2200    0]\n",
      " [ 619    1]\n",
      " [1767    1]\n",
      " [1705    0]\n",
      " [2872    0]\n",
      " [1414    1]\n",
      " [2045    1]\n",
      " [1924    1]\n",
      " [1644    1]\n",
      " [1446    1]\n",
      " [2578    0]\n",
      " [1938    0]\n",
      " [2995    0]\n",
      " [ 297    0]\n",
      " [ 986    1]\n",
      " [ 675    1]\n",
      " [3107    0]\n",
      " [3209    1]\n",
      " [ 475    1]\n",
      " [  40    0]\n",
      " [2353    1]\n",
      " [1611    0]\n",
      " [ 401    1]\n",
      " [ 585    1]\n",
      " [   5    0]\n",
      " [3157    1]\n",
      " [2727    1]\n",
      " [ 262    0]\n",
      " [2376    1]\n",
      " [2260    0]\n",
      " [ 288    1]\n",
      " [ 181    1]\n",
      " [1275    0]\n",
      " [3346    0]\n",
      " [ 861    1]\n",
      " [ 647    1]\n",
      " [2836    0]\n",
      " [ 643    1]\n",
      " [1199    1]\n",
      " [3059    1]\n",
      " [1533    0]\n",
      " [  71    1]\n",
      " [2240    1]\n",
      " [2716    1]\n",
      " [2782    1]\n",
      " [1210    0]\n",
      " [ 824    1]\n",
      " [  88    1]\n",
      " [1892    1]\n",
      " [1770    0]\n",
      " [2386    1]\n",
      " [1283    1]\n",
      " [1905    1]\n",
      " [1880    1]\n",
      " [ 772    1]\n",
      " [1363    1]\n",
      " [2604    1]\n",
      " [3350    1]\n",
      " [2481    1]\n",
      " [2034    0]\n",
      " [1441    1]\n",
      " [1201    1]\n",
      " [2272    1]\n",
      " [1009    1]\n",
      " [2792    1]\n",
      " [2085    1]\n",
      " [1190    1]\n",
      " [ 885    0]\n",
      " [3299    1]\n",
      " [1141    1]\n",
      " [3058    1]\n",
      " [ 476    1]\n",
      " [1176    1]\n",
      " [2689    1]\n",
      " [3337    0]\n",
      " [1365    1]\n",
      " [2844    1]\n",
      " [ 540    1]\n",
      " [ 494    1]\n",
      " [2672    1]\n",
      " [ 712    0]\n",
      " [ 666    1]\n",
      " [1396    1]\n",
      " [3222    0]\n",
      " [2127    1]\n",
      " [2026    1]\n",
      " [1788    1]\n",
      " [ 490    1]\n",
      " [1875    0]\n",
      " [ 888    0]\n",
      " [1138    0]\n",
      " [1606    0]\n",
      " [2460    1]\n",
      " [3147    1]\n",
      " [2289    0]\n",
      " [3115    0]\n",
      " [ 120    0]\n",
      " [1774    0]\n",
      " [3248    1]\n",
      " [1480    0]\n",
      " [1318    1]\n",
      " [2974    1]\n",
      " [2158    1]\n",
      " [1453    0]\n",
      " [2028    1]\n",
      " [2208    1]\n",
      " [ 365    1]\n",
      " [2145    0]\n",
      " [ 144    1]\n",
      " [2182    0]\n",
      " [1455    1]\n",
      " [1400    1]\n",
      " [1957    1]\n",
      " [1349    1]\n",
      " [ 737    0]\n",
      " [1835    1]\n",
      " [1994    1]\n",
      " [1186    1]\n",
      " [1971    1]\n",
      " [1193    1]\n",
      " [ 389    1]\n",
      " [2306    1]\n",
      " [3319    0]\n",
      " [3284    1]\n",
      " [ 806    0]\n",
      " [1965    0]\n",
      " [ 105    1]\n",
      " [ 116    0]\n",
      " [ 269    1]\n",
      " [2133    0]\n",
      " [3231    1]\n",
      " [3043    0]\n",
      " [1899    1]\n",
      " [1110    1]\n",
      " [3232    0]\n",
      " [1607    0]\n",
      " [1444    1]\n",
      " [1556    0]\n",
      " [3135    0]\n",
      " [ 461    1]\n",
      " [2037    1]\n",
      " [ 804    1]\n",
      " [ 115    1]\n",
      " [ 402    0]\n",
      " [ 398    0]\n",
      " [1897    0]\n",
      " [ 763    0]\n",
      " [2515    1]\n",
      " [2100    1]\n",
      " [2275    1]\n",
      " [3122    0]\n",
      " [1077    1]\n",
      " [1656    1]\n",
      " [2633    1]\n",
      " [2179    0]\n",
      " [2458    0]\n",
      " [3049    1]\n",
      " [2943    0]\n",
      " [  94    0]\n",
      " [3193    1]\n",
      " [2432    1]\n",
      " [2325    1]\n",
      " [1517    1]\n",
      " [2009    1]\n",
      " [1579    0]\n",
      " [1990    0]\n",
      " [2300    0]\n",
      " [1297    1]\n",
      " [ 318    1]\n",
      " [ 315    1]\n",
      " [2597    1]\n",
      " [ 268    0]\n",
      " [3080    1]\n",
      " [ 491    1]\n",
      " [ 707    1]\n",
      " [2682    0]\n",
      " [ 280    1]\n",
      " [ 953    0]\n",
      " [1714    1]\n",
      " [ 682    1]\n",
      " [1642    1]\n",
      " [1982    1]\n",
      " [2516    1]\n",
      " [2030    1]\n",
      " [2025    0]\n",
      " [3192    1]\n",
      " [1736    1]\n",
      " [1195    1]\n",
      " [ 808    0]\n",
      " [2870    1]\n",
      " [1033    0]\n",
      " [3304    0]\n",
      " [3104    1]\n",
      " [1439    1]\n",
      " [1742    0]\n",
      " [ 384    0]\n",
      " [1658    1]\n",
      " [1914    0]\n",
      " [1097    1]\n",
      " [1570    1]\n",
      " [ 454    1]\n",
      " [3028    1]\n",
      " [2402    0]\n",
      " [1668    1]\n",
      " [3252    1]\n",
      " [2304    1]\n",
      " [3090    1]\n",
      " [ 558    0]\n",
      " [2659    0]\n",
      " [2948    0]\n",
      " [1479    1]\n",
      " [ 831    0]\n",
      " [2444    0]\n",
      " [  42    0]\n",
      " [1173    1]\n",
      " [   3    1]\n",
      " [1326    1]\n",
      " [2154    0]\n",
      " [3076    1]\n",
      " [3308    0]\n",
      " [2499    1]\n",
      " [2812    1]\n",
      " [ 311    0]\n",
      " [3001    0]\n",
      " [ 184    1]\n",
      " [2992    0]\n",
      " [2022    0]\n",
      " [2031    1]\n",
      " [1079    1]\n",
      " [2422    1]\n",
      " [ 270    1]\n",
      " [2521    0]\n",
      " [ 535    1]\n",
      " [2058    1]\n",
      " [1002    1]\n",
      " [ 726    1]\n",
      " [  12    0]\n",
      " [1519    1]\n",
      " [2916    0]\n",
      " [1524    1]\n",
      " [1342    1]\n",
      " [ 925    1]\n",
      " [3294    0]\n",
      " [1293    0]\n",
      " [2753    0]\n",
      " [2277    0]\n",
      " [ 546    0]\n",
      " [1854    1]\n",
      " [2244    1]\n",
      " [2813    1]\n",
      " [2933    1]\n",
      " [3228    0]\n",
      " [3131    0]\n",
      " [2278    1]\n",
      " [3343    1]\n",
      " [1159    1]\n",
      " [1819    0]\n",
      " [2233    1]\n",
      " [1564    1]\n",
      " [3000    1]\n",
      " [1118    0]\n",
      " [ 837    1]\n",
      " [1442    1]\n",
      " [ 801    0]\n",
      " [1222    0]\n",
      " [2764    1]\n",
      " [ 918    0]\n",
      " [1734    1]\n",
      " [ 616    0]\n",
      " [2419    1]\n",
      " [2068    1]\n",
      " [  70    0]\n",
      " [2007    0]\n",
      " [2246    1]\n",
      " [ 541    1]\n",
      " [3046    1]\n",
      " [1991    1]\n",
      " [2878    0]\n",
      " [1726    1]\n",
      " [1106    1]\n",
      " [1312    0]\n",
      " [1747    0]\n",
      " [3099    0]\n",
      " [2122    0]\n",
      " [1019    1]\n",
      " [1045    1]\n",
      " [3220    1]\n",
      " [ 511    1]\n",
      " [ 765    0]\n",
      " [  43    1]\n",
      " [1772    0]\n",
      " [3067    1]\n",
      " [2492    0]\n",
      " [3179    0]\n",
      " [2411    1]\n",
      " [3002    1]\n",
      " [1272    1]\n",
      " [1585    1]\n",
      " [2524    1]\n",
      " [ 821    1]\n",
      " [1224    0]\n",
      " [2892    0]\n",
      " [2947    1]\n",
      " [3065    1]\n",
      " [1296    1]\n",
      " [1258    0]\n",
      " [1912    1]\n",
      " [1761    1]\n",
      " [1062    1]\n",
      " [2035    1]\n",
      " [2599    1]\n",
      " [2935    0]\n",
      " [2297    1]\n",
      " [1367    1]\n",
      " [2619    1]\n",
      " [1302    0]\n",
      " [2982    1]\n",
      " [ 218    1]\n",
      " [1243    0]\n",
      " [2401    1]\n",
      " [3031    0]\n",
      " [3259    1]\n",
      " [2666    0]\n",
      " [3087    1]\n",
      " [2544    1]\n",
      " [2680    0]\n",
      " [1162    1]\n",
      " [2057    0]\n",
      " [1919    1]\n",
      " [ 678    0]\n",
      " [ 234    1]\n",
      " [1627    1]\n",
      " [1907    1]\n",
      " [3084    1]\n",
      " [ 296    1]\n",
      " [2217    0]\n",
      " [1591    0]\n",
      " [1793    1]\n",
      " [1712    1]\n",
      " [1692    0]\n",
      " [2144    1]\n",
      " [ 431    1]\n",
      " [1061    1]\n",
      " [1254    1]\n",
      " [ 138    1]\n",
      " [3178    1]\n",
      " [2712    0]\n",
      " [ 595    1]\n",
      " [2207    1]\n",
      " [1598    0]\n",
      " [2001    0]\n",
      " [1090    1]\n",
      " [1386    1]\n",
      " [ 521    0]\n",
      " [2380    0]\n",
      " [1621    1]\n",
      " [2416    1]\n",
      " [ 738    1]\n",
      " [2757    1]\n",
      " [ 578    1]\n",
      " [ 217    1]\n",
      " [3082    1]\n",
      " [1006    1]\n",
      " [2837    1]\n",
      " [1973    1]\n",
      " [1116    1]\n",
      " [ 525    0]\n",
      " [ 526    1]\n",
      " [2254    0]\n",
      " [1039    1]\n",
      " [1393    1]\n",
      " [ 904    1]\n",
      " [2105    0]\n",
      " [2398    0]\n",
      " [3025    0]\n",
      " [2921    1]\n",
      " [1916    1]\n",
      " [2765    1]\n",
      " [ 950    1]\n",
      " [3130    0]\n",
      " [2040    1]\n",
      " [ 990    1]\n",
      " [2586    1]\n",
      " [ 216    1]\n",
      " [3266    1]\n",
      " [2881    1]\n",
      " [3242    1]\n",
      " [ 524    1]\n",
      " [ 721    0]\n",
      " [3129    1]\n",
      " [1390    1]\n",
      " [1500    0]\n",
      " [1857    0]\n",
      " [2218    1]\n",
      " [2648    1]\n",
      " [2371    0]\n",
      " [3247    1]\n",
      " [3241    1]\n",
      " [ 676    0]\n",
      " [ 715    1]\n",
      " [3349    0]\n",
      " [1438    1]\n",
      " [2374    0]\n",
      " [2027    0]\n",
      " [ 182    1]\n",
      " [3327    1]\n",
      " [2357    1]\n",
      " [ 419    1]\n",
      " [1930    1]\n",
      " [1628    1]\n",
      " [2397    1]\n",
      " [ 559    1]\n",
      " [  87    1]\n",
      " [2466    0]\n",
      " [ 691    1]\n",
      " [ 154    1]\n",
      " [1981    1]\n",
      " [2408    0]\n",
      " [2451    0]\n",
      " [2358    0]\n",
      " [ 939    0]\n",
      " [3336    0]\n",
      " [3152    0]\n",
      " [1617    1]\n",
      " [2134    1]\n",
      " [2660    1]\n",
      " [ 470    1]\n",
      " [2738    1]\n",
      " [1070    1]\n",
      " [3112    0]\n",
      " [2602    0]]\n",
      "Instances:3368\n",
      "(array([   2,    5,    8,   10,   12,   16,   18,   21,   22,   26,   32,\n",
      "         34,   36,   40,   42,   46,   50,   55,   63,   67,   70,   72,\n",
      "         75,   80,   82,   86,   92,   94,   99,  101,  104,  108,  111,\n",
      "        113,  116,  120,  123,  126,  129,  133,  136,  140,  142,  145,\n",
      "        147,  152,  153,  155,  161,  167,  169,  172,  174,  177,  180,\n",
      "        186,  187,  190,  192,  193,  196,  200,  202,  204,  207,  209,\n",
      "        210,  211,  213,  215,  221,  223,  224,  228,  233,  237,  238,\n",
      "        240,  246,  248,  250,  255,  257,  259,  260,  262,  266,  268,\n",
      "        272,  274,  275,  278,  279,  285,  287,  297,  304,  308,  311,\n",
      "        313,  316,  319,  321,  323,  326,  330,  335,  340,  341,  343,\n",
      "        353,  357,  359,  360,  361,  364,  366,  367,  373,  376,  377,\n",
      "        384,  387,  388,  390,  391,  393,  395,  396,  398,  402,  405,\n",
      "        406,  416,  420,  422,  424,  427,  428,  441,  442,  448,  455,\n",
      "        458,  463,  466,  469,  473,  474,  482,  486,  489,  493,  497,\n",
      "        502,  505,  512,  514,  516,  521,  522,  525,  530,  534,  536,\n",
      "        543,  546,  548,  550,  556,  558,  561,  563,  566,  568,  569,\n",
      "        571,  573,  579,  583,  587,  591,  597,  601,  604,  614,  615,\n",
      "        616,  625,  627,  630,  642,  646,  652,  655,  657,  659,  660,\n",
      "        661,  663,  665,  667,  670,  672,  674,  676,  678,  680,  688,\n",
      "        690,  695,  697,  698,  702,  705,  710,  711,  712,  714,  721,\n",
      "        724,  728,  729,  735,  737,  739,  740,  742,  750,  751,  758,\n",
      "        761,  763,  765,  769,  771,  777,  785,  786,  792,  801,  806,\n",
      "        808,  812,  816,  818,  820,  822,  826,  827,  828,  829,  831,\n",
      "        845,  847,  848,  855,  860,  862,  867,  869,  871,  873,  874,\n",
      "        876,  878,  882,  885,  887,  888,  889,  892,  895,  897,  901,\n",
      "        903,  918,  919,  921,  923,  931,  934,  939,  940,  942,  946,\n",
      "        949,  951,  953,  955,  957,  962,  964,  966,  974,  978,  981,\n",
      "        988,  991,  994,  996, 1003, 1007, 1010, 1011, 1012, 1013, 1015,\n",
      "       1016, 1017, 1021, 1023, 1025, 1028, 1029, 1033, 1037, 1042, 1044,\n",
      "       1046, 1050, 1052, 1054, 1058, 1069, 1071, 1075, 1080, 1081, 1082,\n",
      "       1084, 1086, 1088, 1091, 1093, 1100, 1103, 1105, 1107, 1118, 1122,\n",
      "       1124, 1126, 1128, 1130, 1133, 1136, 1138, 1145, 1148, 1149, 1151,\n",
      "       1153, 1155, 1156, 1161, 1163, 1166, 1169, 1172, 1177, 1180, 1182,\n",
      "       1184, 1187, 1189, 1191, 1194, 1196, 1202, 1203, 1205, 1208, 1210,\n",
      "       1215, 1217, 1219, 1221, 1222, 1224, 1228, 1233, 1237, 1241, 1243,\n",
      "       1246, 1248, 1249, 1253, 1255, 1257, 1258, 1260, 1263, 1264, 1266,\n",
      "       1268, 1275, 1277, 1281, 1282, 1289, 1293, 1295, 1298, 1301, 1302,\n",
      "       1305, 1306, 1308, 1312, 1314, 1316, 1317, 1320, 1322, 1325, 1328,\n",
      "       1330, 1336, 1337, 1341, 1344, 1346, 1351, 1353, 1356, 1358, 1360,\n",
      "       1361, 1364, 1366, 1369, 1370, 1372, 1374, 1376, 1383, 1384, 1385,\n",
      "       1388, 1389, 1397, 1398, 1403, 1404, 1406, 1412, 1419, 1421, 1424,\n",
      "       1426, 1428, 1430, 1432, 1433, 1434, 1449, 1451, 1452, 1453, 1464,\n",
      "       1468, 1469, 1471, 1472, 1480, 1481, 1490, 1491, 1495, 1500, 1503,\n",
      "       1505, 1507, 1509, 1513, 1514, 1518, 1520, 1522, 1526, 1528, 1530,\n",
      "       1533, 1535, 1537, 1540, 1543, 1545, 1547, 1552, 1556, 1560, 1574,\n",
      "       1578, 1579, 1583, 1584, 1587, 1588, 1590, 1591, 1596, 1597, 1598,\n",
      "       1601, 1603, 1606, 1607, 1611, 1613, 1616, 1620, 1623, 1626, 1630,\n",
      "       1631, 1634, 1637, 1641, 1643, 1646, 1648, 1650, 1655, 1657, 1659,\n",
      "       1662, 1665, 1666, 1667, 1673, 1675, 1676, 1680, 1683, 1684, 1686,\n",
      "       1689, 1692, 1696, 1697, 1698, 1700, 1702, 1705, 1707, 1711, 1717,\n",
      "       1727, 1731, 1739, 1742, 1744, 1747, 1751, 1753, 1755, 1757, 1764,\n",
      "       1766, 1768, 1770, 1772, 1774, 1777, 1782, 1785, 1787, 1796, 1802,\n",
      "       1808, 1809, 1811, 1816, 1817, 1819, 1821, 1822, 1824, 1827, 1829,\n",
      "       1832, 1834, 1840, 1841, 1843, 1845, 1847, 1849, 1850, 1857, 1858,\n",
      "       1859, 1862, 1864, 1865, 1867, 1873, 1875, 1883, 1884, 1886, 1891,\n",
      "       1893, 1894, 1897, 1904, 1906, 1909, 1913, 1914, 1917, 1918, 1920,\n",
      "       1925, 1927, 1934, 1938, 1943, 1948, 1954, 1956, 1961, 1963, 1964,\n",
      "       1965, 1967, 1970, 1972, 1974, 1976, 1978, 1980, 1983, 1987, 1990,\n",
      "       1992, 2001, 2004, 2007, 2016, 2017, 2019, 2022, 2025, 2027, 2034,\n",
      "       2036, 2039, 2049, 2054, 2056, 2057, 2060, 2061, 2065, 2066, 2069,\n",
      "       2070, 2072, 2074, 2081, 2084, 2086, 2090, 2091, 2092, 2099, 2103,\n",
      "       2105, 2108, 2110, 2112, 2116, 2122, 2124, 2128, 2130, 2132, 2133,\n",
      "       2135, 2136, 2142, 2143, 2145, 2148, 2150, 2151, 2154, 2156, 2160,\n",
      "       2163, 2164, 2167, 2172, 2174, 2179, 2182, 2190, 2195, 2200, 2201,\n",
      "       2202, 2204, 2205, 2209, 2216, 2217, 2219, 2220, 2221, 2223, 2228,\n",
      "       2229, 2231, 2234, 2236, 2237, 2242, 2245, 2247, 2249, 2253, 2254,\n",
      "       2256, 2260, 2262, 2265, 2270, 2271, 2274, 2277, 2279, 2280, 2281,\n",
      "       2286, 2289, 2292, 2294, 2296, 2300, 2303, 2305, 2308, 2310, 2311,\n",
      "       2314, 2317, 2318, 2321, 2323, 2324, 2328, 2330, 2333, 2337, 2340,\n",
      "       2351, 2352, 2354, 2356, 2358, 2366, 2369, 2371, 2373, 2374, 2380,\n",
      "       2385, 2387, 2389, 2391, 2394, 2398, 2402, 2403, 2404, 2408, 2412,\n",
      "       2414, 2415, 2423, 2427, 2428, 2430, 2436, 2439, 2441, 2443, 2444,\n",
      "       2446, 2450, 2451, 2458, 2461, 2466, 2469, 2472, 2474, 2476, 2478,\n",
      "       2483, 2485, 2487, 2492, 2493, 2494, 2496, 2497, 2501, 2502, 2503,\n",
      "       2507, 2521, 2527, 2531, 2536, 2542, 2545, 2548, 2550, 2554, 2559,\n",
      "       2561, 2563, 2567, 2570, 2572, 2575, 2578, 2580, 2581, 2583, 2585,\n",
      "       2588, 2591, 2592, 2594, 2602, 2603, 2606, 2607, 2608, 2612, 2613,\n",
      "       2618, 2621, 2625, 2629, 2630, 2636, 2641, 2644, 2646, 2649, 2650,\n",
      "       2653, 2656, 2659, 2661, 2663, 2666, 2668, 2669, 2671, 2673, 2675,\n",
      "       2678, 2680, 2682, 2695, 2699, 2701, 2703, 2705, 2708, 2712, 2715,\n",
      "       2717, 2725, 2729, 2737, 2740, 2741, 2744, 2747, 2748, 2749, 2751,\n",
      "       2753, 2754, 2755, 2758, 2760, 2761, 2767, 2770, 2772, 2773, 2775,\n",
      "       2780, 2783, 2784, 2786, 2787, 2788, 2793, 2796, 2802, 2804, 2809,\n",
      "       2817, 2819, 2820, 2821, 2824, 2826, 2829, 2831, 2834, 2836, 2839,\n",
      "       2849, 2855, 2858, 2860, 2861, 2863, 2868, 2872, 2874, 2876, 2878,\n",
      "       2879, 2882, 2884, 2887, 2889, 2892, 2894, 2896, 2899, 2906, 2908,\n",
      "       2911, 2912, 2916, 2919, 2927, 2931, 2935, 2937, 2941, 2943, 2945,\n",
      "       2948, 2950, 2952, 2953, 2956, 2958, 2961, 2965, 2970, 2980, 2986,\n",
      "       2987, 2989, 2992, 2993, 2994, 2995, 2998, 3001, 3003, 3005, 3009,\n",
      "       3011, 3013, 3015, 3016, 3021, 3022, 3023, 3025, 3027, 3031, 3032,\n",
      "       3033, 3035, 3036, 3038, 3041, 3043, 3045, 3047, 3050, 3052, 3056,\n",
      "       3060, 3070, 3073, 3074, 3078, 3085, 3086, 3089, 3092, 3095, 3097,\n",
      "       3099, 3102, 3103, 3105, 3107, 3112, 3114, 3115, 3118, 3122, 3124,\n",
      "       3130, 3131, 3135, 3137, 3140, 3141, 3145, 3148, 3151, 3152, 3155,\n",
      "       3156, 3158, 3161, 3164, 3166, 3168, 3169, 3171, 3174, 3176, 3179,\n",
      "       3181, 3182, 3183, 3189, 3196, 3199, 3202, 3204, 3205, 3208, 3210,\n",
      "       3211, 3215, 3217, 3222, 3224, 3226, 3228, 3230, 3232, 3235, 3236,\n",
      "       3240, 3250, 3251, 3253, 3255, 3257, 3262, 3264, 3265, 3267, 3271,\n",
      "       3274, 3276, 3282, 3285, 3287, 3289, 3291, 3294, 3298, 3304, 3308,\n",
      "       3309, 3311, 3313, 3314, 3317, 3318, 3319, 3322, 3328, 3330, 3332,\n",
      "       3336, 3337, 3338, 3340, 3342, 3344, 3346, 3348, 3349, 3351, 3354,\n",
      "       3357, 3359, 3362], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "print(Classes_test.reset_index().drop(\"index\",axis=1))\n",
    "balancing_array2 = np.array(Classes_test.reset_index()) #Array of class labels. 1 = PD = majority, 0 = SNP = minority\n",
    "print(balancing_array2)\n",
    "\n",
    "\n",
    "balancing_array = np.array(Output) #Array of class labels. 1 = PD = majority, 0 = SNP = minority\n",
    "print(f\"Instances:{len(balancing_array)}\")\n",
    "count_0 = 0\n",
    "count_1 = 0\n",
    "index_list = []\n",
    "\n",
    "for i in balancing_array:\n",
    "    rnd = np.random.choice(balancing_array)\n",
    "    if rnd == 0:\n",
    "        count_0 +=1\n",
    "        itemindex = np.where(balancing_array == rnd)\n",
    "    if rnd == 1:\n",
    "        count_1 +=1\n",
    "        \n",
    "print(itemindex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dfcbde",
   "metadata": {},
   "source": [
    "### Input training sets for each class (PD and SNP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24405e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PD:1803\n",
      "SNP:891\n"
     ]
    }
   ],
   "source": [
    "Training_Outputs = pd.DataFrame([Output]).reset_index(drop=True).transpose() #Converts Output series to dataframe\n",
    "Training_Outputs.columns = ['Class'] #Names column 'Class'\n",
    "Majority_Class= Training_Outputs.drop(Training_Outputs[Training_Outputs['Class'] == 0].index) #Removes all SNP classes; only PD majority class remains\n",
    "Minority_Class = Training_Outputs.drop(Training_Outputs[Training_Outputs['Class'] == 1].index) #Removes all PD classes; only SNP minority class remains\n",
    "#print(\"PD:\",len(Majority_Class),\"SNP:\", len(Minority_Class))\n",
    "\n",
    "# Only the rows in input training set that are PD\n",
    "Input_PD_train = Input_train.loc[Input_train.index.isin(Majority_Class.index.values)]\n",
    "Classes_PD_train = Classes_train.loc[Classes_train.index.isin(Majority_Class.index.values)] #Labels training data of only PD. 1803\n",
    "\n",
    "# Only the rows in X (input training set) that are SNP\n",
    "Input_SNP_train = Input_train.loc[Input_train.index.isin(Minority_Class.index.values)]\n",
    "Classes_SNP_train = Classes_train.loc[Classes_train.index.isin(Minority_Class.index.values)] #Labels training data of only SNP. 891\n",
    "\n",
    "print(f\"PD:{len(Input_PD_train)}\\nSNP:{len(Input_SNP_train)}\") #Returns the number of only PD or SNP instances in respective training data sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b8fdd",
   "metadata": {},
   "source": [
    "### Weighted voting for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee46ca48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "Prob_PD = RFC.predict_proba(Input_PD_train) #Probability prediction, as a mean of all trees in RF, for the PD instances\n",
    "Prob_SNP = RFC.predict_proba(Input_SNP_train) #Probability prediction, as a mean of all trees in RF, for the SNP instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6746be83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbalance ratio:\n",
      "2.0235690235690234:1\n",
      "5 balancing folds needed.\n"
     ]
    }
   ],
   "source": [
    "def Ratio_divide(): #Finds ratio between the 2 classes (i.e the imbalance) and the number of folds required\n",
    "    a = len(Input_PD_train)\n",
    "    b = len(Input_SNP_train)\n",
    "    Divide = a/b\n",
    "    if Divide <= 1:\n",
    "        Ratio = 1/Divide\n",
    "    else:\n",
    "        Ratio = Divide\n",
    "        \n",
    "    if round(Ratio) % 2 == 0:\n",
    "        BF = 2 * round(Ratio) + 1\n",
    "    else:\n",
    "        BF = round(Ratio)\n",
    "    return Ratio, BF;\n",
    "Ratio, BF = Ratio_divide()\n",
    "\n",
    "print(f\"Imbalance ratio:\\n{Ratio}:1\\n{BF} balancing folds needed.\")\n",
    "\n",
    "#Build a model for each balancing fold, predict the confidence scores and find average for each, which gives final vote\n",
    " \n",
    "BF_RFC = RandomForestClassifier(random_state = 42, n_estimators = 100) #Defines the Random Forest. 42 seeds, 100 trees (increase)\n",
    "BF_Prob = []\n",
    "BF_Probtxt = []\n",
    "BF_data = []\n",
    "\n",
    "#Randomly samples the majority class PD, to the same size of minority class SNP, and scores confidence for each instance\n",
    "for i in range (BF):\n",
    "    a = Random_Seed()\n",
    "    BF_Input_PD_train = Input_PD_train.sample(frac = (1/Ratio), random_state = a//100) #Balanced PD input training data \n",
    "    BF_Classes_PD_train = Classes_PD_train.sample(frac = (1/Ratio), random_state = a//100) #Balanced PD output training data\n",
    "    \n",
    "    #Concatanates the input and output balancing fold training data, so a new RFC can be generated. Function\n",
    "    BF_Input_all = shuffle(pd.concat([BF_Input_PD_train, Input_SNP_train]), random_state = a//50)\n",
    "    BF_Output_all = shuffle(pd.concat([BF_Classes_PD_train, Classes_SNP_train]), random_state = a//50)\n",
    "    Full_set = pd.concat([BF_Input_all, BF_Output_all], axis = 1)\n",
    "\n",
    "\n",
    "    #Generates RF and calculates probabilities for each fold\n",
    "    BF_RFC.fit(BF_Input_all, BF_Output_all)\n",
    "    BF_data.append(Full_set) #Builds random forest from training data for each fold\n",
    "    \n",
    "    Prob_index = BF_RFC.predict_proba(BF_Input_all) #Predicts confidence score for each instance\n",
    "    Full_set[['SNP', 'PD']] = Prob_index\n",
    "    Prob_index = Full_set.drop(labels=['Binding','SProtFT0','SProtFT1','SProtFT2','SProtFT3','SProtFT4','SProtFT5','SProtFT6','SProtFT7','SProtFT8','SProtFT9','SProtFT10','SProtFT11','SProtFT12','Interface','Relaccess','Impact','HBonds','SPhobic','CPhilic','BCharge','SSGeom','Voids','MLargest1','MLargest2','MLargest3','MLargest4','MLargest5','MLargest6','MLargest7','MLargest8','MLargest9','MLargest10','NLargest1','NLargest2','NLargest3','NLargest4','NLargest5','NLargest6','NLargest7','NLargest8','NLargest9','NLargest10','Clash','Glycine','Proline','CisPro','dataset_pd'], axis=1, inplace=False)\n",
    "    #Becomes a list\n",
    "    BF_Prob.append(Prob_index) #List with probabilites for all instances\n",
    "    BF_Probtxt.append(Prob_index.to_string())\n",
    "    \n",
    "with open('Balanced probabilities.txt', 'w') as f:\n",
    "    for number, line in zip(range(BF), BF_Probtxt):\n",
    "        f.write(f\"Fold: {number}\\n\\n{line}\\n\\n\\n\")\n",
    "        \n",
    "with open('Balanced training data.txt', 'w') as f:\n",
    "    for number, fold in zip(range(BF), BF_data):\n",
    "        f.write(f\"Fold: {number}\\n\\n{fold}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dead8029",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2049    0.644\n",
       "2       0.800\n",
       "2054    0.956\n",
       "8       0.784\n",
       "2056    0.888\n",
       "        ...  \n",
       "2004    0.756\n",
       "2017    0.852\n",
       "2019    0.780\n",
       "2036    0.804\n",
       "2039    0.692\n",
       "Length: 943, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply weighted vote scheme for a predictor that outputs confidence value between 0 and 1 for each class. \n",
    "def Weighted_Proba(BF_Prob, BF_list):\n",
    "\n",
    "    # Identify which instance appear in all folds, uding the nth fold (random number)\n",
    "    intersect_index = set(BF_Prob[0].index.values) #index of all 1782 instances in first fold\n",
    "    \n",
    "    for i in BF_list: #for all 5 folds\n",
    "        intersect_index = intersect_index.intersection(set(BF_Prob[i].index.values)) #intersection checks if instances are in all folds\n",
    "    intersect_index_list = list(intersect_index) #converts set to list, 945 items\n",
    "\n",
    "    with open('All Instances.txt', 'w') as f:\n",
    "        for line in intersect_index_list:\n",
    "            f.write(f\"{line}\\n\")\n",
    "            \n",
    "    BF_prob_list_PD =[]\n",
    "    BF_prob_list_SNP =[]\n",
    "    for i in BF_list: #for all 5 folds\n",
    "        BF_Prob_instance = BF_Prob[i].loc[intersect_index_list,:] #Returns each dataframe with the common instances\n",
    "        BF_prob_list_PD.append(BF_Prob_instance.iloc[:,1] - BF_Prob_instance.iloc[:,0]) #PD - SNP prob\n",
    "        BF_prob_list_SNP.append(BF_Prob_instance.iloc[:,0] - BF_Prob_instance.iloc[:,1]) #SNP - PD prob   \n",
    "    \n",
    "    SNP_Sum = 0\n",
    "    for i in BF_list:\n",
    "        SNP_Sum += BF_prob_list_SNP[i]\n",
    "    PD_Sum = 0\n",
    "    for i in BF_list:\n",
    "        PD_Sum += BF_prob_list_PD[i]\n",
    "        \n",
    "    S_out = abs((PD_Sum - SNP_Sum)/(len(BF_list) * 2))\n",
    "    return(S_out)\n",
    "\n",
    "Weighted_Proba(BF_Prob, BF_list=range(BF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8fe70fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_out_string = Weighted_Proba(BF_Prob, BF_list=range(BF)).to_string() #Write S_out to text file\n",
    "with open('S_out.txt', 'w') as f:\n",
    "    f.write(S_out_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05628e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2084    0.96\n",
       "1422    0.02\n",
       "3027    0.81\n",
       "2483    0.82\n",
       "2788    0.92\n",
       "        ... \n",
       "2534    0.04\n",
       "50      0.82\n",
       "3127    0.01\n",
       "3018    0.07\n",
       "857     0.00\n",
       "Name: SNP, Length: 1782, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BF_Prob[0].iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a322d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final vote\n",
    "\n",
    "PD = BF_Prob[0].iloc[:,1].to_numpy()\n",
    "SNP = BF_Prob[0].iloc[:,0].to_numpy()\n",
    "# if np.greater(PD,SNP):\n",
    "\n",
    "# PD_Count = 0\n",
    "# SNP_Count = 0\n",
    "# for i in range(len(BF_Prob)):\n",
    "#     if np.greater(PD,SNP):\n",
    "#         PD_Count += 1\n",
    "#     elif BF_Prob[i].iloc[:,0] > BF_Prob[i].iloc[:,1]:\n",
    "#         SNP_Count += 1\n",
    "#     else:\n",
    "#         pass\n",
    "\n",
    "        \n",
    "\n",
    "# PD_Count = 0\n",
    "# for i in Weighted_Proba(BF_Prob, BF_list=range(BF)):\n",
    "#     if i > 0.5:\n",
    "#         PD_Count += 1\n",
    "#     else:\n",
    "#         pass\n",
    "# print(PD_Count,\"samples predicted to be PD\")\n",
    "\n",
    "# SNP_Count = 0\n",
    "# for i in Weighted_Proba(BF_Prob, BF_list=range(BF)):\n",
    "#     if i < 0.5:\n",
    "#         SNP_Count += 1\n",
    "#     else:\n",
    "#         pass\n",
    "# print(SNP_Count,\"samples predicted to be SNP\")\n",
    "\n",
    "\n",
    "# #Evaluation of training after weighted vote\n",
    "# Classes_pred = RFC.predict(Input_test)\n",
    "# print(f\"Confusion Matrix:\\n {confusion_matrix(Classes_test, Classes_pred)}\")\n",
    "# print(f\"MCC:\\n {matthews_corrcoef(Classes_test, Classes_pred)}\")\n",
    "# print(\"F1:\\n\", f1_score(Classes_test, Classes_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb4af5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: retrive the probability from each tree for a single sample  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965f8777",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c426f23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x21d48d5f040>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # **Split data into training and test**\n",
    "# with open('SNPorPD.txt', 'w+') as f:\n",
    "#         data=f.read()\n",
    "#         f.write(str(y_test.to_string()))\n",
    "\n",
    "# # pipeline.fit(X, y) #applies list if transformers to give a fitted model\n",
    "\n",
    "plt.scatter(Classes_test, Output_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f620e35",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m gridsearch \u001b[38;5;241m=\u001b[39m GridSearchCV( \u001b[38;5;66;03m#validation\u001b[39;00m\n\u001b[0;32m      2\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m LogisticRegression(solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      3\u001b[0m     param_grid \u001b[38;5;241m=\u001b[39m {}, \u001b[38;5;66;03m#dictionary of parameters to search through\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     cv \u001b[38;5;241m=\u001b[39m StratifiedKFold(),\n\u001b[0;32m      5\u001b[0m     n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;66;03m#how many processors to run in parallel\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     scoring \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m \n\u001b[1;32m----> 8\u001b[0m     )\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "gridsearch = GridSearchCV( #validation\n",
    "    estimator = LogisticRegression(solver='saga'),\n",
    "    param_grid = {}, #dictionary of parameters to search through\n",
    "    cv = StratifiedKFold(),\n",
    "    n_jobs = 1, #how many processors to run in parallel\n",
    "    scoring = 'f1',\n",
    "    verbose = 3 \n",
    "    ).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15af356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = clf.predict(X_test)\n",
    "# print(\"Training time:\", stop-start)\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "# print(\"MCC:\\n\", matthews_corrcoef(y_test, y_pred))\n",
    "# print(\"F1:\\n\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de8521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 337.844,
   "position": {
    "height": "359.844px",
    "left": "1536px",
    "right": "20px",
    "top": "112px",
    "width": "354px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
